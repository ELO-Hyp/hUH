{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0c6d0f7",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a80901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib as il\n",
    "from hypso import Hypso1, Hypso2\n",
    "import src.deh as deh\n",
    "import copy\n",
    "import gc\n",
    "il.reload(deh)\n",
    "\n",
    "HYPSO_HEIGHT    =1092\n",
    "HYPSO_WIDTH     =598\n",
    "HYPSO_BANDS     =112 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468a889f",
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPSO_IMAGE_DIR = r\"D:\\Downloads\"\n",
    "DATA_DIR        = r\"D:\\Hierarchical Unmixing Label\\hUH\\data\"\n",
    "IMAGES_DIR      = DATA_DIR + \"\\images\"\n",
    "DEH_DIR         = DATA_DIR + \"\\deh_models\"\n",
    "LABELS_DIR      = DATA_DIR + \"\\labels\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e85e33",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bdb61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions\n",
    "def plot_rgb_composite(cm, title=\"\", red_band_index=69, green_band_index=46, blue_band_index=26, aspect=0.1, figsize=(10, 10), height=1092, contrast_enhancement=False):\n",
    "    \"\"\"\n",
    "    Create and plot an RGB composite image from a hyperspectral cube.\n",
    "    \n",
    "    Parameters:\n",
    "    - cm: Input hyperspectral data cube or array (can be flattened (h*w, n_bands) or 3D (h, w, n_bands))\n",
    "    - title: Optional title for the plot\n",
    "    - red_band_index: Index of the band to use for red channel (default: 69)\n",
    "    - green_band_index: Index of the band to use for green channel (default: 46)\n",
    "    - blue_band_index: Index of the band to use for blue channel (default: 26)\n",
    "    - aspect: Aspect ratio for the plot (default: 0.1)\n",
    "    - figsize: Figure size as tuple (width, height) in inches (default: (10, 10))\n",
    "    - height: Height of the image when reshaping from flattened data (default: 1092)\n",
    "    \n",
    "    Returns:\n",
    "    - rgb_image: The processed RGB image\n",
    "    \"\"\"\n",
    "    # Check if input is already a 3D cube or needs reshaping\n",
    "    if len(cm.shape) == 2:  # Flattened data (h*w, n_bands)\n",
    "        width = cm.shape[0] // height\n",
    "        data_cube = cm.reshape(width, height, cm.shape[1])\n",
    "        data_cube = np.transpose(data_cube, (0, 1, 2))  # Ensure correct orientation\n",
    "    elif len(cm.shape) == 3:  # Already a cube (h, w, n_bands)\n",
    "        data_cube = cm\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected input shape: {cm.shape}. Expected 2D or 3D array.\")\n",
    "    \n",
    "    # Extract the specified bands for RGB channels\n",
    "    red_band = data_cube[:, :, red_band_index]\n",
    "    green_band = data_cube[:, :, green_band_index]\n",
    "    blue_band = data_cube[:, :, blue_band_index]\n",
    "\n",
    "    # Stack the bands to create an RGB image\n",
    "    rgb_image = np.stack((red_band, green_band, blue_band), axis=-1)\n",
    "    \n",
    "    # Process data for better visualization\n",
    "    # Replace NaN values with 0\n",
    "    rgb_image[np.isnan(rgb_image)] = 0\n",
    "\n",
    "    # Apply normalization to each channel\n",
    "    for i in range(3):\n",
    "        channel = rgb_image[:,:,i]\n",
    "        \n",
    "        # Always use min-max normalization regardless of value range\n",
    "        min_val = np.nanmin(channel)\n",
    "        max_val = np.nanmax(channel)\n",
    "        \n",
    "        if max_val > min_val:  # Avoid division by zero\n",
    "            # Normalize to [0,1] range\n",
    "            channel = (channel - min_val) / (max_val - min_val)\n",
    "            \n",
    "            # Apply contrast enhancement if requested\n",
    "            if contrast_enhancement and np.any(channel > 0):\n",
    "                # Only enhance contrast if we have enough non-zero values\n",
    "                non_zero_values = channel[channel > 0]\n",
    "                if len(non_zero_values) > 10:  # Arbitrary threshold\n",
    "                    percentiles = np.nanpercentile(channel, [2, 98])\n",
    "                    p_low, p_high = percentiles[0], percentiles[1]\n",
    "                    if p_high > p_low:\n",
    "                        channel = np.clip(channel, p_low, p_high)\n",
    "                        channel = (channel - p_low) / (p_high - p_low)\n",
    "            \n",
    "            rgb_image[:,:,i] = channel\n",
    "    \n",
    "    # Final normalization and cleanup\n",
    "    rgb_image = np.clip(rgb_image, 0, 1)\n",
    "    \n",
    "    # Rotate for proper orientation\n",
    "    # rgb_image = np.rot90(rgb_image)\n",
    "    \n",
    "    # Create and display the plot\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.imshow(np.rot90(rgb_image), aspect=aspect)\n",
    "    plt.axis('off')  # Hide axes for cleaner visualization\n",
    "    \n",
    "    return rgb_image\n",
    "\n",
    "def plot_DEH_overlay(DEH_input, rgb_image, node, opacity=0.5):\n",
    "    \"\"\"\n",
    "    Plot and return the overlay image of DEH figure on top of an RGB image with controllable opacity.\n",
    "    Non-zero DEH values will be shown with the specified opacity, while zero values will be fully transparent.\n",
    "    Both normal and inverse overlays will be shown side by side.\n",
    "\n",
    "    Parameters:\n",
    "    - DEH_input: The DEH input object containing the node data\n",
    "    - rgb_image: The RGB image to overlay the DEH figure on, shape (w, h, 3)\n",
    "    - node: The node to visualize\n",
    "    - opacity: The opacity of the DEH figure where values are non-zero (0.0 to 1.0)\n",
    "\n",
    "    Returns:\n",
    "    - overlay_image: The RGB image with the DEH figure overlaid\n",
    "    \"\"\"\n",
    "\n",
    "    deh_figure = DEH_input.nodes[node].map.reshape(DEH_input.plot_size)\n",
    "    \n",
    "    # Normalize both the DEH figure and the RGB image to the range [0, 1]\n",
    "    deh_figure_normalized = (deh_figure - np.min(deh_figure)) / (np.max(deh_figure) - np.min(deh_figure))\n",
    "    rgb_image_normalized = (rgb_image - np.min(rgb_image)) / (np.max(rgb_image) - np.min(rgb_image))\n",
    "\n",
    "    # Create a red overlay\n",
    "    red_overlay = np.ones_like(deh_figure_normalized)[:, :, np.newaxis] * [1, 0, 0]  # Red color\n",
    "\n",
    "    # Create normal overlay\n",
    "    overlay_image = rgb_image_normalized.copy()\n",
    "    non_zero_mask = deh_figure != 0\n",
    "    overlay_image[non_zero_mask] = (rgb_image_normalized[non_zero_mask] * (1 - opacity) + \n",
    "                                  red_overlay[non_zero_mask] * opacity * deh_figure_normalized[non_zero_mask, np.newaxis])\n",
    "\n",
    "    # Create inverse overlay\n",
    "    inverse_overlay = rgb_image_normalized.copy()\n",
    "    inverse_mask = deh_figure == 0\n",
    "    inverse_overlay[inverse_mask] = (rgb_image_normalized[inverse_mask] * (1 - opacity) + \n",
    "                                   red_overlay[inverse_mask] * opacity)\n",
    "\n",
    "    # Plot both overlays side by side\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(50, 200), gridspec_kw={'wspace': 0.1, 'hspace': 0.1})\n",
    "    \n",
    "    axs[0].imshow(np.rot90(overlay_image), aspect=DEH_input.plot_aspect, vmin=0, vmax=1, interpolation='bicubic')\n",
    "    axs[0].set_title(f\"Node: {node} - Original\")\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    axs[1].imshow(np.rot90(inverse_overlay), aspect=DEH_input.plot_aspect, vmin=0, vmax=1, interpolation='bicubic')\n",
    "    axs[1].set_title(f\"Node: {node} - Inverse\")\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_level_and_overlay(save_file_path, cm_input, plot_only_last_level=False, binarize=False, opacity=0.5):\n",
    "    DEH_temp = deh.DEH(no_negative_residuals=True)\n",
    "    DEH_temp.load(save_file_path)\n",
    "    cube_image=cm_input.reshape(-1,HYPSO_HEIGHT,HYPSO_BANDS)\n",
    "\n",
    "    DEH_temp.plot_size = (cube_image.shape[0],cube_image.shape[1])\n",
    "    DEH_temp.verbose = False\n",
    "    deh_predicted = DEH_temp.simple_predict(cm_input)\n",
    "    rgb_image=plot_rgb_composite(cm_input)\n",
    "    nodes=DEH_temp.nodes\n",
    "    num_levels = max(len(node) for node in DEH_temp.nodes)\n",
    "    levels_to_plot = [num_levels] if plot_only_last_level else range(1, num_levels + 1)\n",
    "    if binarize:\n",
    "        DEH_temp.binarize_lmdas()\n",
    "        DEH_temp.lmda_2_map()\n",
    "    \n",
    "    for level in levels_to_plot:\n",
    "        DEH_temp.display_level(level)\n",
    "        for node in nodes:\n",
    "            if len(node) == level:\n",
    "                overlay_image = plot_DEH_overlay(DEH_temp, rgb_image, node=node, opacity=opacity)\n",
    "\n",
    "def plot_spectra_for_level(save_file_path, cm_input, level_input=\"\"):\n",
    "    DEH_temp = deh.DEH(no_negative_residuals=True)\n",
    "    DEH_temp.load(save_file_path)\n",
    "    cube_image=cm_input.reshape(-1,HYPSO_HEIGHT,HYPSO_BANDS)\n",
    "\n",
    "    DEH_temp.plot_size = (cube_image.shape[0],cube_image.shape[1])\n",
    "    DEH_temp.simple_predict(cm_input)\n",
    "    nodes = DEH_temp.nodes\n",
    "    num_levels = max(len(node) for node in DEH_temp.nodes)\n",
    "    \n",
    "    # If level_input is empty or invalid, use the last level\n",
    "    if level_input == \"\" or not isinstance(level_input, int) or level_input > num_levels or level_input < 1:\n",
    "        level = num_levels\n",
    "    else:\n",
    "        level = level_input\n",
    "        \n",
    "    # Get nodes at specified level\n",
    "    level_nodes = [node for node in nodes if len(node) == level]\n",
    "    print(\"using level\", level, \"with\", len(level_nodes), \"nodes\")\n",
    "    DEH_temp.display_spectra(level_nodes)\n",
    "\n",
    "def save_binarized_labels_to_npy(deh_path, data, save_path=None):\n",
    "    \"\"\"\n",
    "    Save binarized labels from DEH model to a .npy file.\n",
    "    Labels are stored as a dictionary with node names as keys and binary arrays as values.\n",
    "    \n",
    "    Args:\n",
    "        deh_model: The DEH model with binarized labels\n",
    "        data: Input data used for prediction \n",
    "        output_path: Path to save the output .npy file\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    deh_binarized=deh.DEH(no_negative_residuals=True)\n",
    "    deh_binarized.load(deh_path)\n",
    "    # Get predictions and binarize\n",
    "    print(data.shape)\n",
    "    cube=data.reshape(-1,HYPSO_HEIGHT,HYPSO_BANDS)\n",
    "    deh_binarized.plot_size=(cube.shape[0],cube.shape[1])\n",
    "    print(deh_binarized.plot_size)\n",
    "    deh_binarized.simple_predict(data)\n",
    "    deh_binarized.binarize_lmdas()\n",
    "    deh_binarized.lmda_2_map()\n",
    "    \n",
    "    # Create dictionary to store labels\n",
    "    labels_dict = {}\n",
    "    \n",
    "    # Store binary labels for each node\n",
    "    for node in deh_binarized.nodes:\n",
    "        # Get binary labels from node.map which already contains the binarized values\n",
    "        labels = deh_binarized.nodes[node].map.flatten()\n",
    "        labels_dict[node] = labels\n",
    "        \n",
    "    # Save dictionary to .npy file\n",
    "    # Use provided savepath if available, otherwise create from deh_path\n",
    "    if save_path is not None:\n",
    "        output_path = save_path\n",
    "    else:\n",
    "        # Create output path based on deh_path\n",
    "        # Replace file extension and add '_labels' to the filename\n",
    "        base_path = os.path.splitext(deh_path)[0]\n",
    "        output_path = base_path + '_labels.npy'\n",
    "    \n",
    "    print(f\"Saving binarized labels to: {output_path}\")\n",
    "    np.save(output_path, labels_dict)\n",
    "\n",
    "def plot_level_labels(labels_filename, level=None):\n",
    "    \"\"\"\n",
    "    Plot binary labels for nodes at specified level, highlighting overlapping regions.\n",
    "    \n",
    "    Args:\n",
    "        npy_file: Path to .npy file containing binary labels\n",
    "        level: Level of nodes to plot (based on key length). If None, uses max level.\n",
    "    \"\"\"\n",
    "\n",
    "    labels_path=os.path.join(LABELS_DIR, labels_filename)\n",
    "    # Load labels\n",
    "    labels = np.load(labels_path, allow_pickle=True).item()\n",
    "    \n",
    "    # Get all keys and their lengths\n",
    "    key_lengths = [len(k) for k in labels.keys()]\n",
    "    max_level = max(key_lengths)\n",
    "    \n",
    "    # If level not specified or invalid, use max level\n",
    "    if level is None or level > max_level:\n",
    "        level = max_level\n",
    "        \n",
    "    # Get keys for specified level\n",
    "    level_keys = [k for k in labels.keys() if len(k) == level]\n",
    "    \n",
    "    if not level_keys:\n",
    "        print(f\"No nodes found at level {level}\")\n",
    "        return\n",
    "        \n",
    "    # Setup plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    \n",
    "    # Create combined array to track overlaps\n",
    "    first_key = level_keys[0]\n",
    "    combined_labels = np.zeros(labels[first_key].reshape(-1, HYPSO_HEIGHT).shape)\n",
    "    \n",
    "    # First pass - count overlaps\n",
    "    for key in level_keys:\n",
    "        plot_label = labels[key].reshape(-1, HYPSO_HEIGHT)\n",
    "        combined_labels += plot_label\n",
    "    \n",
    "    # Plot each node's labels\n",
    "    for i, key in enumerate(level_keys):\n",
    "        plot_label = labels[key].reshape(-1, HYPSO_HEIGHT)\n",
    "        # Create custom colormap for this node\n",
    "        colors = [(1,1,1,0), plt.cm.rainbow(i/len(level_keys))]  # Transparent white to color\n",
    "        node_cmap = plt.matplotlib.colors.LinearSegmentedColormap.from_list(f'custom_{i}', colors)\n",
    "        alpha = 0.5 if np.any(combined_labels > 1) else 1.0\n",
    "        plt.imshow(np.rot90(plot_label), aspect=0.1, alpha=alpha, cmap=node_cmap)\n",
    "    \n",
    "    # Plot overlaps with a different color if they exist\n",
    "    if np.any(combined_labels > 1):\n",
    "        overlap_mask = combined_labels > 1\n",
    "        overlap_display = np.rot90(overlap_mask.astype(float))\n",
    "        plt.imshow(overlap_display, aspect=0.1, alpha=0.7, cmap='Reds', \n",
    "                   label='Overlapping Regions')\n",
    "    \n",
    "    plt.title(f'Binary Labels for Level {level} Nodes')\n",
    "    \n",
    "    # Add legend\n",
    "    legend_elements = [plt.Rectangle((0,0),1,1, facecolor=plt.cm.rainbow(i/len(level_keys))) \n",
    "                      for i in range(len(level_keys))]\n",
    "    if np.any(combined_labels > 1):\n",
    "        legend_elements.append(plt.Rectangle((0,0),1,1, facecolor='red', alpha=0.7))\n",
    "        legend_labels = list(level_keys) + ['Overlaps']\n",
    "    else:\n",
    "        legend_labels = list(level_keys)\n",
    "    ax.legend(legend_elements, legend_labels, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def print_nodes_by_depth(deh_obj):\n",
    "    \"\"\"\n",
    "    Print all nodes in the DEH object organized by depth.\n",
    "    \n",
    "    Args:\n",
    "        deh_obj: The DEH object containing nodes\n",
    "    \"\"\"\n",
    "    # Get all nodes\n",
    "    all_nodes = list(deh_obj.nodes.keys())\n",
    "    \n",
    "    # Group nodes by depth\n",
    "    nodes_by_depth = {}\n",
    "    for node_id in all_nodes:\n",
    "        depth = len(node_id)\n",
    "        if depth not in nodes_by_depth:\n",
    "            nodes_by_depth[depth] = []\n",
    "        nodes_by_depth[depth].append(node_id)\n",
    "    \n",
    "    # Print nodes by depth\n",
    "    print(\"Nodes by depth:\")\n",
    "    for depth in sorted(nodes_by_depth.keys()):\n",
    "        node_count = len(nodes_by_depth[depth])\n",
    "        print(f\"Depth {depth}: {node_count} nodes\")\n",
    "        # Print the first 10 nodes at this depth as examples\n",
    "        example_nodes = nodes_by_depth[depth][:10]\n",
    "        if example_nodes:\n",
    "            print(f\"  Examples: {', '.join(example_nodes)}\")\n",
    "        if node_count > 10:\n",
    "            print(f\"  ... and {node_count - 10} more\")\n",
    "        print()\n",
    "\n",
    "def tree_walk(deh_obj, n=10, verbose=False):\n",
    "    \"\"\"\n",
    "    Performs a tree walk starting from the root of a DEH object.\n",
    "    The frontier expands by adding children of visited nodes.\n",
    "    The next node to visit is selected based on the highest sum value.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    deh_obj : DEH object\n",
    "        The hierarchical unmixing object to traverse\n",
    "    n : int, default=10\n",
    "        The number of nodes to have in the frontier before stopping\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        The nodes in the frontier when the threshold is reached\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Tree walking...\")\n",
    "    # Initialize with root node\n",
    "    visited = []\n",
    "    frontier = ['']  # Start with root node\n",
    "    \n",
    "    # Continue until frontier reaches desired size\n",
    "    while len(frontier) < n:\n",
    "        if not frontier:\n",
    "            break\n",
    "            \n",
    "        # Find node with highest sum in frontier\n",
    "        max_sum = -float('inf')\n",
    "        max_node = None\n",
    "        max_idx = -1\n",
    "        \n",
    "        for i, node_id in enumerate(frontier):\n",
    "            node_sum = deh_obj.nodes[node_id].map.sum()\n",
    "            if node_sum > max_sum:\n",
    "                max_sum = node_sum\n",
    "                max_node = node_id\n",
    "                max_idx = i\n",
    "        \n",
    "        # Remove the selected node from frontier and add to visited\n",
    "        frontier.pop(max_idx)\n",
    "        visited.append(max_node)\n",
    "        \n",
    "        # Add children to frontier\n",
    "        # Children of node 'x' are 'x0' and 'x1'\n",
    "        left_child = max_node + '0'\n",
    "        right_child = max_node + '1'\n",
    "        \n",
    "        # Check if children exist in the model\n",
    "        if left_child in deh_obj.nodes:\n",
    "            frontier.append(left_child)\n",
    "        if right_child in deh_obj.nodes:\n",
    "            frontier.append(right_child)\n",
    "    \n",
    "    # Calculate sum of all nodes in frontier\n",
    "    frontier_sum = 0\n",
    "    for node_id in frontier:\n",
    "        frontier_sum += deh_obj.nodes[node_id].map.sum()\n",
    "    \n",
    "    # Calculate root node sum for comparison\n",
    "    root_sum = deh_obj.nodes[''].map.sum()\n",
    "    \n",
    "    # Print results\n",
    "    if verbose:\n",
    "        print(f\"Frontier nodes when threshold of {n} nodes reached:\")\n",
    "        for node_id in frontier:\n",
    "            level = len(node_id)  # Level is determined by the length of the node_id\n",
    "            print(f\"Node: {node_id}, Level: {level}, Sum: {deh_obj.nodes[node_id].map.sum():.4f}\")\n",
    "        print(f\"Total sum of frontier nodes: {frontier_sum:.4f}\")\n",
    "        print(f\"Root node sum: {root_sum:.4f}\")\n",
    "        print(f\"Difference: {root_sum - frontier_sum:.4f}\")\n",
    "    \n",
    "    return frontier\n",
    "\n",
    "def trim_tree_to_frontier_nodes(deh_obj, node_list):\n",
    "    \"\"\"\n",
    "    Prune a DEH object by keeping only the nodes in the given list and their ancestors,\n",
    "    after extending all nodes to the same depth.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    deh_obj : DEH object\n",
    "        The DEH object to prune\n",
    "    node_list : list\n",
    "        List of node IDs to keep (frontier nodes)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pruned_deh_obj : DEH object\n",
    "        The pruned DEH object\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find the maximum depth of nodes in the node_list\n",
    "    max_depth = max(len(node_id) for node_id in node_list)\n",
    "    \n",
    "    # Extend all nodes to the same depth by adding '0's\n",
    "    extended_node_list = []\n",
    "    for node_id in node_list:\n",
    "        if len(node_id) < max_depth:\n",
    "            # Extend the node_id by adding '0's until it reaches max_depth\n",
    "            extended_node_id = node_id + '0' * (max_depth - len(node_id))\n",
    "            extended_node_list.append(extended_node_id)\n",
    "        else:\n",
    "            extended_node_list.append(node_id)\n",
    "    \n",
    "    # Create a list of all nodes in the DEH object\n",
    "    all_nodes = list(deh_obj.nodes.keys())\n",
    "    \n",
    "    # Create a set of nodes to keep (extended frontier nodes and their ancestors)\n",
    "    nodes_to_keep = set(extended_node_list)\n",
    "    \n",
    "    # Add ancestors of frontier nodes to the keep list\n",
    "    for node_id in extended_node_list:\n",
    "        # Add all prefixes of the node_id (ancestors)\n",
    "        for i in range(len(node_id)):\n",
    "            nodes_to_keep.add(node_id[:i])\n",
    "    \n",
    "    # Add the root node\n",
    "    nodes_to_keep.add('')\n",
    "    \n",
    "    # Create a list of nodes to delete\n",
    "    nodes_to_delete = []\n",
    "    for node_id in all_nodes:\n",
    "        if node_id not in nodes_to_keep:\n",
    "            # Check if any ancestor is already in the delete list\n",
    "            # If so, we don't need to add this node as it will be deleted with its ancestor\n",
    "            ancestor_in_delete_list = False\n",
    "            for i in range(1, len(node_id)):\n",
    "                if node_id[:i] in nodes_to_delete:\n",
    "                    ancestor_in_delete_list = True\n",
    "                    break\n",
    "            \n",
    "            if not ancestor_in_delete_list:\n",
    "                nodes_to_delete.append(node_id)\n",
    "    \n",
    "    # Sort nodes to delete by depth (delete deepest nodes first)\n",
    "    nodes_to_delete.sort(key=len, reverse=True)\n",
    "    \n",
    "    # Delete the nodes one by one\n",
    "    deleted_count = 0\n",
    "    for node_id in nodes_to_delete:\n",
    "        try:\n",
    "            if node_id in deh_obj.nodes:  # Check if node still exists\n",
    "                # Delete the node (this will delete the node and its descendants)\n",
    "                deh_obj.delete_node(node_id)\n",
    "                deleted_count += 1\n",
    "        except KeyError as e:\n",
    "            # Node might have been already deleted\n",
    "            print(f\"Note: Node {node_id} was already deleted or caused error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Pruned {deleted_count} nodes and its decendants from the DEH object\")\n",
    "    print(f\"Extended node list to depth {max_depth}: {extended_node_list}\")\n",
    "\n",
    "def train_random_DEH(DEH_path, image_data, endmembers,  verbose=False):\n",
    "    print(\"Training Random DEH...\")\n",
    "    DEH_random= deh.DEH(no_negative_residuals=True)\n",
    "    #Init DEH\n",
    "    DEH_random.splitting_size=1000\n",
    "    DEH_random.max_depth=2\n",
    "    DEH_random.max_iter=5\n",
    "    DEH_random.max_nodes=2\n",
    "\n",
    "\n",
    "    cube=image_data.reshape(-1,HYPSO_HEIGHT,HYPSO_BANDS)\n",
    "    #these set the size of the image to be plotted\n",
    "    DEH_random.plot_size = (cube.shape[0],cube.shape[1])\n",
    "    DEH_random.plot_aspect = 0.1\n",
    "\n",
    "    # these set the normaliztion on the data, but right now it is turned off and replaced with a nearest-neighbor noise estimate\n",
    "    DEH_random.weight_power=0\n",
    "    DEH_random.eps = 0.00\n",
    "    #dehx.wf = lambda x: (np.sum(x**2, axis=-1))**(-1/2+1/(2+dehx.eps))\n",
    "\n",
    "    #this switches between Archetype Analysis and Pure Pixel Analysis\n",
    "    DEH_random.aa = False\n",
    "\n",
    "    #this is currently replaced by \"mpp_tol\" in the training code\n",
    "    DEH_random.mixed_pix = 0\n",
    "\n",
    "    # this is the prefactor on the L2 regularization\n",
    "    DEH_random.reg=0\n",
    "\n",
    "    # if this is anything other than zero, it introduces L2 regularization on the trained weights \n",
    "    DEH_random.set_mu(0)\n",
    "\n",
    "    # this turns on a sort of protection to keep endmembers from vanishing. Generally unneeded with new training\n",
    "    DEH_random.use_bonus_boost = False\n",
    "\n",
    "    # turns on normalization of the data. Generally always used\n",
    "    DEH_random.use_norm(True)\n",
    "\n",
    "    # if greater than 1, turns on a sort of spectral sampling with PAA. Occasionally gives good performance\n",
    "    # but I generally keep it at 1 to turn it off\n",
    "    DEH_random.PAA_backcount = 1\n",
    "\n",
    "    # changes the gradient descent learning rate as a proportion of the optimal rate. I recommend keeping it at 1\n",
    "    DEH_random.a_speed= 1.0\n",
    "\n",
    "    #Verbose print\n",
    "    if verbose:\n",
    "        print(\"cm_input.shape: \", image_data.shape)\n",
    "        print(\"cube.shape: \", cube.shape)\n",
    "        print(\"DEH_random.plot_size\", DEH_random.plot_size)\n",
    "    #random Init\n",
    "    if verbose:\n",
    "        print(\"Quick nn\")\n",
    "    DEH_random.neighbors = deh.quick_nn(image_data.reshape(DEH_random.plot_size + (-1,)), k_size=1).flatten()\n",
    "    if verbose: \n",
    "        print(\"Setting neighbor weights\")\n",
    "    DEH_random.set_neighbor_weights_memory_efficient(image_data)\n",
    "    if verbose:\n",
    "        print(\"Random init\")\n",
    "    DEH_random.random_init(image_data, endmembers)\n",
    "    if verbose:\n",
    "        print(\"Simple predict\")\n",
    "    DEH_random_pred=DEH_random.simple_predict(image_data)\n",
    "    \n",
    "    # Update the save path to add '_random' before the file extension\n",
    "    if DEH_path.endswith('.h5'):\n",
    "        DEH_path = DEH_path[:-3] + '_random.h5'\n",
    "    else:\n",
    "        DEH_path = DEH_path + '_random'\n",
    "    \n",
    "    DEH_random.save(DEH_path)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Training Random DEH complete, saved to \", DEH_path)\n",
    "    \n",
    "    return DEH_path\n",
    "\n",
    "def trim_DEH(DEH_path, image_data, endmembers, verbose=False):\n",
    "    print(\"Pruning DEH...\")\n",
    "    DEH_prune=deh.DEH(no_negative_residuals=True)\n",
    "    DEH_prune.load(DEH_path)\n",
    "    DEH_prune.verbose = verbose\n",
    "    DEH_prune_pred=DEH_prune.simple_predict(image_data)\n",
    "    frontier_nodes = tree_walk(DEH_prune, n=endmembers, verbose=verbose)\n",
    "    trim_tree_to_frontier_nodes(DEH_prune, frontier_nodes)\n",
    "    if verbose:\n",
    "        print(\"\\nNodes by depth:\")\n",
    "        print_nodes_by_depth(DEH_prune)\n",
    "    \n",
    "    #Update the save path to reflect pruned model with correct number of endmembers\n",
    "    save_path_parts = DEH_path.split('_')\n",
    "    for i, part in enumerate(save_path_parts):\n",
    "        if 'end' in part:\n",
    "            # Find parts containing 'end' (like '256end')\n",
    "            prefix = part.split('end')[0]  # Get the part before 'end'\n",
    "            suffix = part.split('end')[1] if len(part.split('end')) > 1 else ''  # Get the part after 'end'\n",
    "            # Replace with new endmembers value\n",
    "            save_path_parts[i] = str(endmembers) + 'end' + suffix\n",
    "        if 'random' in part.lower():\n",
    "            # Replace 'random' with 'pruned'\n",
    "            save_path_parts[i] = part.lower().replace('random', 'trimmed')\n",
    "    \n",
    "    # Reconstruct the save path\n",
    "    DEH_path = '_'.join(save_path_parts)\n",
    "    DEH_prune.save(DEH_path)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Pruning DEH complete, saved to \", DEH_path)\n",
    "    \n",
    "    return DEH_path\n",
    "\n",
    "def stabelize_DEH(DEH_path, image_data, saturated_image_data, n_runs=10, step_delta=0.05, verbose=False):\n",
    "    print(\"Stabelizing DEH....\")\n",
    "    DEH_stabelize=deh.DEH(no_negative_residuals=True)\n",
    "    DEH_stabelize.load(DEH_path)\n",
    "    DEH_stabelize.verbose = verbose\n",
    "    if verbose:\n",
    "        print(\"Simple predict\")\n",
    "    DEH_stabelize.simple_predict(image_data)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Quick nn\")\n",
    "    DEH_stabelize.neighbors = deh.quick_nn(image_data.reshape(DEH_stabelize.plot_size + (-1,)), k_size=1).flatten()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Setting neighbor weights\")\n",
    "\n",
    "    DEH_stabelize.set_neighbor_weights_memory_efficient(image_data)\n",
    "    if verbose:\n",
    "        print(\"DEH_stabelized.full_weights.shape\",DEH_stabelize.full_weights.shape)\n",
    "    \n",
    "    #saturated_input = np.asarray(saturated_input, dtype=bool)\n",
    "    DEH_stabelize.full_weights[saturated_image_data]=0\n",
    "    DEH_stabelize.PAA_backcount=1\n",
    "    \n",
    "    # Create a new save path by replacing the last part with 'stabelized'\n",
    "    save_path_parts = DEH_path.split('_')\n",
    "    \n",
    "    save_path_parts = DEH_path.split('_')\n",
    "    for i, part in enumerate(save_path_parts):\n",
    "        if 'random' in part.lower():\n",
    "            # Replace 'random' with 'pruned'\n",
    "            save_path_parts[i] = part.lower().replace('random', 'stabelized')\n",
    "        if 'trimmed' in part.lower():\n",
    "            # Replace 'trimmed' with 'stabelized'\n",
    "            save_path_parts[i] = part.lower().replace('trimmed', 'stabelized')\n",
    "    \n",
    "    # Reconstruct the save path\n",
    "    DEH_path = '_'.join(save_path_parts)\n",
    "    \n",
    "    # Reconstruct the save path\n",
    "    save_name = '_'.join(save_path_parts)\n",
    "    if verbose:\n",
    "        print(f\"Updated save path for stabilization: {save_name}\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Accepted network stablization\")\n",
    "    DEH_stabelize.accepted_network_stablization(image_data, n_runs=n_runs, n_pts=(1000,10000), obj_record=(), sampling_points=(), mpp_tol=0.2, step_delta=step_delta, reg_max=0.2, name=save_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93aa6b1",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97164b7",
   "metadata": {},
   "source": [
    "### L1D + MACHI DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c28e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_10_L1D_112_MACHI = np.load(f'{IMAGES_DIR}//combined_10_L1D_112_MACHI.npy')\n",
    "combined_10_L1D_112_MACHI_saturated = np.load(f'{IMAGES_DIR}//combined_10_saturated.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378c2a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "yucatan2_2025_02_06_L1D         = np.load(f'{IMAGES_DIR}//yucatan2_2025-02-06T16-01-18Z-l1a_flat_L1D_112_MACHI.npy')\n",
    "kemigawa_2024_12_17_L1D         = np.load(f'{IMAGES_DIR}//kemigawa_2024-12-17T01-01-32Z-l1a_flat_L1D_112_MACHI.npy')\n",
    "chapala_2025_02_24_L1D          = np.load(f'{IMAGES_DIR}//chapala_2025-02-24T16-52-47Z-l1a_flat_L1D_112_MACHI.npy')\n",
    "grizzlybay_2025_01_27_L1D       = np.load(f'{IMAGES_DIR}//grizzlybay_2025-01-27T18-19-56Z-l1a_flat_L1D_112_MACHI.npy')\n",
    "victoriaLand_2025_02_07_L1D     = np.load(f'{IMAGES_DIR}//victoriaLand_2025-02-07T20-35-33Z-l1a_flat_L1D_112_MACHI.npy')\n",
    "catala_2025_01_28_L1D           = np.load(f'{IMAGES_DIR}//catala_2025-01-28T19-17-32Z-l1a_flat_L1D_112_MACHI.npy')\n",
    "khnifiss_2025_02_12_L1D         = np.load(f'{IMAGES_DIR}//khnifiss_2025-02-12T11-05-35Z-l1a_flat_L1D_112_MACHI.npy')\n",
    "menindee_2025_02_18_L1D         = np.load(f'{IMAGES_DIR}//menindee_2025-02-18T00-10-42Z-l1a_flat_L1D_112_MACHI.npy')\n",
    "tampa_2024_11_12_L1D            = np.load(f'{IMAGES_DIR}//tampa_2024-11-12T15-31-55Z-l1a_flat_L1D_112_MACHI.npy')\n",
    "falklandsatlantic_2024_12_18_L1D= np.load(f'{IMAGES_DIR}//falklandsatlantic_2024-12-18T13-25-18Z-l1a_flat_L1D_112_MACHI.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f26c83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat=combined_10_L1D_112_MACHI\n",
    "cube=flat.reshape(-1,HYPSO_HEIGHT,HYPSO_BANDS)\n",
    "saturated=combined_10_L1D_112_MACHI_saturated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6423d7a",
   "metadata": {},
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7412b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEH_name = \"1_10img_256end_L1D_112_MACHI.h5\"\n",
    "DEH_path=os.path.join(DEH_DIR, DEH_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dd5efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEH_random_path=train_random_DEH(DEH_path, flat, 256, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747cb10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEH_trimmed_path=trim_DEH(DEH_random_path, flat, 8, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805ff734",
   "metadata": {},
   "outputs": [],
   "source": [
    "stabelize_DEH(DEH_trimmed_path, flat, saturated, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918aafe2",
   "metadata": {},
   "source": [
    "## GENERATE LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2cfb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stabelized='10img_8end_L1D_112_MACHI_stabelized_aa.h5'\n",
    "stabelized_path=(os.path.join(DEH_DIR, stabelized))\n",
    "print(f\"Loading stabelized DEH from: {stabelized_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740c2cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name='caspiansea1_2025-04-08T07-11-56Z-l1a_flat_L1D_112_MACHI.npy'\n",
    "image_path=os.path.join(IMAGES_DIR, image_name)\n",
    "corrected_image=np.load(image_path)\n",
    "save_path = os.path.join(LABELS_DIR, os.path.basename(image_name).split('.')[0] + '_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8de4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_binarized_labels_to_npy(stabelized_path, corrected_image, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314fc337",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873b3770",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_level_and_overlay(stabelized_path, tampa_2024_11_12_L1D, plot_only_last_level=True, binarize=True, opacity=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d661285c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectra_for_level(stabelized_path, tampa_2024_11_12_L1D, level_input=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036a6289",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_level_labels(\"caspiansea1_2025-04-08T07-11-56Z-l1a_flat_L1D_112_MACHI_labels.npy\", level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe361b3",
   "metadata": {},
   "source": [
    "## DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78649404",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = \"10img_256end_L1D_112_MACHI_random.h5\"\n",
    "verbose=True\n",
    "#Init DEH\n",
    "DEH= deh.DEH(no_negative_residuals=True)\n",
    "DEH.splitting_size=1000\n",
    "DEH.max_depth=2\n",
    "DEH.max_iter=5\n",
    "DEH.max_nodes=2\n",
    "\n",
    "#these set the size of the image to be plotted\n",
    "DEH.plot_size = (cube.shape[0],cube.shape[1])\n",
    "DEH.plot_aspect = 0.1\n",
    "\n",
    "# these set the normaliztion on the data, but right now it is turned off and replaced with a nearest-neighbor noise estimate\n",
    "DEH.weight_power=0\n",
    "DEH.eps = 0.00\n",
    "#dehx.wf = lambda x: (np.sum(x**2, axis=-1))**(-1/2+1/(2+dehx.eps))\n",
    "\n",
    "#this switches between Archetype Analysis and Pure Pixel Analysis\n",
    "DEH.aa = False\n",
    "\n",
    "#this is currently replaced by \"mpp_tol\" in the training code\n",
    "DEH.mixed_pix = 0\n",
    "\n",
    "# this is the prefactor on the L2 regularization\n",
    "DEH.reg=0\n",
    "\n",
    "# if this is anything other than zero, it introduces L2 regularization on the trained weights \n",
    "DEH.set_mu(0)\n",
    "\n",
    "# this turns on a sort of protection to keep endmembers from vanishing. Generally unneeded with new training\n",
    "DEH.use_bonus_boost = False\n",
    "\n",
    "# turns on normalization of the data. Generally always used\n",
    "DEH.use_norm(True)\n",
    "\n",
    "# if greater than 1, turns on a sort of spectral sampling with PAA. Occasionally gives good performance\n",
    "# but I generally keep it at 1 to turn it off\n",
    "DEH.PAA_backcount = 1\n",
    "\n",
    "# changes the gradient descent learning rate as a proportion of the optimal rate. I recommend keeping it at 1\n",
    "DEH.a_speed= 1.0\n",
    "\n",
    "#Verbose print\n",
    "if verbose:\n",
    "    print(\"cm_input.shape: \", flat.shape)\n",
    "    print(\"cube.shape: \", cube.shape)\n",
    "    print(\"DEH.plot_size\", DEH.plot_size)\n",
    "    print(\"saturated_input.shape\", saturated.shape)\n",
    "\n",
    "#random Init\n",
    "if verbose:\n",
    "    print(\"Quick nn\")\n",
    "DEH.neighbors = deh.quick_nn(flat.reshape(DEH.plot_size + (-1,)), k_size=1).flatten()\n",
    "\n",
    "if verbose: \n",
    "    print(\"Setting neighbor weights\")\n",
    "DEH.set_neighbor_weights_memory_efficient(flat)\n",
    "if verbose:\n",
    "    print(\"Random init\")\n",
    "DEH.random_init(flat, 256, seed=np.random.randint(0, 1000))\n",
    "if verbose:\n",
    "    print(\"Simple predict\")\n",
    "DEH_pred=DEH.simple_predict(flat)\n",
    "\n",
    "frontier_nodes = tree_walk(DEH, n=8, verbose=True)\n",
    "print(f\"Frontier nodes: {frontier_nodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a4933e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEH.save(os.path.join(DEH_DIR, save_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a7ca62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the pruning function on DEH_LOAD with the frontier nodes\n",
    "trim_tree_to_frontier_nodes(DEH, frontier_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e169b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEH.save(os.path.join(DEH_DIR, save_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5cc911",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stabelized\n",
    "print(\"Quick nn\")\n",
    "DEH.neighbors = deh.quick_nn(flat.reshape(DEH.plot_size + (-1,)), k_size=1).flatten()\n",
    "\n",
    "print(\"Setting neighbor weights\")\n",
    "DEH.set_neighbor_weights_memory_efficient(flat)\n",
    "\n",
    "#set saturated pixels to 0\n",
    "DEH.full_weights[saturated]=0\n",
    "DEH.PAA_backcount=1\n",
    "\n",
    "print(\"Accepted network stablization\")\n",
    "#accepted_network_stablization saves the model to the save_name\n",
    "DEH.accepted_network_stablization(flat, n_runs=10, n_pts=(1000,10000), obj_record=(), sampling_points=(), mpp_tol=0.2, step_delta=0.05, reg_max=0.2, name=save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9c0df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print nodes by depth for the original and pruned DEH objects\n",
    "print_nodes_by_depth(DEH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hypso",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
