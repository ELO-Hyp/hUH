{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0c6d0f7",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a80901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib as il\n",
    "from hypso import Hypso1, Hypso2\n",
    "import src.deh as deh\n",
    "import copy\n",
    "import gc\n",
    "il.reload(deh)\n",
    "sys.path.append(os.path.abspath(\"D:/Hierarchical Unmixing Label\"))\n",
    "HYPSO_WIDTH=598\n",
    "HYPSO_HEIGHT=1092\n",
    "HYPSO_BANDS=112 # using bands 6:118 of 120"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e85e33",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bdb61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions\n",
    "def plot_cm_rgb_composite(cm, title=\"\", red_band_index=69, green_band_index=46, blue_band_index=26, aspect=0.1, figsize=(10, 10), height=1092, contrast_enhancement=False):\n",
    "    \"\"\"\n",
    "    Create and plot an RGB composite image from a hyperspectral cube.\n",
    "    \n",
    "    Parameters:\n",
    "    - cm: Input hyperspectral data cube or array (can be flattened (h*w, n_bands) or 3D (h, w, n_bands))\n",
    "    - title: Optional title for the plot\n",
    "    - red_band_index: Index of the band to use for red channel (default: 69)\n",
    "    - green_band_index: Index of the band to use for green channel (default: 46)\n",
    "    - blue_band_index: Index of the band to use for blue channel (default: 26)\n",
    "    - aspect: Aspect ratio for the plot (default: 0.1)\n",
    "    - figsize: Figure size as tuple (width, height) in inches (default: (10, 10))\n",
    "    - height: Height of the image when reshaping from flattened data (default: 1092)\n",
    "    \n",
    "    Returns:\n",
    "    - rgb_image: The processed RGB image\n",
    "    \"\"\"\n",
    "    # Check if input is already a 3D cube or needs reshaping\n",
    "    if len(cm.shape) == 2:  # Flattened data (h*w, n_bands)\n",
    "        width = cm.shape[0] // height\n",
    "        data_cube = cm.reshape(width, height, cm.shape[1])\n",
    "        data_cube = np.transpose(data_cube, (0, 1, 2))  # Ensure correct orientation\n",
    "    elif len(cm.shape) == 3:  # Already a cube (h, w, n_bands)\n",
    "        data_cube = cm\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected input shape: {cm.shape}. Expected 2D or 3D array.\")\n",
    "    \n",
    "    # Extract the specified bands for RGB channels\n",
    "    red_band = data_cube[:, :, red_band_index]\n",
    "    green_band = data_cube[:, :, green_band_index]\n",
    "    blue_band = data_cube[:, :, blue_band_index]\n",
    "\n",
    "    # Stack the bands to create an RGB image\n",
    "    rgb_image = np.stack((red_band, green_band, blue_band), axis=-1)\n",
    "    \n",
    "    # Process data for better visualization\n",
    "    # Replace NaN values with 0\n",
    "    rgb_image[np.isnan(rgb_image)] = 0\n",
    "\n",
    "    # Apply normalization to each channel\n",
    "    for i in range(3):\n",
    "        channel = rgb_image[:,:,i]\n",
    "        \n",
    "        # Always use min-max normalization regardless of value range\n",
    "        min_val = np.nanmin(channel)\n",
    "        max_val = np.nanmax(channel)\n",
    "        \n",
    "        if max_val > min_val:  # Avoid division by zero\n",
    "            # Normalize to [0,1] range\n",
    "            channel = (channel - min_val) / (max_val - min_val)\n",
    "            \n",
    "            # Apply contrast enhancement if requested\n",
    "            if contrast_enhancement and np.any(channel > 0):\n",
    "                # Only enhance contrast if we have enough non-zero values\n",
    "                non_zero_values = channel[channel > 0]\n",
    "                if len(non_zero_values) > 10:  # Arbitrary threshold\n",
    "                    percentiles = np.nanpercentile(channel, [2, 98])\n",
    "                    p_low, p_high = percentiles[0], percentiles[1]\n",
    "                    if p_high > p_low:\n",
    "                        channel = np.clip(channel, p_low, p_high)\n",
    "                        channel = (channel - p_low) / (p_high - p_low)\n",
    "            \n",
    "            rgb_image[:,:,i] = channel\n",
    "    \n",
    "    # Final normalization and cleanup\n",
    "    rgb_image = np.clip(rgb_image, 0, 1)\n",
    "    \n",
    "    # Rotate for proper orientation\n",
    "    rgb_image = np.rot90(rgb_image)\n",
    "    \n",
    "    # Create and display the plot\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.imshow(rgb_image, aspect=aspect)\n",
    "    plt.axis('off')  # Hide axes for cleaner visualization\n",
    "    \n",
    "    return rgb_image\n",
    "\n",
    "def create_rgb_composite_from_satobj(satobj, title=\"\", cube=\"L1A\", red_band_index=69, green_band_index=46, blue_band_index=26, aspect=0.1, figsize=(10, 10), display=True, contrast_enhancement=False ):\n",
    "    \"\"\"\n",
    "    Create an RGB composite image from a satellite object (satobj).\n",
    "\n",
    "    Parameters:\n",
    "    - satobj: An instance of the satellite object containing the image data.\n",
    "    - red_band_index: The index of the band to use for the red channel (default: 69).\n",
    "    - green_band_index: The index of the band to use for the green channel (default: 46).\n",
    "    - blue_band_index: The index of the band to use for the blue channel (default: 26).\n",
    "    - aspect: Aspect ratio for the plot (default: 0.1).\n",
    "    - figsize: Figure size as tuple (width, height) in inches (default: (10, 10)).\n",
    "    - display: Whether to display the image (default: True).\n",
    "    - cube: Which data cube to use (\"L1A\", \"L1B\", \"L1C\", or \"L1D\") (default: \"L1A\").\n",
    "\n",
    "    Returns:\n",
    "    - rgb_image: A 3D numpy array representing the RGB composite image.\n",
    "    \"\"\"\n",
    "    # Get the data cube from the satellite object\n",
    "    if cube ==\"L1A\":\n",
    "        data_cube = satobj.l1a_cube  \n",
    "    elif cube ==\"L1B\":\n",
    "        data_cube = satobj.l1b_cube  \n",
    "    elif cube ==\"L1C\":\n",
    "        data_cube = satobj.l1c_cube \n",
    "    elif cube ==\"L1D\":\n",
    "        data_cube = satobj.l1d_cube\n",
    "    else:\n",
    "        print(\"cube not valid, defaulting tp L1A\")\n",
    "        data_cube = satobj.l1a_cube     \n",
    "    \n",
    "    # Extract the specified bands for RGB channels\n",
    "    red_band = data_cube[:, :, red_band_index]\n",
    "    green_band = data_cube[:, :, green_band_index]\n",
    "    blue_band = data_cube[:, :, blue_band_index]\n",
    "\n",
    "    # Stack the bands to create an RGB image\n",
    "    rgb_image = np.stack((red_band, green_band, blue_band), axis=-1)\n",
    "\n",
    "    # Handle NaN values\n",
    "    rgb_image[np.isnan(rgb_image)] = 0\n",
    "\n",
    "    # Apply normalization to each channel\n",
    "    for i in range(3):\n",
    "        channel = rgb_image[:,:,i]\n",
    "        \n",
    "        # Always use min-max normalization regardless of value range\n",
    "        min_val = np.nanmin(channel)\n",
    "        max_val = np.nanmax(channel)\n",
    "        \n",
    "        if max_val > min_val:  # Avoid division by zero\n",
    "            # Normalize to [0,1] range\n",
    "            channel = (channel - min_val) / (max_val - min_val)\n",
    "            \n",
    "            # Apply contrast enhancement if requested\n",
    "            if contrast_enhancement and np.any(channel > 0):\n",
    "                # Only enhance contrast if we have enough non-zero values\n",
    "                non_zero_values = channel[channel > 0]\n",
    "                if len(non_zero_values) > 10:  # Arbitrary threshold\n",
    "                    percentiles = np.nanpercentile(channel, [2, 98])\n",
    "                    p_low, p_high = percentiles[0], percentiles[1]\n",
    "                    if p_high > p_low:\n",
    "                        channel = np.clip(channel, p_low, p_high)\n",
    "                        channel = (channel - p_low) / (p_high - p_low)\n",
    "            \n",
    "            rgb_image[:,:,i] = channel\n",
    "    \n",
    "    # Final normalization and cleanup\n",
    "    rgb_image = np.clip(rgb_image, 0, 1)\n",
    "    \n",
    "    # Create properly oriented image for display\n",
    "    display_image = np.rot90(rgb_image)\n",
    "    \n",
    "    # Display the image if requested\n",
    "    if display:\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        ax.imshow(display_image, aspect=aspect, interpolation='nearest')\n",
    "        ax.set_title(title+\" \"+cube)\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    return rgb_image\n",
    "\n",
    "def plot_DEH_overlay(DEH_input, rgb_image, node, opacity=0.5):\n",
    "    \"\"\"\n",
    "    Plot and return the overlay image of DEH figure on top of an RGB image with controllable opacity.\n",
    "    Non-zero DEH values will be shown with the specified opacity, while zero values will be fully transparent.\n",
    "    Both normal and inverse overlays will be shown side by side.\n",
    "\n",
    "    Parameters:\n",
    "    - DEH_input: The DEH input object containing the node data\n",
    "    - rgb_image: The RGB image to overlay the DEH figure on, shape (w, h, 3)\n",
    "    - node: The node to visualize\n",
    "    - opacity: The opacity of the DEH figure where values are non-zero (0.0 to 1.0)\n",
    "\n",
    "    Returns:\n",
    "    - overlay_image: The RGB image with the DEH figure overlaid\n",
    "    \"\"\"\n",
    "    deh_figure = DEH_input.nodes[node].map.reshape(DEH_input.plot_size)\n",
    "    \n",
    "    # Normalize both the DEH figure and the RGB image to the range [0, 1]\n",
    "    deh_figure_normalized = (deh_figure - np.min(deh_figure)) / (np.max(deh_figure) - np.min(deh_figure))\n",
    "    rgb_image_normalized = (rgb_image - np.min(rgb_image)) / (np.max(rgb_image) - np.min(rgb_image))\n",
    "\n",
    "    # Create a red overlay\n",
    "    red_overlay = np.ones_like(deh_figure_normalized)[:, :, np.newaxis] * [1, 0, 0]  # Red color\n",
    "\n",
    "    # Create normal overlay\n",
    "    overlay_image = rgb_image_normalized.copy()\n",
    "    non_zero_mask = deh_figure != 0\n",
    "    overlay_image[non_zero_mask] = (rgb_image_normalized[non_zero_mask] * (1 - opacity) + \n",
    "                                  red_overlay[non_zero_mask] * opacity * deh_figure_normalized[non_zero_mask, np.newaxis])\n",
    "\n",
    "    # Create inverse overlay\n",
    "    inverse_overlay = rgb_image_normalized.copy()\n",
    "    inverse_mask = deh_figure == 0\n",
    "    inverse_overlay[inverse_mask] = (rgb_image_normalized[inverse_mask] * (1 - opacity) + \n",
    "                                   red_overlay[inverse_mask] * opacity)\n",
    "\n",
    "    # Plot both overlays side by side\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(50, 200), gridspec_kw={'wspace': 0.1, 'hspace': 0.1})\n",
    "    \n",
    "    axs[0].imshow(np.rot90(overlay_image), aspect=DEH_input.plot_aspect, vmin=0, vmax=1, interpolation='bicubic')\n",
    "    axs[0].set_title(f\"Node: {node} - Original\")\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    axs[1].imshow(np.rot90(inverse_overlay), aspect=DEH_input.plot_aspect, vmin=0, vmax=1, interpolation='bicubic')\n",
    "    axs[1].set_title(f\"Node: {node} - Inverse\")\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return overlay_image\n",
    "\n",
    "def DEH_init_stabelize(DEH_input, cm_input, saturated_input, save_name, n_endmembers=8, n_runs=20, verbose=True):\n",
    "    #Init DEH\n",
    "    DEH_input.splitting_size=1000\n",
    "    DEH_input.max_depth=2\n",
    "    DEH_input.max_iter=5\n",
    "    DEH_input.max_nodes=2\n",
    "\n",
    "\n",
    "    cube=cm_input.reshape(-1,1092,112)\n",
    "    #these set the size of the image to be plotted\n",
    "    DEH_input.plot_size = (cube.shape[0],cube.shape[1])\n",
    "    DEH_input.plot_aspect = 0.1\n",
    "\n",
    "    # these set the normaliztion on the data, but right now it is turned off and replaced with a nearest-neighbor noise estimate\n",
    "    DEH_input.weight_power=0\n",
    "    DEH_input.eps = 0.00\n",
    "    #dehx.wf = lambda x: (np.sum(x**2, axis=-1))**(-1/2+1/(2+dehx.eps))\n",
    "\n",
    "    #this switches between Archetype Analysis and Pure Pixel Analysis\n",
    "    DEH_input.aa = False\n",
    "\n",
    "    #this is currently replaced by \"mpp_tol\" in the training code\n",
    "    DEH_input.mixed_pix = 0\n",
    "\n",
    "    # this is the prefactor on the L2 regularization\n",
    "    DEH_input.reg=0\n",
    "\n",
    "    # if this is anything other than zero, it introduces L2 regularization on the trained weights \n",
    "    DEH_input.set_mu(0)\n",
    "\n",
    "    # this turns on a sort of protection to keep endmembers from vanishing. Generally unneeded with new training\n",
    "    DEH_input.use_bonus_boost = False\n",
    "\n",
    "    # turns on normalization of the data. Generally always used\n",
    "    DEH_input.use_norm(True)\n",
    "\n",
    "    # if greater than 1, turns on a sort of spectral sampling with PAA. Occasionally gives good performance\n",
    "    # but I generally keep it at 1 to turn it off\n",
    "    DEH_input.PAA_backcount = 1\n",
    "\n",
    "    # changes the gradient descent learning rate as a proportion of the optimal rate. I recommend keeping it at 1\n",
    "    DEH_input.a_speed= 1.0\n",
    "\n",
    "    #Verbose print\n",
    "    if verbose:\n",
    "        print(\"cm_input.shape: \", cm_input.shape)\n",
    "        print(\"cube.shape: \", cube.shape)\n",
    "        print(\"DEH_input.plot_size\", DEH_input.plot_size)\n",
    "        print(\"saturated_input.shape\", saturated_input.shape)\n",
    "        print(\"Quick nn\")\n",
    "    \n",
    "    #random Init\n",
    "    DEH_input.neighbors = deh.quick_nn(cm_input.reshape(DEH_input.plot_size + (-1,)), k_size=1).flatten()\n",
    "    if verbose: \n",
    "        print(\"Setting neighbor weights\")\n",
    "    DEH_input.set_neighbor_weights(cm_input)\n",
    "    if verbose:\n",
    "        print(\"Random init\")\n",
    "    DEH_input.random_init(cm_input, n_endmembers)\n",
    "    if verbose:\n",
    "        print(\"Simple predict\")\n",
    "    deh_pred=DEH_input.simple_predict(cm_input)\n",
    "    if verbose:\n",
    "        print(\"Getting trimmed network\")\n",
    "\n",
    "    #Stabelized\n",
    "    DEH_stabelized=DEH_input.get_trimmed_network(DEH_input.nodes) #Can replace with copy of DEH_input instead of get trimmed network since it acceptes all nodes\n",
    "    if verbose:\n",
    "        print(\"Quick nn\")\n",
    "    DEH_stabelized.neighbors = deh.quick_nn(cm_input.reshape(DEH_stabelized.plot_size + (-1,)), k_size=1).flatten()\n",
    "    if verbose:\n",
    "        print(\"Setting neighbor weights\")\n",
    "\n",
    "    DEH_stabelized.set_neighbor_weights(cm_input)\n",
    "    if verbose:\n",
    "        print(\"DEH_stabelized.full_weights.shape\",DEH_stabelized.full_weights.shape)\n",
    "    #saturated_input = np.asarray(saturated_input, dtype=bool)\n",
    "    DEH_stabelized.full_weights[saturated_input]=0\n",
    "    DEH_stabelized.PAA_backcount=1\n",
    "\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Accepted network stablization\")\n",
    "    DEH_stabelized.accepted_network_stablization(cm_input, n_runs=n_runs, n_pts=(1000,10000), obj_record=(), sampling_points=(), mpp_tol=0.2, step_delta=0.01, reg_max=0.2, name=save_name)\n",
    "\n",
    "    return DEH_input, DEH_stabelized\n",
    "\n",
    "def plot_level_and_overlay(save_file_path, cm_input, plot_only_last_level=False, binarize=False, opacity=0.5):\n",
    "    DEH_temp = deh.DEH(no_negative_residuals=True)\n",
    "    DEH_temp.load(save_file_path)\n",
    "    cube_image=cm_input.reshape(-1,1092,112)\n",
    "\n",
    "    DEH_temp.plot_size = (cube_image.shape[0],cube_image.shape[1])\n",
    "    DEH_temp.verbose = False\n",
    "    deh_predicted = DEH_temp.simple_predict(cm_input)\n",
    "    rgb_image=plot_cm_rgb_composite(cm_input)\n",
    "    nodes=DEH_temp.nodes\n",
    "    num_levels = max(len(node) for node in DEH_temp.nodes)\n",
    "    levels_to_plot = [num_levels] if plot_only_last_level else range(1, num_levels + 1)\n",
    "    if binarize:\n",
    "        DEH_temp.binarize_lmdas()\n",
    "        DEH_temp.lmda_2_map()\n",
    "    \n",
    "    for level in levels_to_plot:\n",
    "        DEH_temp.display_level(level)\n",
    "        for node in nodes:\n",
    "            if len(node) == level:\n",
    "                overlay_image = plot_DEH_overlay(DEH_temp, rgb_image, node=node, opacity=opacity)\n",
    "\n",
    "def plot_spectra_for_level(save_file_path, cm_input, level_input=\"\"):\n",
    "    DEH_temp = deh.DEH(no_negative_residuals=True)\n",
    "    DEH_temp.load(save_file_path)\n",
    "    cube_image=cm_input.reshape(-1,1092,112)\n",
    "\n",
    "    DEH_temp.plot_size = (cube_image.shape[0],cube_image.shape[1])\n",
    "    DEH_temp.simple_predict(cm_input)\n",
    "    nodes = DEH_temp.nodes\n",
    "    num_levels = max(len(node) for node in DEH_temp.nodes)\n",
    "    \n",
    "    # If level_input is empty or invalid, use the last level\n",
    "    if level_input == \"\" or not isinstance(level_input, int) or level_input > num_levels or level_input < 1:\n",
    "        level = num_levels\n",
    "    else:\n",
    "        level = level_input\n",
    "        \n",
    "    # Get nodes at specified level\n",
    "    level_nodes = [node for node in nodes if len(node) == level]\n",
    "    print(\"using level\", level, \"with\", len(level_nodes), \"nodes\")\n",
    "    DEH_temp.display_spectra(level_nodes)\n",
    "\n",
    "def binarize_DEH(save_file_path, cm_input):\n",
    "    DEH_temp = deh.DEH(no_negative_residuals=True)\n",
    "    DEH_temp.load(save_file_path)\n",
    "    cube_image=cm_input.reshape(-1,1092,112)\n",
    "\n",
    "    DEH_temp.plot_size = (cube_image.shape[0],cube_image.shape[1])\n",
    "    DEH_temp.simple_predict(cm_input)\n",
    "    DEH_binarized=copy.deepcopy(DEH_temp)\n",
    "    DEH_binarized.binarize_lmdas()\n",
    "    DEH_binarized.lmda_2_map()\n",
    "\n",
    "    return DEH_binarized\n",
    "\n",
    "def load_DEH(save_file_path):\n",
    "    DEH_temp = deh.DEH(no_negative_residuals=True)\n",
    "    DEH_temp.load(save_file_path)\n",
    "    return DEH_temp\n",
    "\n",
    "def save_binarized_labels_to_npy(deh_path, data, save_path=None):\n",
    "    \"\"\"\n",
    "    Save binarized labels from DEH model to a .npy file.\n",
    "    Labels are stored as a dictionary with node names as keys and binary arrays as values.\n",
    "    \n",
    "    Args:\n",
    "        deh_model: The DEH model with binarized labels\n",
    "        data: Input data used for prediction \n",
    "        output_path: Path to save the output .npy file\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    deh_binarized=deh.DEH(no_negative_residuals=True)\n",
    "    deh_binarized.load(deh_path)\n",
    "    # Get predictions and binarize\n",
    "    print(data.shape)\n",
    "    cube=data.reshape(-1,1092,112)\n",
    "    deh_binarized.plot_size=(cube.shape[0],cube.shape[1])\n",
    "    print(deh_binarized.plot_size)\n",
    "    deh_binarized.simple_predict(data)\n",
    "    deh_binarized.binarize_lmdas()\n",
    "    deh_binarized.lmda_2_map()\n",
    "    \n",
    "    # Create dictionary to store labels\n",
    "    labels_dict = {}\n",
    "    \n",
    "    # Store binary labels for each node\n",
    "    for node in deh_binarized.nodes:\n",
    "        # Get binary labels from node.map which already contains the binarized values\n",
    "        labels = deh_binarized.nodes[node].map.flatten()\n",
    "        labels_dict[node] = labels\n",
    "        \n",
    "    # Save dictionary to .npy file\n",
    "    # Use provided savepath if available, otherwise create from deh_path\n",
    "    if save_path is not None:\n",
    "        output_path = save_path\n",
    "    else:\n",
    "        # Create output path based on deh_path\n",
    "        # Replace file extension and add '_labels' to the filename\n",
    "        base_path = os.path.splitext(deh_path)[0]\n",
    "        output_path = base_path + '_labels.npy'\n",
    "    \n",
    "    print(f\"Saving binarized labels to: {output_path}\")\n",
    "    np.save(output_path, labels_dict)\n",
    "\n",
    "def plot_level_labels(npy_file, level=None):\n",
    "    \"\"\"\n",
    "    Plot binary labels for nodes at specified level, highlighting overlapping regions.\n",
    "    \n",
    "    Args:\n",
    "        npy_file: Path to .npy file containing binary labels\n",
    "        level: Level of nodes to plot (based on key length). If None, uses max level.\n",
    "    \"\"\"\n",
    "    # Load labels\n",
    "    labels = np.load(npy_file, allow_pickle=True).item()\n",
    "    \n",
    "    # Get all keys and their lengths\n",
    "    key_lengths = [len(k) for k in labels.keys()]\n",
    "    max_level = max(key_lengths)\n",
    "    \n",
    "    # If level not specified or invalid, use max level\n",
    "    if level is None or level > max_level:\n",
    "        level = max_level\n",
    "        \n",
    "    # Get keys for specified level\n",
    "    level_keys = [k for k in labels.keys() if len(k) == level]\n",
    "    \n",
    "    if not level_keys:\n",
    "        print(f\"No nodes found at level {level}\")\n",
    "        return\n",
    "        \n",
    "    # Setup plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    \n",
    "    # Create combined array to track overlaps\n",
    "    first_key = level_keys[0]\n",
    "    combined_labels = np.zeros(labels[first_key].reshape(-1, 1092).shape)\n",
    "    \n",
    "    # First pass - count overlaps\n",
    "    for key in level_keys:\n",
    "        plot_label = labels[key].reshape(-1, 1092)\n",
    "        combined_labels += plot_label\n",
    "    \n",
    "    # Plot each node's labels\n",
    "    for i, key in enumerate(level_keys):\n",
    "        plot_label = labels[key].reshape(-1, 1092)\n",
    "        # Create custom colormap for this node\n",
    "        colors = [(1,1,1,0), plt.cm.rainbow(i/len(level_keys))]  # Transparent white to color\n",
    "        node_cmap = plt.matplotlib.colors.LinearSegmentedColormap.from_list(f'custom_{i}', colors)\n",
    "        alpha = 0.5 if np.any(combined_labels > 1) else 1.0\n",
    "        plt.imshow(np.rot90(plot_label), aspect=0.1, alpha=alpha, cmap=node_cmap)\n",
    "    \n",
    "    # Plot overlaps with a different color if they exist\n",
    "    if np.any(combined_labels > 1):\n",
    "        overlap_mask = combined_labels > 1\n",
    "        overlap_display = np.rot90(overlap_mask.astype(float))\n",
    "        plt.imshow(overlap_display, aspect=0.1, alpha=0.7, cmap='Reds', \n",
    "                   label='Overlapping Regions')\n",
    "    \n",
    "    plt.title(f'Binary Labels for Level {level} Nodes')\n",
    "    \n",
    "    # Add legend\n",
    "    legend_elements = [plt.Rectangle((0,0),1,1, facecolor=plt.cm.rainbow(i/len(level_keys))) \n",
    "                      for i in range(len(level_keys))]\n",
    "    if np.any(combined_labels > 1):\n",
    "        legend_elements.append(plt.Rectangle((0,0),1,1, facecolor='red', alpha=0.7))\n",
    "        legend_labels = list(level_keys) + ['Overlaps']\n",
    "    else:\n",
    "        legend_labels = list(level_keys)\n",
    "    ax.legend(legend_elements, legend_labels, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def find_top_endmembers(deh_obj, n=8, plot=False, verbose=False):\n",
    "    \"\"\"\n",
    "    Find the n endmembers with the highest abundance in the DEH object.\n",
    "    \n",
    "    Args:\n",
    "        deh_obj: The DEH object to analyze\n",
    "        n: Number of top endmembers to return (default: 8)\n",
    "        plot: Whether to plot the top endmembers (default: True)\n",
    "        verbose: Whether to print detailed information (default: False)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of top endmembers with their node IDs and abundance values\n",
    "    \"\"\"\n",
    "    # Get the deepest level nodes (endmembers)\n",
    "    max_level = max(len(node_id) for node_id in deh_obj.nodes.keys())\n",
    "    endmember_nodes = {node_id: node for node_id, node in deh_obj.nodes.items() if len(node_id) == max_level}\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Total endmembers at level {max_level}: {len(endmember_nodes)}\")\n",
    "    \n",
    "    # Calculate abundance for each endmember\n",
    "    abundances = {}\n",
    "    for node_id, node in endmember_nodes.items():\n",
    "        if hasattr(node, 'map'):\n",
    "            abundances[node_id] = deh_obj.nodes[node_id].map.sum()\n",
    "        elif hasattr(node, 'abundance'):\n",
    "            abundances[node_id] = node.abundance\n",
    "        elif hasattr(node, 'weight'):\n",
    "            abundances[node_id] = node.weight\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f\"Warning: Node {node_id} has no map, abundance or weight attribute\")\n",
    "    \n",
    "    # Sort by abundance and get top n\n",
    "    top_endmembers = dict(sorted(abundances.items(), key=lambda x: x[1], reverse=True)[:n])\n",
    "    \n",
    "    # Print results if verbose\n",
    "    if verbose:\n",
    "        print(f\"\\nTop {n} endmembers by abundance:\")\n",
    "        for i, (node_id, abundance) in enumerate(top_endmembers.items(), 1):\n",
    "            print(f\"{i}. Node {node_id}: Abundance = {abundance:.4f}\")\n",
    "    \n",
    "    return top_endmembers\n",
    "\n",
    "def print_nodes_by_depth(deh_obj):\n",
    "    \"\"\"\n",
    "    Print all nodes in the DEH object organized by depth.\n",
    "    \n",
    "    Args:\n",
    "        deh_obj: The DEH object containing nodes\n",
    "    \"\"\"\n",
    "    # Get all nodes\n",
    "    all_nodes = list(deh_obj.nodes.keys())\n",
    "    \n",
    "    # Group nodes by depth\n",
    "    nodes_by_depth = {}\n",
    "    for node_id in all_nodes:\n",
    "        depth = len(node_id)\n",
    "        if depth not in nodes_by_depth:\n",
    "            nodes_by_depth[depth] = []\n",
    "        nodes_by_depth[depth].append(node_id)\n",
    "    \n",
    "    # Print nodes by depth\n",
    "    print(\"Nodes by depth:\")\n",
    "    for depth in sorted(nodes_by_depth.keys()):\n",
    "        node_count = len(nodes_by_depth[depth])\n",
    "        print(f\"Depth {depth}: {node_count} nodes\")\n",
    "        # Print the first 10 nodes at this depth as examples\n",
    "        example_nodes = nodes_by_depth[depth][:10]\n",
    "        if example_nodes:\n",
    "            print(f\"  Examples: {', '.join(example_nodes)}\")\n",
    "        if node_count > 10:\n",
    "            print(f\"  ... and {node_count - 10} more\")\n",
    "        print()\n",
    "\n",
    "def tree_walk(deh_obj, n=10, verbose=False):\n",
    "    \"\"\"\n",
    "    Performs a tree walk starting from the root of a DEH object.\n",
    "    The frontier expands by adding children of visited nodes.\n",
    "    The next node to visit is selected based on the highest sum value.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    deh_obj : DEH object\n",
    "        The hierarchical unmixing object to traverse\n",
    "    n : int, default=10\n",
    "        The number of nodes to have in the frontier before stopping\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        The nodes in the frontier when the threshold is reached\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Tree walking...\")\n",
    "    # Initialize with root node\n",
    "    visited = []\n",
    "    frontier = ['']  # Start with root node\n",
    "    \n",
    "    # Continue until frontier reaches desired size\n",
    "    while len(frontier) < n:\n",
    "        if not frontier:\n",
    "            break\n",
    "            \n",
    "        # Find node with highest sum in frontier\n",
    "        max_sum = -float('inf')\n",
    "        max_node = None\n",
    "        max_idx = -1\n",
    "        \n",
    "        for i, node_id in enumerate(frontier):\n",
    "            node_sum = deh_obj.nodes[node_id].map.sum()\n",
    "            if node_sum > max_sum:\n",
    "                max_sum = node_sum\n",
    "                max_node = node_id\n",
    "                max_idx = i\n",
    "        \n",
    "        # Remove the selected node from frontier and add to visited\n",
    "        frontier.pop(max_idx)\n",
    "        visited.append(max_node)\n",
    "        \n",
    "        # Add children to frontier\n",
    "        # Children of node 'x' are 'x0' and 'x1'\n",
    "        left_child = max_node + '0'\n",
    "        right_child = max_node + '1'\n",
    "        \n",
    "        # Check if children exist in the model\n",
    "        if left_child in deh_obj.nodes:\n",
    "            frontier.append(left_child)\n",
    "        if right_child in deh_obj.nodes:\n",
    "            frontier.append(right_child)\n",
    "    \n",
    "    # Calculate sum of all nodes in frontier\n",
    "    frontier_sum = 0\n",
    "    for node_id in frontier:\n",
    "        frontier_sum += deh_obj.nodes[node_id].map.sum()\n",
    "    \n",
    "    # Calculate root node sum for comparison\n",
    "    root_sum = deh_obj.nodes[''].map.sum()\n",
    "    \n",
    "    # Print results\n",
    "    if verbose:\n",
    "        print(f\"Frontier nodes when threshold of {n} nodes reached:\")\n",
    "        for node_id in frontier:\n",
    "            level = len(node_id)  # Level is determined by the length of the node_id\n",
    "            print(f\"Node: {node_id}, Level: {level}, Sum: {deh_obj.nodes[node_id].map.sum():.4f}\")\n",
    "        print(f\"Total sum of frontier nodes: {frontier_sum:.4f}\")\n",
    "        print(f\"Root node sum: {root_sum:.4f}\")\n",
    "        print(f\"Difference: {root_sum - frontier_sum:.4f}\")\n",
    "    \n",
    "    return frontier\n",
    "\n",
    "def prune_deh_obj(deh_obj, node_list):\n",
    "    \"\"\"\n",
    "    Prune a DEH object by keeping only the nodes in the given list and their ancestors,\n",
    "    after extending all nodes to the same depth.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    deh_obj : DEH object\n",
    "        The DEH object to prune\n",
    "    node_list : list\n",
    "        List of node IDs to keep (frontier nodes)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pruned_deh_obj : DEH object\n",
    "        The pruned DEH object\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find the maximum depth of nodes in the node_list\n",
    "    max_depth = max(len(node_id) for node_id in node_list)\n",
    "    \n",
    "    # Extend all nodes to the same depth by adding '0's\n",
    "    extended_node_list = []\n",
    "    for node_id in node_list:\n",
    "        if len(node_id) < max_depth:\n",
    "            # Extend the node_id by adding '0's until it reaches max_depth\n",
    "            extended_node_id = node_id + '0' * (max_depth - len(node_id))\n",
    "            extended_node_list.append(extended_node_id)\n",
    "        else:\n",
    "            extended_node_list.append(node_id)\n",
    "    \n",
    "    # Create a list of all nodes in the DEH object\n",
    "    all_nodes = list(deh_obj.nodes.keys())\n",
    "    \n",
    "    # Create a set of nodes to keep (extended frontier nodes and their ancestors)\n",
    "    nodes_to_keep = set(extended_node_list)\n",
    "    \n",
    "    # Add ancestors of frontier nodes to the keep list\n",
    "    for node_id in extended_node_list:\n",
    "        # Add all prefixes of the node_id (ancestors)\n",
    "        for i in range(len(node_id)):\n",
    "            nodes_to_keep.add(node_id[:i])\n",
    "    \n",
    "    # Add the root node\n",
    "    nodes_to_keep.add('')\n",
    "    \n",
    "    # Create a list of nodes to delete\n",
    "    nodes_to_delete = []\n",
    "    for node_id in all_nodes:\n",
    "        if node_id not in nodes_to_keep:\n",
    "            # Check if any ancestor is already in the delete list\n",
    "            # If so, we don't need to add this node as it will be deleted with its ancestor\n",
    "            ancestor_in_delete_list = False\n",
    "            for i in range(1, len(node_id)):\n",
    "                if node_id[:i] in nodes_to_delete:\n",
    "                    ancestor_in_delete_list = True\n",
    "                    break\n",
    "            \n",
    "            if not ancestor_in_delete_list:\n",
    "                nodes_to_delete.append(node_id)\n",
    "    \n",
    "    # Sort nodes to delete by depth (delete deepest nodes first)\n",
    "    nodes_to_delete.sort(key=len, reverse=True)\n",
    "    \n",
    "    # Delete the nodes one by one\n",
    "    deleted_count = 0\n",
    "    for node_id in nodes_to_delete:\n",
    "        try:\n",
    "            if node_id in deh_obj.nodes:  # Check if node still exists\n",
    "                # Delete the node (this will delete the node and its descendants)\n",
    "                deh_obj.delete_node(node_id)\n",
    "                deleted_count += 1\n",
    "        except KeyError as e:\n",
    "            # Node might have been already deleted\n",
    "            print(f\"Note: Node {node_id} was already deleted or caused error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Pruned {deleted_count} nodes and its decendants from the DEH object\")\n",
    "    print(f\"Extended node list to depth {max_depth}: {extended_node_list}\")\n",
    "\n",
    "def train_random_DEH(image_data,endmembers, save_name, verbose=False):\n",
    "    print(\"Training Random DEH...\")\n",
    "    DEH_random= deh.DEH(no_negative_residuals=True)\n",
    "    #Init DEH\n",
    "    DEH_random.splitting_size=1000\n",
    "    DEH_random.max_depth=2\n",
    "    DEH_random.max_iter=5\n",
    "    DEH_random.max_nodes=2\n",
    "\n",
    "\n",
    "    cube=image_data.reshape(-1,HYPSO_HEIGHT,112)\n",
    "    #these set the size of the image to be plotted\n",
    "    DEH_random.plot_size = (cube.shape[0],cube.shape[1])\n",
    "    DEH_random.plot_aspect = 0.1\n",
    "\n",
    "    # these set the normaliztion on the data, but right now it is turned off and replaced with a nearest-neighbor noise estimate\n",
    "    DEH_random.weight_power=0\n",
    "    DEH_random.eps = 0.00\n",
    "    #dehx.wf = lambda x: (np.sum(x**2, axis=-1))**(-1/2+1/(2+dehx.eps))\n",
    "\n",
    "    #this switches between Archetype Analysis and Pure Pixel Analysis\n",
    "    DEH_random.aa = False\n",
    "\n",
    "    #this is currently replaced by \"mpp_tol\" in the training code\n",
    "    DEH_random.mixed_pix = 0\n",
    "\n",
    "    # this is the prefactor on the L2 regularization\n",
    "    DEH_random.reg=0\n",
    "\n",
    "    # if this is anything other than zero, it introduces L2 regularization on the trained weights \n",
    "    DEH_random.set_mu(0)\n",
    "\n",
    "    # this turns on a sort of protection to keep endmembers from vanishing. Generally unneeded with new training\n",
    "    DEH_random.use_bonus_boost = False\n",
    "\n",
    "    # turns on normalization of the data. Generally always used\n",
    "    DEH_random.use_norm(True)\n",
    "\n",
    "    # if greater than 1, turns on a sort of spectral sampling with PAA. Occasionally gives good performance\n",
    "    # but I generally keep it at 1 to turn it off\n",
    "    DEH_random.PAA_backcount = 1\n",
    "\n",
    "    # changes the gradient descent learning rate as a proportion of the optimal rate. I recommend keeping it at 1\n",
    "    DEH_random.a_speed= 1.0\n",
    "\n",
    "    #Verbose print\n",
    "    if verbose:\n",
    "        print(\"cm_input.shape: \", image_data.shape)\n",
    "        print(\"cube.shape: \", cube.shape)\n",
    "        print(\"DEH_random.plot_size\", DEH_random.plot_size)\n",
    "    #random Init\n",
    "    if verbose:\n",
    "        print(\"Quick nn\")\n",
    "    DEH_random.neighbors = deh.quick_nn(image_data.reshape(DEH_random.plot_size + (-1,)), k_size=1).flatten()\n",
    "    if verbose: \n",
    "        print(\"Setting neighbor weights\")\n",
    "    DEH_random.set_neighbor_weights(image_data)\n",
    "    if verbose:\n",
    "        print(\"Random init\")\n",
    "    DEH_random.random_init(image_data, endmembers)\n",
    "    if verbose:\n",
    "        print(\"Simple predict\")\n",
    "    DEH_random_pred=DEH_random.simple_predict(image_data)\n",
    "    DEH_random.save(save_name)\n",
    "    print(\"Training Random DEH complete, saved to \", save_name)\n",
    "    \n",
    "def prune_DEH(DEH_path, image_data, endmembers, verbose=False):\n",
    "    print(\"Pruning DEH...\")\n",
    "    DEH_prune=deh.DEH(no_negative_residuals=True)\n",
    "    DEH_prune.load(DEH_path)\n",
    "    DEH_prune.verbose = verbose\n",
    "    DEH_prune_pred=DEH_prune.simple_predict(image_data)\n",
    "    frontier_nodes = tree_walk(DEH_prune, n=endmembers, verbose=verbose)\n",
    "    prune_deh_obj(DEH_prune, frontier_nodes)\n",
    "    if verbose:\n",
    "        print(\"\\nNodes by depth:\")\n",
    "        print_nodes_by_depth(DEH_prune)\n",
    "    # Update the save path to reflect pruned model with correct number of endmembers\n",
    "    save_path_parts = DEH_path.split('_')\n",
    "    for i, part in enumerate(save_path_parts):\n",
    "        if 'end' in part:\n",
    "            # Find parts containing 'end' (like '256end')\n",
    "            prefix = part.split('end')[0]  # Get the part before 'end'\n",
    "            suffix = part.split('end')[1] if len(part.split('end')) > 1 else ''  # Get the part after 'end'\n",
    "            # Replace with new endmembers value\n",
    "            save_path_parts[i] = str(endmembers) + 'end' + suffix\n",
    "        if 'random' in part.lower():\n",
    "            # Replace 'random' with 'pruned'\n",
    "            save_path_parts[i] = part.lower().replace('random', 'pruned')\n",
    "    \n",
    "    # Reconstruct the save path\n",
    "    DEH_path = '_'.join(save_path_parts)\n",
    "    if verbose:\n",
    "        print(f\"Updated save path: {DEH_path}\")\n",
    "    DEH_prune.save(DEH_path)\n",
    "    print(\"Pruning DEH complete, saved to \", DEH_path)\n",
    "    return DEH_path\n",
    "\n",
    "def stabelize_DEH(DEH_path, image_data, saturated_image_data, n_runs=10, step_delta=0.05, verbose=False):\n",
    "    print(\"Stabelizing DEH....\")\n",
    "    DEH_stabelize=deh.DEH(no_negative_residuals=True)\n",
    "    DEH_stabelize.load(DEH_path)\n",
    "    DEH_stabelize.verbose = verbose\n",
    "    if verbose:\n",
    "        print(\"Simple predict\")\n",
    "    DEH_stabelize.simple_predict(image_data)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Quick nn\")\n",
    "    DEH_stabelize.neighbors = deh.quick_nn(image_data.reshape(DEH_stabelize.plot_size + (-1,)), k_size=1).flatten()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Setting neighbor weights\")\n",
    "\n",
    "    DEH_stabelize.set_neighbor_weights(image_data)\n",
    "    if verbose:\n",
    "        print(\"DEH_stabelized.full_weights.shape\",DEH_stabelize.full_weights.shape)\n",
    "    \n",
    "    #saturated_input = np.asarray(saturated_input, dtype=bool)\n",
    "    DEH_stabelize.full_weights[saturated_image_data]=0\n",
    "    DEH_stabelize.PAA_backcount=1\n",
    "    \n",
    "    \n",
    "    # Create a new save path by replacing the last part with 'stabelized'\n",
    "    save_path_parts = DEH_path.split('_')\n",
    "    \n",
    "    # Get file extension if present\n",
    "    if '.' in save_path_parts[-1]:\n",
    "        last_part, extension = save_path_parts[-1].rsplit('.', 1)\n",
    "        # Remove .h5 extension if present\n",
    "        if extension.lower() == 'h5':\n",
    "            save_path_parts[-1] = 'stabelized'\n",
    "        else:\n",
    "            save_path_parts[-1] = 'stabelized.' + extension\n",
    "    else:\n",
    "        save_path_parts[-1] = 'stabelized'\n",
    "    \n",
    "    # Reconstruct the save path\n",
    "    save_name = '_'.join(save_path_parts)\n",
    "    if verbose:\n",
    "        print(f\"Updated save path for stabilization: {save_name}\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Accepted network stablization\")\n",
    "    DEH_stabelize.accepted_network_stablization(image_data, n_runs=n_runs, n_pts=(1000,10000), obj_record=(), sampling_points=(), mpp_tol=0.2, step_delta=step_delta, reg_max=0.2, name=save_name)\n",
    "    print(\"Stabelizing DEH complete\")\n",
    "\n",
    "def reset_deh_for_reinit(deh_model):\n",
    "    \"\"\"\n",
    "    Reset a DEH model so it can be randomly initialized again.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    deh_model : DEH\n",
    "        The DEH model to reset\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    deh_model : DEH\n",
    "        The reset DEH model ready for new initialization\n",
    "    \"\"\"\n",
    "    # Clear the tree structure\n",
    "    deh_model.nodes = {}\n",
    "    deh_model.lmdas = {}\n",
    "    deh_model.leaf_mean = {}\n",
    "    deh_model.path_to_node = {}\n",
    "    deh_model.node_to_path = {}\n",
    "    deh_model.splitting_nodes = set()\n",
    "    deh_model.weights = {}\n",
    "    deh_model.intercepts = {}\n",
    "    \n",
    "    # Clear model states\n",
    "    if hasattr(deh_model, 'models'):\n",
    "        deh_model.models = {}\n",
    "    \n",
    "    # Clear cache and results\n",
    "    deh_model.preds = None\n",
    "    deh_model.path_map = None\n",
    "    deh_model.obj = float('inf')\n",
    "    \n",
    "    # Reset key parameters (if you've modified these)\n",
    "    # deh_model.aa = False  # Only reset if you want to change this\n",
    "    # deh_model.use_norm(True)  # Only reset if you want to change this\n",
    "    \n",
    "    # Note: We're keeping neighbors and full_weights since they're \n",
    "    # dependent on the input data, not the model structure\n",
    "    \n",
    "    return deh_model\n",
    "\n",
    "def memory_efficient_set_neighbor_weights(deh_obj, data, batch_size=10000):\n",
    "    \"\"\"Memory-efficient version of setting neighbor weights\"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    n_samples = data.shape[0]\n",
    "    deh_obj.full_weights = np.ones(n_samples)\n",
    "    \n",
    "    # Process in batches\n",
    "    for start_idx in range(0, n_samples, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, n_samples)\n",
    "        batch_indices = np.arange(start_idx, end_idx)\n",
    "        \n",
    "        # Only process non-saturated data if relevant\n",
    "        rel_data = batch_indices\n",
    "        \n",
    "        # Skip if no relevant data in this batch\n",
    "        if len(rel_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Process this batch\n",
    "        neighbor_indices = deh_obj.neighbors[rel_data]\n",
    "        \n",
    "        # Calculate differences and weights for this batch\n",
    "        diffs = data[rel_data] - data[neighbor_indices]\n",
    "        expected_errs = np.sum(diffs**2, axis=1)\n",
    "        \n",
    "        # Update weights\n",
    "        deh_obj.full_weights[rel_data] = 1 / (np.sqrt(expected_errs) + np.mean(np.sqrt(expected_errs)))\n",
    "    \n",
    "    # Normalize weights\n",
    "    if np.sum(deh_obj.full_weights) > 0:\n",
    "        deh_obj.full_weights /= np.mean(deh_obj.full_weights[deh_obj.full_weights > 0])\n",
    "    \n",
    "    return deh_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93aa6b1",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc07b771",
   "metadata": {},
   "source": [
    "### Old TOA DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ce3e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_combined_loaded = np.load('D:\\Hierarchical Unmixing Label\\hUH\\images\\cm_combined_10.npy')\n",
    "saturated_combined_loaded = np.load('D:\\Hierarchical Unmixing Label\\hUH\\images\\saturated_combined_10.npy')\n",
    "saturated_combined_loaded = np.asarray(saturated_combined_loaded, dtype=bool)\n",
    "\n",
    "rgb_combined = plot_cm_rgb_composite(cm_combined_loaded, \"cm_combined_loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f198b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "grizzlybay_cm_norm = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\grizzlybay_2024-09-30T18-25-29Z-l1a_cm_norm.npy')\n",
    "victoriaLand_cm_norm = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\victoriaLand_2025-01-03T13-05-25Z-l1a_cm_norm.npy')\n",
    "catala_cm_norm = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\catala_2025-01-28T19-17-32Z-l1a_cm_norm.npy')\n",
    "menindee_cm_norm = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\menindee_2025-01-21T00-00-40Z-l1a_cm_norm.npy')\n",
    "deepbay_cm_norm = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\deepbay_2025-01-24T18-21-34Z-l1a_cm_norm.npy')\n",
    "kemigawa_cm_norm = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\kemigawa_2025-01-30T01-03-42Z-l1a_cm_norm.npy')\n",
    "chad_cm_norm = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\chad_2025-01-30T08-58-47Z-l1a_cm_norm.npy')\n",
    "mjosa_cm_norm = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\mjosa_2025-01-30T10-19-48Z-l1a_cm_norm.npy')\n",
    "chetumalbay_cm_norm = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\chetumalbay_2025-01-17T16-01-46Z-l1a_cm_norm.npy')\n",
    "lacrau_cm_norm = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\lacrau_2025-01-17T09-40-10Z-l1a_cm_norm.npy')\n",
    "\n",
    "rgb_grizzlybay = plot_cm_rgb_composite(grizzlybay_cm_norm, \"grizzlybay\")\n",
    "rgb_victoriaLand = plot_cm_rgb_composite(victoriaLand_cm_norm, \"victoriaLand\")\n",
    "rgb_catala = plot_cm_rgb_composite(catala_cm_norm, \"catala\")\n",
    "rgb_menindee = plot_cm_rgb_composite(menindee_cm_norm, \"menindee\")\n",
    "rgb_deepbay = plot_cm_rgb_composite(deepbay_cm_norm, \"deepbay\")\n",
    "rgb_kemigawa = plot_cm_rgb_composite(kemigawa_cm_norm, \"kemigawa\")\n",
    "rgb_chad = plot_cm_rgb_composite(chad_cm_norm, \"chad\")\n",
    "rgb_mjosa = plot_cm_rgb_composite(mjosa_cm_norm, \"mjosa\")\n",
    "rgb_chetumalbay = plot_cm_rgb_composite(chetumalbay_cm_norm, \"chetumalbay\")\n",
    "rgb_lacrau = plot_cm_rgb_composite(lacrau_cm_norm, \"lacrau\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00da8d3",
   "metadata": {},
   "source": [
    "### L1B DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f971994c",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_10_v2_L1B = np.load('D:\\Hierarchical Unmixing Label\\hUH\\images\\combined_10_v2_L1B.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69574976",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_combined_10_v2_L1B = plot_cm_rgb_composite(combined_10_v2_L1B, \"combined_10_v2_L1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ce5a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1b_yucatan2_2025_02_06         = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\yucatan2_2025-02-06T16-01-18Z-l1a_cm_l1b.npy')\n",
    "l1b_kemigawa_2024_12_17         = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\kemigawa_2024-12-17T01-01-32Z-l1a_cm_l1b.npy')\n",
    "l1b_chapala_2025_02_24          = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\chapala_2025-02-24T16-52-47Z-l1a_cm_l1b.npy')\n",
    "l1b_grizzlybay_2025_01_27       = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\grizzlybay_2025-01-27T18-19-56Z-l1a_cm_l1b.npy')\n",
    "l1b_victoriaLand_2025_02_07     = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\victoriaLand_2025-02-07T20-35-33Z-l1a_cm_l1b.npy')\n",
    "l1b_catala_2025_01_28           = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\catala_2025-01-28T19-17-32Z-l1a_cm_l1b.npy')\n",
    "l1b_khnifiss_2025_02_12         = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\khnifiss_2025-02-12T11-05-35Z-l1a_cm_l1b.npy')\n",
    "l1b_menindee_2025_02_18         = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\menindee_2025-02-18T00-10-42Z-l1a_cm_l1b.npy')\n",
    "l1b_falklandsatlantic_2024_12_18= np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\falklandsatlantic_2024-12-18T13-25-18Z-l1a_cm_l1b.npy')\n",
    "l1b_tampa_2024_11_12            = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\tampa_2024-11-12T15-31-55Z-l1a_cm_l1b.npy')\n",
    "# l1b_aquawatchmoreton_2024_09_02 = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\aquawatchmoreton_2024-09-02T23-10-28Z-l1a_cm_l1b.npy')\n",
    "# l1b_mjosa_2025_02_11            = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\mjosa_2025-02-11T09-56-45Z-l1a_cm_l1b.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05792ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_l1b_yucatan2_2025_02_06 = plot_cm_rgb_composite(l1b_yucatan2_2025_02_06, \"yucatan2_2025_02_06\")\n",
    "rgb_l1b_kemigawa_2024_12_17 = plot_cm_rgb_composite(l1b_kemigawa_2024_12_17, \"kemigawa_2024_12_17\")\n",
    "rgb_l1b_chapala_2025_02_24 = plot_cm_rgb_composite(l1b_chapala_2025_02_24, \"chapala_2025_02_24\")\n",
    "rgb_l1b_grizzlybay_2025_01_27 = plot_cm_rgb_composite(l1b_grizzlybay_2025_01_27, \"grizzlybay_2025_01_27\")\n",
    "rgb_l1b_victoriaLand_2025_02_07 = plot_cm_rgb_composite(l1b_victoriaLand_2025_02_07, \"victoriaLand_2025_02_07\")\n",
    "rgb_l1b_catala_2025_01_28 = plot_cm_rgb_composite(l1b_catala_2025_01_28, \"catala_2025_01_28\")\n",
    "rgb_l1b_khnifiss_2025_02_12 = plot_cm_rgb_composite(l1b_khnifiss_2025_02_12, \"khnifiss_2025_02_12\")\n",
    "rgb_l1b_menindee_2025_02_18 = plot_cm_rgb_composite(l1b_menindee_2025_02_18, \"menindee_2025_02_18\")\n",
    "rgb_l1b_falklandsatlantic_2024_12_18 = plot_cm_rgb_composite(l1b_falklandsatlantic_2024_12_18, \"falklandsatlantic_2024_12_18\")\n",
    "rgb_l1b_tampa_2024_11_12 = plot_cm_rgb_composite(l1b_tampa_2024_11_12, \"tampa_2024_11_12\")\n",
    "# rgb_l1b_aquawatchmoreton_2024_09_02 = plot_cm_rgb_composite(l1b_aquawatchmoreton_2024_09_02, \"aquawatchmoreton_2024_09_02\")\n",
    "# rgb_l1b_mjosa_2025_02_11 = plot_cm_rgb_composite(l1b_mjosa_2025_02_11, \"mjosa_2025_02_11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4399b6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "saturated_combined_10_v2_L1B = np.load('D:\\Hierarchical Unmixing Label\\hUH\\images\\saturated_combined_10_v2_L1B.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb7a491",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1b_yucatan2_2025_02_06_sat         = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\yucatan2_2025-02-06T16-01-18Z-l1a_saturated_l1b.npy')\n",
    "l1b_kemigawa_2024_12_17_sat         = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\kemigawa_2024-12-17T01-01-32Z-l1a_saturated_l1b.npy')\n",
    "l1b_chapala_2025_02_24_sat          = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\chapala_2025-02-24T16-52-47Z-l1a_saturated_l1b.npy')\n",
    "l1b_grizzlybay_2025_01_27_sat       = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\grizzlybay_2025-01-27T18-19-56Z-l1a_saturated_l1b.npy')\n",
    "l1b_victoriaLand_2025_02_07_sat     = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\victoriaLand_2025-02-07T20-35-33Z-l1a_saturated_l1b.npy')\n",
    "l1b_catala_2025_01_28_sat           = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\catala_2025-01-28T19-17-32Z-l1a_saturated_l1b.npy')\n",
    "l1b_khnifiss_2025_02_12_sat         = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\khnifiss_2025-02-12T11-05-35Z-l1a_saturated_l1b.npy')\n",
    "l1b_menindee_2025_02_18_sat         = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\menindee_2025-02-18T00-10-42Z-l1a_saturated_l1b.npy')\n",
    "l1b_falklandsatlantic_2024_12_18_sat= np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\falklandsatlantic_2024-12-18T13-25-18Z-l1a_saturated_l1b.npy')\n",
    "l1b_tampa_2024_11_12_sat            = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\tampa_2024-11-12T15-31-55Z-l1a_saturated_l1b.npy')\n",
    "# l1b_aquawatchmoreton_2024_09_02_sat = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\aquawatchmoreton_2024-09-02T23-10-28Z-l1a_saturated_l1b.npy')\n",
    "# l1b_mjosa_2025_02_11_sat            = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\mjosa_2025-02-11T09-56-45Z-l1a_saturated_l1b.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97164b7",
   "metadata": {},
   "source": [
    "### L1D + MACHI DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c28e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_10_L1D_112_MACHI = np.load('D:\\Hierarchical Unmixing Label\\hUH\\images\\combined_10_L1D_112_MACHI.npy')\n",
    "saturated_combined_10_L1D_112_MACHI = np.load('D:\\Hierarchical Unmixing Label\\hUH\\images\\saturated_combined_10_L1D_112_MACHI.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6423d7a",
   "metadata": {},
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b82f74",
   "metadata": {},
   "source": [
    "### Train V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459006d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random init and stabelize the network 700min\n",
    "DEH_function = deh.DEH(no_negative_residuals=True)\n",
    "save_name = \"save/dehx_10img_8end_10runs_fast_stabelized\"\n",
    "#DEH_combined, DEH_combined_stabelized = DEH_init_stabelize(DEH_input=DEH_function,  cm_input=cm_combined_loaded, saturated_input=saturated_combined_loaded, save_name=save_name, n_endmembers=8, n_runs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a52c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "catala_cm_norm = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\catala_2025-01-28T19-17-32Z-l1a_cm_norm.npy')\n",
    "catala_saturated = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\catala_2025-01-28T19-17-32Z-l1a_saturated.npy')\n",
    "rgb_catala = plot_cm_rgb_composite(catala_cm_norm, \"catala\")\n",
    "catala_save_name = \"save/DEH_catala_16end_10runs_fast_stabelized\"\n",
    "#DEH_catala=deh.DEH(no_negative_residuals=True)\n",
    "#DEH_catala, DEH_catala_stabelized = deh_init_stabelize(DEH_input=DEH_catala, cm_input=catala_cm_norm, saturated_input=catala_saturated, save_name=catala_save_name, n_endmembers=16, n_runs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62112ae6",
   "metadata": {},
   "source": [
    "### Train V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ec70b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEH_V2= deh.DEH(no_negative_residuals=True)\n",
    "save_name_V2 = \"save/L1B_10img_16end_random\"\n",
    "verbose=True\n",
    "#Init DEH\n",
    "DEH_V2.splitting_size=1000\n",
    "DEH_V2.max_depth=2\n",
    "DEH_V2.max_iter=5\n",
    "DEH_V2.max_nodes=2\n",
    "\n",
    "\n",
    "cube=combined_10_v2_L1B.reshape(-1,1092,112)\n",
    "#these set the size of the image to be plotted\n",
    "DEH_V2.plot_size = (cube.shape[0],cube.shape[1])\n",
    "DEH_V2.plot_aspect = 0.1\n",
    "\n",
    "# these set the normaliztion on the data, but right now it is turned off and replaced with a nearest-neighbor noise estimate\n",
    "DEH_V2.weight_power=0\n",
    "DEH_V2.eps = 0.00\n",
    "#dehx.wf = lambda x: (np.sum(x**2, axis=-1))**(-1/2+1/(2+dehx.eps))\n",
    "\n",
    "#this switches between Archetype Analysis and Pure Pixel Analysis\n",
    "DEH_V2.aa = False\n",
    "\n",
    "#this is currently replaced by \"mpp_tol\" in the training code\n",
    "DEH_V2.mixed_pix = 0\n",
    "\n",
    "# this is the prefactor on the L2 regularization\n",
    "DEH_V2.reg=0\n",
    "\n",
    "# if this is anything other than zero, it introduces L2 regularization on the trained weights \n",
    "DEH_V2.set_mu(0)\n",
    "\n",
    "# this turns on a sort of protection to keep endmembers from vanishing. Generally unneeded with new training\n",
    "DEH_V2.use_bonus_boost = False\n",
    "\n",
    "# turns on normalization of the data. Generally always used\n",
    "DEH_V2.use_norm(True)\n",
    "\n",
    "# if greater than 1, turns on a sort of spectral sampling with PAA. Occasionally gives good performance\n",
    "# but I generally keep it at 1 to turn it off\n",
    "DEH_V2.PAA_backcount = 1\n",
    "\n",
    "# changes the gradient descent learning rate as a proportion of the optimal rate. I recommend keeping it at 1\n",
    "DEH_V2.a_speed= 1.0\n",
    "\n",
    "#Verbose print\n",
    "if verbose:\n",
    "    print(\"cm_input.shape: \", combined_10_v2_L1B.shape)\n",
    "    print(\"cube.shape: \", cube.shape)\n",
    "    print(\"DEH_V2.plot_size\", DEH_V2.plot_size)\n",
    "    print(\"saturated_input.shape\", saturated_combined_10_v2_L1B.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd12a35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random Init\n",
    "if verbose:\n",
    "    print(\"Quick nn\")\n",
    "DEH_V2.neighbors = deh.quick_nn(combined_10_v2_L1B.reshape(DEH_V2.plot_size + (-1,)), k_size=1).flatten()\n",
    "if verbose: \n",
    "    print(\"Setting neighbor weights\")\n",
    "DEH_V2.set_neighbor_weights(combined_10_v2_L1B)\n",
    "if verbose:\n",
    "    print(\"Random init\")\n",
    "DEH_V2.random_init(combined_10_v2_L1B, 16)\n",
    "if verbose:\n",
    "    print(\"Simple predict\")\n",
    "deh_v2_pred=DEH_V2.simple_predict(combined_10_v2_L1B)\n",
    "if verbose:\n",
    "    print(\"Getting trimmed network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73975927",
   "metadata": {},
   "source": [
    "### Random training 256 endmembers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208aab7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = \"save/L1D_112_MACHI_10img_256end_random\"\n",
    "verbose=True\n",
    "flat=combined_10_L1D_112_MACHI\n",
    "cube=flat.reshape(-1,1092,112)\n",
    "saturated=saturated_combined_10_L1D_112_MACHI\n",
    "\n",
    "\n",
    "#Init DEH\n",
    "DEH_V3= deh.DEH(no_negative_residuals=True)\n",
    "DEH_V3.splitting_size=1000\n",
    "DEH_V3.max_depth=2\n",
    "DEH_V3.max_iter=5\n",
    "DEH_V3.max_nodes=2\n",
    "\n",
    "#these set the size of the image to be plotted\n",
    "DEH_V3.plot_size = (cube.shape[0],cube.shape[1])\n",
    "DEH_V3.plot_aspect = 0.1\n",
    "\n",
    "# these set the normaliztion on the data, but right now it is turned off and replaced with a nearest-neighbor noise estimate\n",
    "DEH_V3.weight_power=0\n",
    "DEH_V3.eps = 0.00\n",
    "#dehx.wf = lambda x: (np.sum(x**2, axis=-1))**(-1/2+1/(2+dehx.eps))\n",
    "\n",
    "#this switches between Archetype Analysis and Pure Pixel Analysis\n",
    "DEH_V3.aa = False\n",
    "\n",
    "#this is currently replaced by \"mpp_tol\" in the training code\n",
    "DEH_V3.mixed_pix = 0\n",
    "\n",
    "# this is the prefactor on the L2 regularization\n",
    "DEH_V3.reg=0\n",
    "\n",
    "# if this is anything other than zero, it introduces L2 regularization on the trained weights \n",
    "DEH_V3.set_mu(0)\n",
    "\n",
    "# this turns on a sort of protection to keep endmembers from vanishing. Generally unneeded with new training\n",
    "DEH_V3.use_bonus_boost = False\n",
    "\n",
    "# turns on normalization of the data. Generally always used\n",
    "DEH_V3.use_norm(True)\n",
    "\n",
    "# if greater than 1, turns on a sort of spectral sampling with PAA. Occasionally gives good performance\n",
    "# but I generally keep it at 1 to turn it off\n",
    "DEH_V3.PAA_backcount = 1\n",
    "\n",
    "# changes the gradient descent learning rate as a proportion of the optimal rate. I recommend keeping it at 1\n",
    "DEH_V3.a_speed= 1.0\n",
    "\n",
    "#Verbose print\n",
    "if verbose:\n",
    "    print(\"cm_input.shape: \", flat.shape)\n",
    "    print(\"cube.shape: \", cube.shape)\n",
    "    print(\"DEH_V3.plot_size\", DEH_V3.plot_size)\n",
    "    print(\"saturated_input.shape\", saturated.shape)\n",
    "\n",
    "#random Init\n",
    "if verbose:\n",
    "    print(\"Quick nn\")\n",
    "DEH_V3.neighbors = deh.quick_nn(flat.reshape(DEH_V3.plot_size + (-1,)), k_size=1).flatten()\n",
    "if verbose: \n",
    "    print(\"Setting neighbor weights\")\n",
    "# DEH_V3.set_neighbor_weights(flat)\n",
    "DEH_V3.set_neighbor_weights_memory_efficient(flat)\n",
    "if verbose:\n",
    "    print(\"Random init\")\n",
    "DEH_V3.random_init(flat, 256, seed=np.random.randint(0, 1000))\n",
    "if verbose:\n",
    "    print(\"Simple predict\")\n",
    "DEH_V3_pred=DEH_V3.simple_predict(flat)\n",
    "\n",
    "frontier_nodes = tree_walk(DEH_V3, n=8, verbose=True)\n",
    "print(f\"Frontier nodes: {frontier_nodes}\")\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909f3733",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del DEH_V3_pred\n",
    "del DEH_V3.nodes\n",
    "del DEH_V3\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecbd03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEH_V3.save(save_name + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b36ba6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the pruning function on DEH_V3_LOAD with the frontier nodes\n",
    "prune_deh_obj(DEH_V3, frontier_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9059d238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print nodes by depth for the original and pruned DEH objects\n",
    "print(\"\\nPruned DEH object:\")\n",
    "print_nodes_by_depth(DEH_V3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917c95b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stabelized\n",
    "# DEH_V3_pred=DEH_V3_LOAD.simple_predict(combined_10_v2_L1B)\n",
    "# DEH_V3_LOAD.plot_size=(5980,1092)\n",
    "# verbose=False\n",
    "#DEH_V3_LOAD_stab=DEH_V3_LOAD.get_trimmed_network(DEH_V3_LOAD.nodes) #Can replace with copy of DEH_input instead of get trimmed network since it acceptes all nodes\n",
    "# if verbose:\n",
    "print(\"Quick nn\")\n",
    "DEH_V3.neighbors = deh.quick_nn(flat.reshape(DEH_V3.plot_size + (-1,)), k_size=1).flatten()\n",
    "# if verbose:\n",
    "print(\"Setting neighbor weights\")\n",
    "\n",
    "DEH_V3.set_neighbor_weights_memory_efficient(flat)\n",
    "# if verbose:\n",
    "print(\"DEH_stabelized.full_weights.shape\",DEH_V3.full_weights.shape)\n",
    "#saturated_input = np.asarray(saturated_input, dtype=bool)\n",
    "DEH_V3.full_weights[saturated]=0\n",
    "DEH_V3.PAA_backcount=1\n",
    "\n",
    "# if verbose:\n",
    "print(\"Accepted network stablization\")\n",
    "DEH_V3.accepted_network_stablization(flat, n_runs=10, n_pts=(1000,10000), obj_record=(), sampling_points=(), mpp_tol=0.2, step_delta=0.05, reg_max=0.2, name=save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314fc337",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873b3770",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_level_and_overlay(r'D:\\Hierarchical Unmixing Label\\hUH\\save\\dehx_10img_8end_10runs_fast_stabelized_aa_FINAL.h5', lacrau_cm_norm, plot_only_last_level=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9856c859",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_level_and_overlay(r'D:\\Hierarchical Unmixing Label\\hUH\\save\\dehx_10img_8end_10runs_fast_stabelized_aa_FINAL.h5', kemigawa_cm_norm, plot_only_last_level=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3db1a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_level_and_overlay(r'D:\\Hierarchical Unmixing Label\\hUH\\save\\dehx_10img_8end_10runs_fast_stabelized_aa_FINAL.h5', grizzlybay_cm_norm, plot_only_last_level=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d661285c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectra_for_level(r'D:\\Hierarchical Unmixing Label\\hUH\\save\\dehx_10img_8end_10runs_fast_stabelized_aa_FINAL.h5', lacrau_cm_norm, level_input=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c7e98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_level_and_overlay(r'D:\\Hierarchical Unmixing Label\\hUH\\save\\DEH_catala_16end_10runs_fast_stabelized_aa_FINAL.h5',catala_cm_norm, plot_only_last_level=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d421c7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_level_and_overlay(r'D:\\Hierarchical Unmixing Label\\hUH\\save\\DEH_catala_16end_10runs_fast_stabelized_aa_FINAL.h5',catala_cm_norm, plot_only_last_level=True, binarize=True, opacity=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe361b3",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0d78d8",
   "metadata": {},
   "source": [
    "### TEST 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f392b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEH_SNOW= deh.DEH(no_negative_residuals=True)\n",
    "DEH_SNOW.load(r'D:\\Hierarchical Unmixing Label\\hUH\\save\\DEH_catala_16end_10runs_fast_stabelized_aa_FINAL.h5')\n",
    "pred=DEH_SNOW.simple_predict(catala_cm_norm)\n",
    "DEH_SNOW_binarized=copy.deepcopy(DEH_SNOW)\n",
    "DEH_SNOW_binarized.binarize_lmdas()\n",
    "DEH_SNOW_binarized.lmda_2_map()\n",
    "DEH_SNOW_binarized.display_level(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955a4eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "binarized_model = copy.deepcopy(DEH_SNOW)\n",
    "save_binarized_labels_to_npy(binarized_model, catala_cm_norm, \"catala_binary_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a6a40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "catala_cm_norm = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\catala_2025-01-28T19-17-32Z-l1a_cm_norm.npy')\n",
    "rgb_catala = plot_cm_rgb_composite(catala_cm_norm, \"catala\")\n",
    "plot_level_labels(\"save\\catala_binary_labels.npy\", level=1)\n",
    "plot_level_labels(\"save\\catala_binary_labels.npy\", level=2)\n",
    "plot_level_labels(\"save\\catala_binary_labels.npy\", level=3)\n",
    "plot_level_labels(\"save\\catala_binary_labels.npy\", level=4)\n",
    "plot_level_labels(\"save\\catala_binary_labels.npy\", level=5)\n",
    "plot_level_labels(\"save\\catala_binary_labels.npy\", level=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b637cb32",
   "metadata": {},
   "source": [
    "### TEST 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a497f187",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEH_V2_grizzly=copy.deepcopy(DEH_V2)\n",
    "DEH_V2_grizzly.plot_size = (598,1092)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81b6b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEH_V2_grizzly.simple_predict(l1b_grizzlybay_2025_01_27)\n",
    "plt.figure(figsize=(8,2))\n",
    "plt.axis('off')\n",
    "plt.imshow(rgb_l1b_grizzlybay_2025_01_27, aspect=0.1)\n",
    "DEH_V2_grizzly.display_level(1)\n",
    "DEH_V2_grizzly.display_level(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf843da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEH_V2_falklands=copy.deepcopy(DEH_V2)\n",
    "DEH_V2_falklands.plot_size = (598,1092)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b62ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEH_V2_falklands.simple_predict(l1b_falklandsatlantic_2024_12_18)\n",
    "plt.figure(figsize=(8,2))\n",
    "plt.axis('off')\n",
    "plt.imshow(rgb_l1b_falklandsatlantic_2024_12_18, aspect=0.1)\n",
    "DEH_V2_falklands.display_level(1)\n",
    "DEH_V2_falklands.display_level(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba58f074",
   "metadata": {},
   "source": [
    "### TEST REDUCE AND STABELIZE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251cb17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_10_L1D_112_MACHI = np.load('D:\\Hierarchical Unmixing Label\\hUH\\images\\combined_10_L1D_112_MACHI.npy')\n",
    "saturated_combined_10_L1D_112_MACHI = np.load('D:\\Hierarchical Unmixing Label\\hUH\\images\\saturated_combined_10_L1D_112_MACHI.npy')\n",
    "flat=combined_10_L1D_112_MACHI\n",
    "saturated=saturated_combined_10_L1D_112_MACHI\n",
    "DEH_V3_LOAD= deh.DEH(no_negative_residuals=True)\n",
    "DEH_V3_LOAD.load(r'D:\\Hierarchical Unmixing Label\\hUH\\save\\L1D_112_MACHI_10img_256end_random.h5')\n",
    "DEH_V3_LOAD.verbose = True\n",
    "DEH_V3_pred=DEH_V3_LOAD.simple_predict(flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32dc648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the tree walk function on DEH_V3_LOAD\n",
    "frontier_nodes = tree_walk(DEH_V3_LOAD, n=8)\n",
    "print(frontier_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15ff611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the pruning function on DEH_V3_LOAD with the frontier nodes\n",
    "prune_deh_obj(DEH_V3_LOAD, frontier_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e36512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print nodes by depth for the original and pruned DEH objects\n",
    "print(\"\\nPruned DEH object:\")\n",
    "print_nodes_by_depth(DEH_V3_LOAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636675bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stabelized\n",
    "# DEH_V3_pred=DEH_V3_LOAD.simple_predict(combined_10_v2_L1B)\n",
    "# DEH_V3_LOAD.plot_size=(5980,1092)\n",
    "# verbose=False\n",
    "#DEH_V3_LOAD_stab=DEH_V3_LOAD.get_trimmed_network(DEH_V3_LOAD.nodes) #Can replace with copy of DEH_input instead of get trimmed network since it acceptes all nodes\n",
    "# if verbose:\n",
    "print(\"Quick nn\")\n",
    "DEH_V3_LOAD.neighbors = deh.quick_nn(flat.reshape(DEH_V3_LOAD.plot_size + (-1,)), k_size=1).flatten()\n",
    "# if verbose:\n",
    "print(\"Setting neighbor weights\")\n",
    "\n",
    "DEH_V3_LOAD.set_neighbor_weights(flat)\n",
    "# if verbose:\n",
    "print(\"DEH_stabelized.full_weights.shape\",DEH_V3_LOAD.full_weights.shape)\n",
    "#saturated_input = np.asarray(saturated_input, dtype=bool)\n",
    "DEH_V3_LOAD.full_weights[saturated]=0\n",
    "DEH_V3_LOAD.PAA_backcount=1\n",
    "\n",
    "save_name=\"save\\MACHI_10img_256to8_stab.h5\"\n",
    "# if verbose:\n",
    "print(\"Accepted network stablization\")\n",
    "DEH_V3_LOAD.accepted_network_stablization(flat, n_runs=10, n_pts=(1000,10000), obj_record=(), sampling_points=(), mpp_tol=0.2, step_delta=0.05, reg_max=0.2, name=save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb51aad",
   "metadata": {},
   "source": [
    "* test reglectance data that is scaled to 0-1? may be neccesary for DEH\n",
    "* test this to see if sparsift works better then\n",
    "* work out the sparse tree function\n",
    "* speed up trainnig, sparsify sweep loop to slow? faster iteration? \n",
    "* stuck at around .8 but threshold is at .2\n",
    "* look into the traditional training sparse tree output\n",
    "* better prints form accepted stabilization\n",
    "* better naming \n",
    "\n",
    "After training\n",
    "* data should be in range 0-1, else errors happens\n",
    "* sparsify sweep does not need step of 0.01 or 20 steps. very slow\n",
    "* lines in data, will see if persists in binarysation, might be a problem with the data, since it is radiance based not reflectance\n",
    "* optimally is should be TOA with atmosphere correction-> bottom of atmosphere reflection\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb399a1d",
   "metadata": {},
   "source": [
    "### TEST 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34619d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_10_v2_MACHI = np.load('D:\\Hierarchical Unmixing Label\\hUH\\images\\combined_10_v2_MACHI.npy')\n",
    "DEH_v3_aa= deh.DEH(no_negative_residuals=True)\n",
    "DEH_v3_aa.load(r'D:\\Hierarchical Unmixing Label\\hUH\\save\\MACHI_10img_256to8_stab.h5_aa_FINAL.h5')\n",
    "DEH_v3_aa_pred=DEH_v3_aa.simple_predict(combined_10_v2_MACHI)\n",
    "DEH_v3_aa.binarize_lmdas()\n",
    "DEH_v3_aa.lmda_2_map()\n",
    "DEH_v3_aa.display_level(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0c9f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_binarized_labels_to_npy(DEH_v3_aa, combined_10_v2_MACHI, \"save\\combined_MACHI_binary_labels.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc6f438",
   "metadata": {},
   "source": [
    "### TEST 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06943f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "aregantsea2_2025_03_11_MACHI = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\aregantsea2_2025-03-11T08-12-43Z-l1a_cm_machi.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df54a740",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DEH_v3_aa= deh.DEH(no_negative_residuals=True)\n",
    "DEH_v3_aa.load(r'D:\\Hierarchical Unmixing Label\\hUH\\save\\MACHI_10img_256to8_stab.h5_aa_FINAL.h5')\n",
    "DEH_v3_aa.plot_size=(598,1092)\n",
    "DEH_v3_aa_pred=DEH_v3_aa.simple_predict(aregantsea2_2025_03_11_MACHI)\n",
    "DEH_v3_aa.display_level(4)\n",
    "DEH_v3_aa.binarize_lmdas()\n",
    "DEH_v3_aa.lmda_2_map()\n",
    "DEH_v3_aa.display_level(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa36fb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_binarized_labels_to_npy(DEH_v3_aa, aregantsea2_2025_03_11_MACHI, r\"save\\aregantsea2_2025_03_11_MACHI_binary_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9f9c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "losmanzanosfire_2025_03_11_MACHI = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\losmanzanosfire_2025-03-11T14-56-42Z-l1a_cm_MACHI.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad35eb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DEH_v3_aa= deh.DEH(no_negative_residuals=True)\n",
    "DEH_v3_aa.load(r'D:\\Hierarchical Unmixing Label\\hUH\\save\\MACHI_10img_256to8_stab.h5_aa_FINAL.h5')\n",
    "DEH_v3_aa.plot_size=(598,1092)\n",
    "DEH_v3_aa_pred=DEH_v3_aa.simple_predict(losmanzanosfire_2025_03_11_MACHI)\n",
    "DEH_v3_aa.display_level(4)\n",
    "DEH_v3_aa.binarize_lmdas()\n",
    "DEH_v3_aa.lmda_2_map()\n",
    "DEH_v3_aa.display_level(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e65642",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_binarized_labels_to_npy(DEH_v3_aa, losmanzanosfire_2025_03_11_MACHI, r\"save\\losmanzanosfire_2025_03_11_MACHI_binary_labels.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c0ee04",
   "metadata": {},
   "source": [
    "### TEST L1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4c9e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aregantsea2_2025_03_11 = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\aregantsea2_2025-03-11T08-12-43Z-l1a_cm_L1B.npy')\n",
    "aregantsea2_2025_03_11_saturated = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\aregantsea2_2025-03-11T08-12-43Z-l1a_saturated_L1B.npy')\n",
    "save_name=\"save/L1B_1img_256end_random.h5\"\n",
    "DEH_load=\"D:\\Hierarchical Unmixing Label\\hUH\\save\\L1B_1img_256end_random.h5\"\n",
    "labels_path=\"save\\L1B_1img_8end_stabelized_labels\"\n",
    "stabelized_path=\"save\\L1B_1img_8end_stabelized_ppa.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124873f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_random_DEH(aregantsea2_2025_03_11,endmembers=256,save_name=save_name,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444a2f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_path=prune_DEH(save_name,image_data=aregantsea2_2025_03_11,endmembers=8,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f50bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stabelize_DEH(DEH_path=pruned_path, image_data=aregantsea2_2025_03_11, saturated_image_data=aregantsea2_2025_03_11_saturated, n_runs=10, step_delta=0.05, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d5a40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_binarized_labels_to_npy(stabelized_path, aregantsea2_2025_03_11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90d6418",
   "metadata": {},
   "source": [
    "### L1D 112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d839b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_10_L1D_112_MACHI = np.load('D:\\Hierarchical Unmixing Label\\hUH\\images\\combined_10_L1D_112_MACHI.npy')\n",
    "saturated_combined_10_L1D_112_MACHI = np.load('D:\\Hierarchical Unmixing Label\\hUH\\images\\saturated_combined_10_L1D_112_MACHI.npy')\n",
    "\n",
    "flat=combined_10_L1D_112_MACHI\n",
    "saturated=saturated_combined_10_L1D_112_MACHI\n",
    "stabelized_path='save\\L1D_112_MACHI_10img_8end_stabelized_aa.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ec518c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tampa_2024_11_12_L1D_112=np.load(r'images\\tampa_2024-11-12T15-31-55Z-l1a_cm_L1D_112_MACHI.npy')\n",
    "tampa_2024_11_12_L1D_112_save=r'save\\tampa_2024_11_12_L1D_112_labels'\n",
    "save_binarized_labels_to_npy(stabelized_path, tampa_2024_11_12_L1D_112, tampa_2024_11_12_L1D_112_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59015c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_binarized_labels_to_npy(stabelized_path, combined_10_L1D_112_MACHI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92990125",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DEH_L1D= deh.DEH(no_negative_residuals=True)\n",
    "DEH_L1D.load(r'D:\\Hierarchical Unmixing Label\\hUH\\save\\L1D_112_MACHI_10img_8end_stabelized_aa.h5')\n",
    "DEH_L1D.verbose = True\n",
    "DEH_L1D_pred=DEH_L1D.simple_predict(flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b47d4d2",
   "metadata": {},
   "source": [
    "### L1A 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41d282c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stabelized_path='save\\L1D_112_MACHI_10img_8end_stabelized_aa.h5'\n",
    "image = np.load(r'images\\tampa_2024-11-12T15-31-55Z-l1a_cm_L1D_112_MACHI.npy')\n",
    "save_path=r'save\\tampa_2024_11_12_L1A_120_labels'\n",
    "save_binarized_labels_to_npy(stabelized_path, image, save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hypso",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
