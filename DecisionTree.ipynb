{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib as il\n",
    "from hypso import Hypso1, Hypso2\n",
    "from sklearn.svm import LinearSVC\n",
    "import src.deh as deh\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPSO_HEIGHT=1092\n",
    "HYPSO_WIDTH=598\n",
    "HYPSO_BANDS=120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPSO_1_NC_DIR  = r\"D:\\HYPSO_1_NC\"\n",
    "HYPSO_2_NC_DIR  = r\"D:\\HYPSO_2_NC\"\n",
    "DATA_DIR        = r\"D:\\Hierarchical Unmixing Label\\hUH\\data\"\n",
    "IMAGES_DIR      = DATA_DIR + r\"\\images\"\n",
    "DEH_DIR         = DATA_DIR + r\"\\deh_models\"\n",
    "LABELS_DIR      = DATA_DIR + r\"\\labels\"\n",
    "TREE_DIR        = DATA_DIR + r\"\\tree_models\"\n",
    "PRED_DIR        = DATA_DIR + r\"\\predictions\"\n",
    "SVM_DIR    =r'\\\\wsl.localhost\\Ubuntu-24.04\\home\\lofty\\CODE\\onboard-pipeline-modules\\ground_training'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASS AND FUCNTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryDecisionTree:\n",
    "    def __init__(self, verbose=False):\n",
    "        self.verbose = verbose\n",
    "        self.all_labels = {}\n",
    "        self.endmembers = []\n",
    "        self.splitting_nodes = []\n",
    "        self.models = {}  # Dictionary to store SVM models for each splitting node\n",
    "        \n",
    "    def initialize_tree_structure(self, image_gt):\n",
    "        \"\"\"Initialize the tree structure by identifying endmembers and splitting nodes\"\"\"\n",
    "        print(\"Initializing Binary Decision Tree structure...\")\n",
    "        all_keys=image_gt.keys()\n",
    "        self.identify_set_endmembers(all_keys)\n",
    "        self.identify_set_splitting_nodes(all_keys)\n",
    "        \n",
    "    def identify_set_endmembers(self, all_keys):\n",
    "        max_length = max(len(key) for key in all_keys)\n",
    "        self.endmembers = []\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"IDENTIFYING ENDMEMBERS:\")\n",
    "            print(\"=\"*50)\n",
    "            print(f\"Maximum key length found: {max_length}\")\n",
    "            print(\"-\"*50)\n",
    "        \n",
    "        for key in all_keys:\n",
    "            if len(key) == max_length:\n",
    "                self.endmembers.append(key)\n",
    "                if self.verbose:\n",
    "                    print(f\"Found endmember: '{key}'\")\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Total endmembers identified: {len(self.endmembers)}\")\n",
    "        \n",
    "    def identify_set_splitting_nodes(self, all_keys):\n",
    "        self.splitting_nodes = []\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"IDENTIFYING SPLITTING NODES:\")\n",
    "            print(\"=\"*50)\n",
    "        \n",
    "        for key in all_keys:\n",
    "            has_zero = key + '0' in all_keys\n",
    "            has_one = key + '1' in all_keys\n",
    "            if has_zero and has_one:\n",
    "                self.splitting_nodes.append(key)\n",
    "                if self.verbose:\n",
    "                    print(f\"Found splitting node: '{key}' â†’ branches to '{key}0' and '{key}1'\")\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"-\"*50)\n",
    "            print(f\"Total splitting nodes identified: {len(self.splitting_nodes)}\")\n",
    "\n",
    "    def _preprocess_input(self, X):\n",
    "        \"\"\"\n",
    "        Preprocess input data to ensure it's in the right format (n_samples, n_bands)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy.ndarray\n",
    "            Hyperspectral image data. Can be:\n",
    "            - Flattened array of shape (n_samples, n_bands)\n",
    "            - Cube of shape (height, width, n_bands) or (n_bands, height, width)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        X_processed : numpy.ndarray\n",
    "            Processed data of shape (n_samples, n_bands)\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print(f\"\\nPreprocessing input data with shape: {X.shape}\")\n",
    "            \n",
    "        # If already 2D with samples as first dimension, return as is\n",
    "        if len(X.shape) == 2:\n",
    "            if self.verbose:\n",
    "                # print(f\"Input is already in correct format: {X.shape}\")\n",
    "                print(\"-\"*50)\n",
    "            return X\n",
    "        \n",
    "        # Handle 3D data (cube)\n",
    "        elif len(X.shape) == 3:\n",
    "            # Determine which dimension is the spectral dimension\n",
    "            # Typically, spectral dimension is the smallest\n",
    "            dims = np.array(X.shape)\n",
    "            spectral_dim = np.argmin(dims)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"Detected spectral dimension: {spectral_dim}\")\n",
    "            \n",
    "            if spectral_dim == 0:  # (n_bands, height, width)\n",
    "                n_bands, height, width = X.shape\n",
    "                if self.verbose:\n",
    "                    print(f\"Reshaping from (n_bands={n_bands}, height={height}, width={width}) to ({height*width}, {n_bands})\")\n",
    "                    print(\"-\"*50)\n",
    "                return X.reshape(n_bands, -1).T  # Reshape to (height*width, n_bands)\n",
    "            \n",
    "            elif spectral_dim == 2:  # (height, width, n_bands)\n",
    "                height, width, n_bands = X.shape\n",
    "                if self.verbose:\n",
    "                    print(f\"Reshaping from (height={height}, width={width}, n_bands={n_bands}) to ({height*width}, {n_bands})\")\n",
    "                    print(\"-\"*50)\n",
    "                return X.reshape(-1, n_bands)  # Reshape to (height*width, n_bands)\n",
    "            \n",
    "            else:  # (height, n_bands, width) - unusual but handle it\n",
    "                height, n_bands, width = X.shape\n",
    "                if self.verbose:\n",
    "                    print(f\"Unusual format detected: (height={height}, n_bands={n_bands}, width={width})\")\n",
    "                    print(f\"Reshaping to ({height*width}, {n_bands})\")\n",
    "                    print(\"-\"*50)\n",
    "                return X.transpose(0, 2, 1).reshape(-1, n_bands)  # Reshape to (height*width, n_bands)\n",
    "        \n",
    "        \n",
    "        else:\n",
    "            if self.verbose:\n",
    "                print(f\"Error: Unsupported input shape: {X.shape}\")\n",
    "            raise ValueError(f\"Unsupported input shape: {X.shape}. Expected 2D or 3D array.\")\n",
    "    \n",
    "    def train(self, X, labels=None):\n",
    "        \"\"\"\n",
    "        Train SVMs for each splitting node\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy.ndarray\n",
    "            Hyperspectral image data. Can be:\n",
    "            - Flattened array of shape (n_samples, n_bands)\n",
    "            - Cube of shape (height, width, n_bands) or (n_bands, height, width)\n",
    "        labels : dict, optional\n",
    "            Labels to use for training. If None, uses the labels provided during initialization.\n",
    "        \"\"\"\n",
    "        from sklearn.svm import LinearSVC\n",
    "        \n",
    "        # Process input data to ensure it's in the right format (n_samples, n_bands)\n",
    "        X = self._preprocess_input(X)\n",
    "        \n",
    "        # Use provided labels or fall back to the ones from initialization\n",
    "        training_labels = labels if labels is not None else self.all_labels\n",
    "        \n",
    "        # if self.verbose:\n",
    "        if True:\n",
    "            print(\"Training SVMs for each splitting node...\")\n",
    "        \n",
    "        for node in self.splitting_nodes:\n",
    "            print(\"Splitting node:\",node)\n",
    "            # Get labels for this node\n",
    "            y_parent = training_labels[node]\n",
    "            \n",
    "            # Only consider pixels that belong to this node\n",
    "            mask = y_parent == 1\n",
    "            X_node = X[mask]\n",
    "            \n",
    "            # Get labels for children nodes\n",
    "            left_child = node + '0'\n",
    "            right_child = node + '1'\n",
    "            \n",
    "            # Create binary labels for SVM (1 for right child, 0 for left child)\n",
    "            y_train = np.zeros(X_node.shape[0], dtype=int)\n",
    "            \n",
    "            # Find indices where right child is 1\n",
    "            if right_child in training_labels:\n",
    "                right_mask = training_labels[right_child][mask] == 1\n",
    "                y_train[right_mask] = 1\n",
    "            \n",
    "            # Train LinearSVC\n",
    "            model = LinearSVC(dual='auto', random_state=42)\n",
    "            model.fit(X_node, y_train)\n",
    "            \n",
    "            # Store the model\n",
    "            self.models[node] = model\n",
    "            \n",
    "            # if self.verbose:\n",
    "            if True:\n",
    "                print(f\"Trained model for node '{node}'\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict endmember classes for input data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy.ndarray\n",
    "            Hyperspectral image data. Can be:\n",
    "            - Flattened array of shape (n_samples, n_bands)\n",
    "            - Cube of shape (height, width, n_bands) or (n_bands, height, width)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        predictions : dict\n",
    "            Dictionary with keys for each endmember and values as binary masks\n",
    "        \"\"\"\n",
    "        # Process input data to ensure it's in the right format (n_samples, n_bands)\n",
    "        X = self._preprocess_input(X)\n",
    "        \n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # Initialize predictions with all True for root node\n",
    "        current_predictions = {\n",
    "            '': np.ones(n_samples, dtype=bool)\n",
    "        }\n",
    "        \n",
    "        # Process each level of the tree\n",
    "        for level in range(max(len(node) for node in self.splitting_nodes) + 1):\n",
    "            # Get nodes at this level\n",
    "            level_nodes = [node for node in self.splitting_nodes if len(node) == level]\n",
    "            \n",
    "            for node in level_nodes:\n",
    "                # Skip if node doesn't exist in current_predictions\n",
    "                if node not in current_predictions:\n",
    "                    continue\n",
    "                # Skip if no samples belong to this node\n",
    "                if not np.any(current_predictions[node]):\n",
    "                    continue\n",
    "                \n",
    "                # Get samples that belong to this node\n",
    "                node_mask = current_predictions[node]\n",
    "                X_node = X[node_mask]\n",
    "                \n",
    "                # Predict using the SVM model\n",
    "                if len(X_node) > 0:\n",
    "                    y_pred = self.models[node].predict(X_node)\n",
    "                    \n",
    "                    # Create masks for children\n",
    "                    left_child = node + '0'\n",
    "                    right_child = node + '1'\n",
    "                    \n",
    "                    # Initialize child predictions\n",
    "                    if left_child not in current_predictions:\n",
    "                        current_predictions[left_child] = np.zeros(n_samples, dtype=bool)\n",
    "                    if right_child not in current_predictions:\n",
    "                        current_predictions[right_child] = np.zeros(n_samples, dtype=bool)\n",
    "                    \n",
    "                    # Update predictions for children\n",
    "                    left_indices = np.where(node_mask)[0][y_pred == 0]\n",
    "                    right_indices = np.where(node_mask)[0][y_pred == 1]\n",
    "                    \n",
    "                    current_predictions[left_child][left_indices] = True\n",
    "                    current_predictions[right_child][right_indices] = True\n",
    "        \n",
    "        # For non-splitting nodes that just have a single child with '0' appended\n",
    "        for key in self.all_labels.keys():\n",
    "            if key not in self.splitting_nodes and key != '':\n",
    "                # Find the parent node\n",
    "                parent = key[:-1]\n",
    "                if parent in current_predictions and key not in current_predictions:\n",
    "                    # If this is a non-splitting child, it inherits parent's prediction\n",
    "                    current_predictions[key] = current_predictions[parent].copy()\n",
    "        \n",
    "        # Extract predictions for endmembers\n",
    "        endmember_predictions = {}\n",
    "        for endmember in self.endmembers:\n",
    "            if endmember in current_predictions:\n",
    "                endmember_predictions[endmember] = current_predictions[endmember]\n",
    "            else:\n",
    "                # If endmember not in predictions, try to find its parent\n",
    "                parent = endmember[:-1]\n",
    "                while parent and parent not in current_predictions:\n",
    "                    parent = parent[:-1]\n",
    "                if parent:\n",
    "                    endmember_predictions[endmember] = current_predictions[parent].copy()\n",
    "                else:\n",
    "                    endmember_predictions[endmember] = np.zeros(n_samples, dtype=bool)\n",
    "        \n",
    "        return endmember_predictions\n",
    "\n",
    "    def evaluate(self, X, gt_labels=None):\n",
    "        \"\"\"\n",
    "        Evaluate the model on test data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy.ndarray\n",
    "            Hyperspectral image data. Can be:\n",
    "            - Flattened array of shape (n_samples, n_bands)\n",
    "            - Cube of shape (height, width, n_bands) or (n_bands, height, width)\n",
    "        gt_labels : dict, optional\n",
    "            Ground truth labels for evaluation. If None, uses self.all_labels\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        accuracy : float\n",
    "            Overall accuracy of endmember predictions\n",
    "        \"\"\"\n",
    "        if gt_labels is None:\n",
    "            gt_labels = self.all_labels\n",
    "            \n",
    "        # if self.verbose:\n",
    "        if True:\n",
    "            print(\"Starting evaluation...\")\n",
    "            \n",
    "        predictions = self.predict(X)\n",
    "        \n",
    "        # Calculate accuracy for endmembers\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Evaluating accuracy for {len(self.endmembers)} endmembers...\")\n",
    "            \n",
    "        endmember_accuracies = {}\n",
    "        for endmember in self.endmembers:\n",
    "            if endmember in predictions and endmember in gt_labels:\n",
    "                pred = predictions[endmember]\n",
    "                true = gt_labels[endmember]\n",
    "                \n",
    "                endmember_correct = np.sum(pred == true)\n",
    "                endmember_total = len(true)\n",
    "                \n",
    "                correct += endmember_correct\n",
    "                total += endmember_total\n",
    "                \n",
    "                if self.verbose:\n",
    "                    endmember_acc = endmember_correct / endmember_total if endmember_total > 0 else 0\n",
    "                    endmember_accuracies[endmember] = endmember_acc\n",
    "                    print(f\"  Endmember '{endmember}': {endmember_acc:.4f} ({endmember_correct}/{endmember_total})\")\n",
    "        \n",
    "        accuracy = correct / total if total > 0 else 0\n",
    "        \n",
    "        # if self.verbose:\n",
    "        if True:\n",
    "            print(f\"Overall accuracy: {accuracy:.4f} ({correct}/{total} pixels)\")\n",
    "            \n",
    "        return accuracy\n",
    "\n",
    "    def plot_input_image(self, image_data, slice_idx=None, figsize=(15, 5), cmap='viridis', rgb_bands=(69, 46, 26)):\n",
    "        \"\"\"Plot input image data as single band, RGB composite, or flat image.\"\"\"\n",
    "        plt.figure(figsize=figsize)\n",
    "        \n",
    "        # Preprocess input data for more efficient handling\n",
    "        image_data = np.asarray(image_data)  # Ensure numpy array\n",
    "        \n",
    "        # Handle flat images (h*w, n_bands)\n",
    "        if len(image_data.shape) == 2 and image_data.shape[1] > 3:\n",
    "            slice_idx = slice_idx if slice_idx is not None else image_data.shape[1] // 2\n",
    "            band_data = image_data[:, slice_idx]\n",
    "            \n",
    "            # Try to reshape to a square-ish image if possible\n",
    "            side = int(np.sqrt(image_data.shape[0]))\n",
    "            reshaped_data = band_data.reshape(side, side) if side * side == image_data.shape[0] else band_data.reshape(-1, 1)\n",
    "            plt.imshow(np.flipud(np.rot90(reshaped_data)), cmap=cmap, aspect=0.1)\n",
    "            plt.colorbar(label='Intensity')\n",
    "            #plt.title(f'Input Image - Band {slice_idx} (Flattened)')\n",
    "        \n",
    "        # Handle 3D data (h, w, n_bands)\n",
    "        elif len(image_data.shape) == 3:\n",
    "            h, w, n_bands = image_data.shape\n",
    "            slice_idx = slice_idx if slice_idx is not None else n_bands // 2\n",
    "            \n",
    "            # Create RGB composite if bands are specified\n",
    "            if rgb_bands is not None and len(rgb_bands) == 3:\n",
    "                r_idx, g_idx, b_idx = rgb_bands\n",
    "                if max(rgb_bands) > n_bands - 1:\n",
    "                    raise ValueError(f\"RGB band indices {rgb_bands} exceed available bands (0-{n_bands-1})\")\n",
    "                \n",
    "                # More efficient RGB creation - preallocate and process in one go\n",
    "                rgb_img = np.zeros((h, w, 3), dtype=np.float32)\n",
    "                for i, band_idx in enumerate([r_idx, g_idx, b_idx]):\n",
    "                    band = image_data[:, :, band_idx].astype(np.float32)\n",
    "                    # Normalize only if needed\n",
    "                    band_min, band_max = band.min(), band.max()\n",
    "                    if band_min != band_max:\n",
    "                        band = (band - band_min) / (band_max - band_min)\n",
    "                    rgb_img[:, :, i] = band\n",
    "                \n",
    "                plt.imshow(np.flipud(np.rot90(rgb_img)), aspect=0.1)\n",
    "                #plt.title(f'RGB Composite (R:{r_idx}, G:{g_idx}, B:{b_idx})')\n",
    "            else:\n",
    "                plt.imshow(np.flipud(np.rot90(image_data[:, :, slice_idx])), cmap=cmap, aspect=0.1)\n",
    "                plt.colorbar(label='Intensity')\n",
    "                #plt.title(f'Input Image - Band {slice_idx}')\n",
    "        \n",
    "        # Handle other cases\n",
    "        else:\n",
    "            plt.imshow(np.flipud(np.rot90(image_data)), cmap=cmap, aspect=0.1)\n",
    "            plt.colorbar(label='Intensity')\n",
    "            #plt.title('Input Image')\n",
    "        \n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_ground_truth(self, labels=None, key='', figsize=(15, 5), cmap='viridis'):\n",
    "        \"\"\"Plot ground truth labels.\"\"\"\n",
    "        # Preprocess input\n",
    "        labels = self.all_labels if labels is None else labels\n",
    "        shape = labels[''].shape\n",
    "        reshape = labels[''].reshape(-1, HYPSO_HEIGHT)\n",
    "        height, width = reshape.shape[0], reshape.shape[1]\n",
    "        \n",
    "        # Handle dictionary-type labels\n",
    "        if isinstance(labels, dict):\n",
    "            if key == '':\n",
    "                # Create a combined view of all endmembers\n",
    "                plt.figure(figsize=figsize)\n",
    "                first_val = list(labels.values())[0]\n",
    "                \n",
    "                if len(first_val.shape) == 1:\n",
    "                    # Use the dimensions we already determined\n",
    "                    combined_labels = np.zeros((height, width), dtype=int)\n",
    "                    endmember_keys = [k for k in labels.keys() if k in self.endmembers]\n",
    "                    \n",
    "                    # Process data before visualization\n",
    "                    for i, k in enumerate(endmember_keys):\n",
    "                        if k == '':\n",
    "                            continue\n",
    "                        try:\n",
    "                            label_reshaped = labels[k].reshape(height, width)\n",
    "                            combined_labels[label_reshaped == 1] = i + 1\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error reshaping key '{k}': {e}\")\n",
    "                            continue\n",
    "                    \n",
    "                    # Rotate the image 90 degrees and set aspect ratio\n",
    "                    plt.imshow(np.flipud(np.rot90(combined_labels)), cmap=cmap, aspect=0.1)\n",
    "                    #plt.title('Ground Truth Labels - Combined View')\n",
    "                else:\n",
    "                    print(\"Cannot display labels: unexpected format\")\n",
    "            else:\n",
    "                # Plot just the specified key\n",
    "                if key not in labels:\n",
    "                    print(f\"Key '{key}' not found in labels\")\n",
    "                    return\n",
    "                \n",
    "                label_data = labels[key]\n",
    "                if len(label_data.shape) == 1:\n",
    "                    try:\n",
    "                        # Use the dimensions we already determined\n",
    "                        label_data = label_data.reshape(height, width)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reshaping label data: {e}\")\n",
    "                        return\n",
    "                \n",
    "                plt.figure(figsize=figsize)\n",
    "                # Use viridis colormap for the binary data and rotate the image 90 degrees\n",
    "                plt.imshow(np.flipud(np.rot90(label_data)), cmap=cmap, vmin=0, vmax=1, aspect=0.1)\n",
    "                #plt.title(f'Ground Truth Label for \"{key}\"')\n",
    "        \n",
    "        # Handle array-type labels\n",
    "        elif hasattr(labels, 'shape'):\n",
    "            # Preprocess array data\n",
    "            labels = np.asarray(labels)  # Ensure numpy array\n",
    "            if len(labels.shape) == 1:\n",
    "                try:\n",
    "                    # Use the dimensions we already determined\n",
    "                    labels = labels.reshape(height, width)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reshaping label data: {e}\")\n",
    "                    return\n",
    "            \n",
    "            plt.figure(figsize=figsize)\n",
    "            # Rotate the image 90 degrees and set aspect ratio\n",
    "            plt.imshow(np.flipud(np.rot90(labels)), cmap=cmap, aspect=0.1)\n",
    "            #plt.title('Ground Truth Labels')\n",
    "        else:\n",
    "            print(\"Cannot plot labels: unsupported format\")\n",
    "            return\n",
    "        \n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_prediction(self, prediction=None, key='', figsize=(15, 5), cmap='viridis'):\n",
    "        \"\"\"Plot prediction results.\"\"\"\n",
    "        # Preprocess input\n",
    "        if prediction is None:\n",
    "            if not hasattr(self, 'last_prediction'):\n",
    "                print(\"No prediction available. Run predict() first.\")\n",
    "                return\n",
    "            prediction = self.last_prediction\n",
    "        \n",
    "        # Determine dimensions from the data\n",
    "        first_key = next(iter(prediction))\n",
    "        shape = prediction[first_key].shape\n",
    "        reshape = prediction[first_key].reshape(-1, HYPSO_HEIGHT)\n",
    "        height, width = reshape.shape[0], reshape.shape[1]\n",
    "        \n",
    "        # Handle dictionary-type predictions\n",
    "        if isinstance(prediction, dict):\n",
    "            if key == '':\n",
    "                plt.figure(figsize=figsize)\n",
    "                first_val = list(prediction.values())[0]\n",
    "                \n",
    "                if len(first_val.shape) == 1:\n",
    "                    combined_pred = np.zeros((height, width), dtype=int)\n",
    "                    endmember_keys = [k for k in prediction.keys() if k in self.endmembers]\n",
    "                    \n",
    "                    # Process data before visualization\n",
    "                    for i, k in enumerate(endmember_keys):\n",
    "                        if k == '':\n",
    "                            continue\n",
    "                        try:\n",
    "                            pred_reshaped = prediction[k].reshape(height, width)\n",
    "                            combined_pred[pred_reshaped == True] = i + 1\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error reshaping key '{k}': {e}\")\n",
    "                            continue\n",
    "                    \n",
    "                    # Rotate the image 90 degrees and set aspect ratio\n",
    "                    plt.imshow(np.flipud(np.rot90(combined_pred)), cmap=cmap, vmin=0, aspect=0.1)\n",
    "                    #plt.title('Predictions - Combined View')\n",
    "                else:\n",
    "                    print(\"Cannot display predictions: unexpected format\")\n",
    "            else:\n",
    "                if key not in prediction:\n",
    "                    print(f\"Key '{key}' not found in predictions\")\n",
    "                    return\n",
    "                \n",
    "                pred_data = prediction[key]\n",
    "                if len(pred_data.shape) == 1:\n",
    "                    try:\n",
    "                        # Use the dimensions we already determined\n",
    "                        pred_data = pred_data.reshape(height, width)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reshaping prediction data: {e}\")\n",
    "                        return\n",
    "                \n",
    "                plt.figure(figsize=figsize)\n",
    "                # Use viridis colormap for the binary data, rotate 90 degrees and set aspect ratio\n",
    "                plt.imshow(np.flipud(np.rot90(pred_data)), cmap=cmap, vmin=0, vmax=1, aspect=0.1)\n",
    "                # plt.title(f'Prediction for key \"{key}\"')\n",
    "        \n",
    "        # Handle array-type predictions\n",
    "        elif hasattr(prediction, 'shape'):\n",
    "            # Preprocess array data\n",
    "            prediction = np.asarray(prediction)  # Ensure numpy array\n",
    "            if len(prediction.shape) == 1:\n",
    "                try:\n",
    "                    # Use the dimensions we already determined\n",
    "                    prediction = prediction.reshape(height, width)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reshaping prediction data: {e}\")\n",
    "                    return\n",
    "            \n",
    "            plt.figure(figsize=figsize)\n",
    "            # Rotate the image 90 degrees and set aspect ratio\n",
    "            plt.imshow(np.flipud(np.rot90(prediction)), cmap=cmap, aspect=0.1, )\n",
    "            # plt.title('Prediction')\n",
    "        else:\n",
    "            print(\"Cannot plot prediction: unsupported format\")\n",
    "            return\n",
    "        \n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def save_model_parameters(self, filename='decision_tree_model.pkl'):\n",
    "        \"\"\"\n",
    "        Save the trained SVM parameters of the decision tree to a file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        filename : str\n",
    "            The name of the file to save the model parameters to.\n",
    "        \"\"\"\n",
    "        import pickle\n",
    "        import os\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Preparing to save model parameters...\")\n",
    "            # for node, model in self.models.items():\n",
    "            #     if hasattr(model, 'coef_'):\n",
    "            #         print(f\"Node {node} SVM weights shape: {model.coef_.shape}\")\n",
    "            #         print(f\"Node {node} SVM weights: {model.coef_}\")\n",
    "            #     if hasattr(model, 'intercept_'):\n",
    "            #         print(f\"Node {node} SVM intercept: {model.intercept_}\")\n",
    "        \n",
    "        # Create a dictionary to store all model data\n",
    "        model_data = {\n",
    "            'models': self.models,  # This is your dictionary of SVM models\n",
    "            'endmembers': self.endmembers,\n",
    "            'splitting_nodes': self.splitting_nodes,\n",
    "            'all_labels': self.all_labels  # Save the labels too\n",
    "        }\n",
    "        \n",
    "        # Ensure weights folder exists\n",
    "        weights_folder = 'weights'\n",
    "        os.makedirs(weights_folder, exist_ok=True)\n",
    "        \n",
    "        # Create full path to save file in weights folder\n",
    "        filepath = os.path.join(weights_folder, filename)\n",
    "        \n",
    "        try:\n",
    "            if self.verbose:\n",
    "                print(f\"Saving model to {filepath}...\")\n",
    "            with open(filepath, 'wb') as f:\n",
    "                pickle.dump(model_data, f)\n",
    "            print(f\"Model parameters successfully saved to {filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving model parameters: {e}\")\n",
    "\n",
    "    def load_model_parameters(self, filename='decision_tree_model.pkl'):\n",
    "        \"\"\"\n",
    "        Load the trained SVM parameters for the decision tree from a file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        filename : str\n",
    "            The name of the file to load the model parameters from.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        bool\n",
    "            True if loading was successful, False otherwise.\n",
    "        \"\"\"\n",
    "        import pickle\n",
    "        import os\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Attempting to load model parameters from {filename}...\")\n",
    "        \n",
    "        # Create full path to load file from weights folder\n",
    "        # filepath = os.path.join('weights', filename)\n",
    "        filepath=filename\n",
    "        \n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"Error: Model file {filepath} not found\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            if self.verbose:\n",
    "                print(\"Reading model file...\")\n",
    "            with open(filepath, 'rb') as f:\n",
    "                model_data = pickle.load(f)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(\"Restoring model components...\")\n",
    "            \n",
    "            # Restore the model components\n",
    "            self.models = model_data['models']\n",
    "            self.endmembers = model_data['endmembers']\n",
    "            self.splitting_nodes = model_data['splitting_nodes']\n",
    "            \n",
    "            # Optionally restore labels if they were saved\n",
    "            if 'all_labels' in model_data:\n",
    "                self.all_labels = model_data['all_labels']\n",
    "                if self.verbose:\n",
    "                    print(\"Labels restored from saved model\")\n",
    "            \n",
    "            print(f\"Model parameters successfully loaded from {filepath}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model parameters: {e}\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pipeline(image_path,tree_path, deh_path, hypso=1, verbose=False):\n",
    "    # Check if the image path is a .npy or .nc file\n",
    "    \n",
    "    if image_path.endswith('.npy'):\n",
    "        # Load numpy array directly\n",
    "        image_data = np.load(image_path)\n",
    "        channels=image_data.shape[-1]    \n",
    "        if image_data.ndim == 3:\n",
    "            image_data_flat = image_data.reshape(-1,channels)\n",
    "    elif image_path.endswith('.nc'):\n",
    "        # Use hypso to load .nc file\n",
    "        try:\n",
    "            image_data= nc_to_image(image_path, hypso, verbose, machi=True)\n",
    "        except ImportError:\n",
    "            print(\"Error: hypso package is required to process .nc files\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing .nc file: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Unsupported file format: {image_path}\")\n",
    "        print(\"Supported formats: .npy, .nc\")\n",
    "        return None\n",
    "    \n",
    "    # Print information about the image data values\n",
    "    print(f\"Image data shape: {image_data.shape}\")\n",
    "    \n",
    "    #create label form deh\n",
    "    image_gt = generate_huH_labels(image_data, deh_path, verbose=verbose, channels=channels)\n",
    "\n",
    "    # Load the decision tree model\n",
    "    tree = BinaryDecisionTree(image_gt,verbose=verbose)\n",
    "    if not tree.load_model_parameters(tree_path):\n",
    "        print(\"Failed to load tree model\")\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Make predictions using the tree\n",
    "    predictions = tree.predict(image_data)\n",
    "    evaluation = tree.evaluate(image_data,image_gt)\n",
    "    print(evaluation)\n",
    "    \n",
    "    tree.plot_input_image(image_data.reshape(-1,HYPSO_HEIGHT,channels))\n",
    "    tree.plot_ground_truth(image_gt)\n",
    "    tree.plot_prediction(predictions)\n",
    "    # Return the predictions\n",
    "    return predictions\n",
    "\n",
    "def nc_to_image(image_path, hypso=1, verbose=False, machi=False):\n",
    "    if hypso==2:\n",
    "        satobj = Hypso2(path=image_path, verbose=verbose)\n",
    "    else:\n",
    "        satobj = Hypso1(path=image_path, verbose=verbose)\n",
    "    # Reshape if needed (depends on hypso output format)\n",
    "    satobj.generate_l1b_cube()\n",
    "    data = satobj.l1b_cube\n",
    "    #satobj.generate_l1c_cube()\n",
    "    #satobj.generate_l1d_cube()\n",
    "    image_data = np.array(data)[:,:,6:118]\n",
    "    image_data_flat=image_data.reshape(-1,112)\n",
    "\n",
    "    return image_data_flat\n",
    "\n",
    "def generate_huH_labels(image_file, deh_path, verbose=False, channels=120):\n",
    "    DEH_model=deh.DEH(no_negative_residuals=True)\n",
    "    DEH_model.load(deh_path)\n",
    "    image_file_cube=image_file.reshape(-1,HYPSO_HEIGHT,channels)\n",
    "    DEH_model.plot_size=(image_file_cube[0], image_file_cube[1])\n",
    "    DEH_model.simple_predict(image_file)\n",
    "    DEH_model.binarize_lmdas()\n",
    "    DEH_model.lmda_2_map()\n",
    "    labels_dict={}\n",
    "    for node in DEH_model.nodes:\n",
    "        labels = DEH_model.nodes[node].map.flatten()\n",
    "        labels_dict[node] = labels\n",
    "    return labels_dict\n",
    "\n",
    "def save_tree_svms_to_binary_remapped(tree, output_dir):\n",
    "    \"\"\"\n",
    "    Save all SVM models in the decision tree as binary files, with class labels remapped\n",
    "    to sequential integers starting from 0.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    tree : BinaryDecisionTree\n",
    "        The decision tree containing trained SVM models\n",
    "    output_dir : str\n",
    "        Directory where the binary files will be saved\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        A dictionary containing the mapping information\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import struct\n",
    "    import numpy as np\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get all splitting nodes with trained models\n",
    "    splitting_nodes = tree.splitting_nodes\n",
    "    print(f\"Found {len(splitting_nodes)} splitting nodes in the tree\")\n",
    "    \n",
    "    # Map each binary path to the corresponding endmember decimal value\n",
    "    path_to_value = {}\n",
    "    for i, endmember_label in enumerate(tree.endmembers):\n",
    "        if endmember_label:  # Skip empty endmembers if any\n",
    "            # Convert binary string to decimal\n",
    "            em_value = int(endmember_label, 2)\n",
    "            path_to_value[endmember_label] = em_value\n",
    "    \n",
    "    # Collect all unique class values\n",
    "    all_classes = set()\n",
    "    \n",
    "    # Identify all class values\n",
    "    for node in splitting_nodes:\n",
    "        # Skip nodes without models\n",
    "        if node not in tree.models:\n",
    "            continue\n",
    "        \n",
    "        # Find all endmembers that would go to the left branch (node+'0')\n",
    "        left_values = []\n",
    "        # Find all endmembers that would go to the right branch (node+'1')\n",
    "        right_values = []\n",
    "        \n",
    "        for path, value in path_to_value.items():\n",
    "            # Only process endmembers whose paths are long enough to be affected by this node\n",
    "            if len(path) > len(node):\n",
    "                # Check if this path goes through the current node\n",
    "                if path.startswith(node):\n",
    "                    # This is a descendent of the current node, check which branch\n",
    "                    next_bit = path[len(node)]\n",
    "                    if next_bit == '0':\n",
    "                        left_values.append(value)\n",
    "                    else:\n",
    "                        right_values.append(value)\n",
    "        \n",
    "        # Find lowest class in each branch (by decimal value)\n",
    "        if left_values:\n",
    "            all_classes.add(min(left_values))\n",
    "        if right_values:\n",
    "            all_classes.add(min(right_values))\n",
    "    \n",
    "    # Sort classes for consistent mapping\n",
    "    original_classes = sorted(list(all_classes))\n",
    "    \n",
    "    # Create mapping from original classes to sequential integers\n",
    "    class_mapping = {orig: i for i, orig in enumerate(original_classes)}\n",
    "    reverse_mapping = {i: orig for i, orig in enumerate(original_classes)}\n",
    "    \n",
    "    print(f\"Found {len(original_classes)} unique class values in the tree\")\n",
    "    print(f\"Original classes: {original_classes}\")\n",
    "    print(f\"Remapped to: {list(range(len(original_classes)))}\")\n",
    "    print(f\"Mapping: {class_mapping}\")\n",
    "    \n",
    "    # Count of successfully saved models\n",
    "    saved_count = 0\n",
    "    \n",
    "    for node in splitting_nodes:\n",
    "        # Check if this node has an SVM model\n",
    "        if node not in tree.models:\n",
    "            print(f\"Warning: Node '{node}' is marked as a splitting node but has no SVM model\")\n",
    "            continue\n",
    "        \n",
    "        # Get the SVM model for this node\n",
    "        svm_model = tree.models[node]\n",
    "        \n",
    "        # Find all endmembers that would go to the left branch (node+'0')\n",
    "        left_values = []\n",
    "        # Find all endmembers that would go to the right branch (node+'1')\n",
    "        right_values = []\n",
    "        \n",
    "        for path, value in path_to_value.items():\n",
    "            # Only process endmembers whose paths are long enough to be affected by this node\n",
    "            if len(path) > len(node):\n",
    "                # Check if this path goes through the current node\n",
    "                if path.startswith(node):\n",
    "                    # This is a descendent of the current node, check which branch\n",
    "                    next_bit = path[len(node)]\n",
    "                    if next_bit == '0':\n",
    "                        left_values.append(value)\n",
    "                    else:\n",
    "                        right_values.append(value)\n",
    "        \n",
    "        # Find lowest class in each branch (by decimal value)\n",
    "        left_min = min(left_values) if left_values else 0\n",
    "        right_min = min(right_values) if right_values else 0\n",
    "        \n",
    "        # Remap class values to sequential indices\n",
    "        left_mapped = class_mapping[left_min] if left_min in class_mapping else 0 \n",
    "        right_mapped = class_mapping[right_min] if right_min in class_mapping else 0\n",
    "        \n",
    "        # Format the filename using remapped values\n",
    "        svm_model_name = f\"lsm{left_mapped:02d}{right_mapped:02d}\"\n",
    "        filepath = os.path.join(output_dir, svm_model_name)\n",
    "        \n",
    "        print(f\"Node {node}: Splitting between left branch (orig={left_min}, remapped={left_mapped}) and right branch (orig={right_min}, remapped={right_mapped})\")\n",
    "        \n",
    "        # Debug: Check if weights are non-zero before saving\n",
    "        if hasattr(svm_model, 'coef_'):\n",
    "            weights = svm_model.coef_[0]\n",
    "            non_zero_count = np.sum(np.abs(weights) > 1e-10)\n",
    "            if non_zero_count == 0:\n",
    "                print(f\"  WARNING: All weights are zero for node {node}!\")\n",
    "                \n",
    "                # Try to access the SVM model's raw data directly\n",
    "                if hasattr(svm_model, '_impl') and hasattr(svm_model._impl, 'raw_coef_'):\n",
    "                    print(\"  Attempting to use _impl.raw_coef_ instead...\")\n",
    "                    weights = svm_model._impl.raw_coef_\n",
    "                else:\n",
    "                    print(\"  Could not find alternative weight source\")\n",
    "        \n",
    "        # Open the file for binary writing\n",
    "        with open(filepath, \"wb\") as file:\n",
    "            # Write REMAPPED class labels as uint8\n",
    "            file.write(struct.pack(\"BB\", left_mapped, right_mapped))\n",
    "            \n",
    "            # Write intercept as float32\n",
    "            intercept = svm_model.intercept_[0]\n",
    "            file.write(struct.pack('f', intercept))\n",
    "            \n",
    "            # Write weights as float32\n",
    "            if hasattr(svm_model, 'coef_'):\n",
    "                weights = svm_model.coef_[0]\n",
    "                file.write(struct.pack(f'{len(weights)}f', *weights))\n",
    "                print(f\"  Saved {len(weights)} weights (non-zero: {non_zero_count})\")\n",
    "            else:\n",
    "                print(\"  WARNING: Model does not have coef_ attribute!\")\n",
    "        \n",
    "        print(f\"  Saved model to {svm_model_name}\")\n",
    "        saved_count += 1\n",
    "    \n",
    "    print(f\"Successfully saved {saved_count} SVM models to {output_dir}\")\n",
    "    \n",
    "    # Return the mapping for reference\n",
    "    return {\n",
    "        \"original_to_remapped\": class_mapping,\n",
    "        \"remapped_to_original\": reverse_mapping,\n",
    "        \"original_classes\": original_classes,\n",
    "        \"remapped_classes\": list(range(len(original_classes)))\n",
    "    }\n",
    "\n",
    "def save_prediction(predictions, save_name):\n",
    "        \"\"\"\n",
    "        Save the prediction results to a file\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        predictions : dict\n",
    "            Dictionary containing prediction masks for each endmember\n",
    "        save_path : str\n",
    "            Path where to save the predictions\n",
    "        \"\"\"\n",
    "        save_name = save_name + '_pred.npy'\n",
    "        save_path=os.path.join(PRED_DIR,save_name)\n",
    "        np.save(save_path, predictions)\n",
    "\n",
    "def load_prediction(prediction_name):\n",
    "    \"\"\"\n",
    "    Load the prediction results from a file\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    prediction_name : str\n",
    "        filename of prediction file\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing prediction masks for each endmember\n",
    "    \"\"\"\n",
    "    prediction_path=os.path.join(PRED_DIR,prediction_name)\n",
    "    return np.load(prediction_path, allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HYPSO 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_raw_path=os.path.join(TREE_DIR,'combined_10_L1A_120_16end_TREE.pkl')\n",
    "tree_corrected_path=os.path.join(TREE_DIR,'combined_10_L1D_112_MACHI_16end_TREE.pkl')\n",
    "print(tree_raw_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_10_L1A_120                 = np.load(os.path.join(IMAGES_DIR,'combined_10_L1A_120.npy'))\n",
    "print(combined_10_L1A_120.shape)\n",
    "combined_10_L1D_112_MACHI           = np.load(os.path.join(IMAGES_DIR,'combined_10_L1D_112_MACHI.npy'))\n",
    "print(combined_10_L1D_112_MACHI.shape)\n",
    "combined_10_L1D_112_MACHI_labels    = np.load(os.path.join(LABELS_DIR,'combined_10_L1D_112_MACHI_16end_labels.npy'), allow_pickle=True).item()\n",
    "print(combined_10_L1D_112_MACHI_labels.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tampa_2024_11_12_L1A_120        =np.load(os.path.join(IMAGES_DIR, 'tampa_2024-11-12T15-31-55Z-l1a_flat_L1A_120.npy'))\n",
    "tampa_2024_11_12_L1A_120_labels =np.load(os.path.join(LABELS_DIR, 'tampa_2024-11-12T15-31-55Z-l1a_flat_L1D_112_MACHI_16end_labels.npy'),allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caspiansea1_2025_04_08_l1A_120=np.load(os.path.join(IMAGES_DIR, 'caspiansea1_2025-04-08T07-11-56Z-l1a_flat_L1A_120.npy'))\n",
    "caspiansea1_2025_04_08_l1A_120_labels=np.load(os.path.join(LABELS_DIR, 'caspiansea1_2025-04-08T07-11-56Z-l1a_flat_L1D_112_MACHI_16end_labels.npy'),allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vancouver_2025_05_04_L1A_120=np.load(os.path.join(IMAGES_DIR, 'vancouver_2025-05-04T19-12-24Z-l1a_flat_L1A_120.npy'))\n",
    "vancouver_2025_05_04_L1A_120_labels=np.load(os.path.join(LABELS_DIR, 'vancouver_2025-05-04T19-12-24Z-l1a_flat_L1D_112_MACHI_16end_labels.npy'),allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yucatan2_2025_02_06_L1A         = np.load(f'{IMAGES_DIR}//yucatan2_2025-02-06T16-01-18Z-l1a_flat_L1A_120.npy')\n",
    "kemigawa_2024_12_17_L1A         = np.load(f'{IMAGES_DIR}//kemigawa_2024-12-17T01-01-32Z-l1a_flat_L1A_120.npy')\n",
    "chapala_2025_02_24_L1A          = np.load(f'{IMAGES_DIR}//chapala_2025-02-24T16-52-47Z-l1a_flat_L1A_120.npy')\n",
    "grizzlybay_2025_01_27_L1A       = np.load(f'{IMAGES_DIR}//grizzlybay_2025-01-27T18-19-56Z-l1a_flat_L1A_120.npy')\n",
    "victoriaLand_2025_02_07_L1A     = np.load(f'{IMAGES_DIR}//victoriaLand_2025-02-07T20-35-33Z-l1a_flat_L1A_120.npy')\n",
    "catala_2025_01_28_L1A           = np.load(f'{IMAGES_DIR}//catala_2025-01-28T19-17-32Z-l1a_flat_L1A_120.npy')\n",
    "khnifiss_2025_02_12_L1A         = np.load(f'{IMAGES_DIR}//khnifiss_2025-02-12T11-05-35Z-l1a_flat_L1A_120.npy')\n",
    "menindee_2025_02_18_L1A         = np.load(f'{IMAGES_DIR}//menindee_2025-02-18T00-10-42Z-l1a_flat_L1A_120.npy')\n",
    "tampa_2024_11_12_L1A            = np.load(f'{IMAGES_DIR}//tampa_2024-11-12T15-31-55Z-l1a_flat_L1A_120.npy')\n",
    "falklandsatlantic_2024_12_18_L1A= np.load(f'{IMAGES_DIR}//falklandsatlantic_2024-12-18T13-25-18Z-l1a_flat_L1A_120.npy')\n",
    "L1A_dict = {\n",
    "    'yucatan2_2025_02_06': yucatan2_2025_02_06_L1A,\n",
    "    'kemigawa_2024_12_17': kemigawa_2024_12_17_L1A,\n",
    "    'chapala_2025_02_24': chapala_2025_02_24_L1A,\n",
    "    'grizzlybay_2025_01_27': grizzlybay_2025_01_27_L1A,\n",
    "    'victoriaLand_2025_02_07': victoriaLand_2025_02_07_L1A,\n",
    "    'catala_2025_01_28': catala_2025_01_28_L1A,\n",
    "    'khnifiss_2025_02_12': khnifiss_2025_02_12_L1A,\n",
    "    'menindee_2025_02_18': menindee_2025_02_18_L1A,\n",
    "    'tampa_2024_11_12': tampa_2024_11_12_L1A,\n",
    "    'falklandsatlantic_2024_12_18': falklandsatlantic_2024_12_18_L1A\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data_raw          =combined_10_L1A_120\n",
    "save_name_raw           ='combined_10_L1A_120'\n",
    "image_data_corrected    =combined_10_L1D_112_MACHI\n",
    "save_name_corrected     ='combined_10_L1D_112_MACHI'\n",
    "image_gt                =combined_10_L1D_112_MACHI_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_raw=load_prediction('combined_10_L1A_120_pred.npy')\n",
    "prediction_corrected=load_prediction('combined_10_L1D_112_MACHI_pred.npy')\n",
    "predictions_tampa_2024_11_12=load_prediction('tampa_2024_11_12_pred.npy')\n",
    "predictions_caspiansea1_2025_04_08=load_prediction('caspiansea1_2025_04_08_pred.npy')\n",
    "prediction_vancouver_2025_05_04_L1A_120=load_prediction('vancouver_2025_05_04_L1A_120_pred.npy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HYPSO 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H2_tree_raw_path        =os.path.join(TREE_DIR,'H2_10img_16end_L1A_120_stabelized_aa_TREE.pkl')\n",
    "H2_tree_corrected_path  =os.path.join(TREE_DIR,'H2_10img_16end_L1D_112_MACHI_stabelized_aa_TREE.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H2_10_L1A_120                 = np.load(os.path.join(IMAGES_DIR,'H2_10_L1A_120.npy'))\n",
    "print(H2_10_L1A_120.shape)\n",
    "H2_10_L1D_112_MACHI           = np.load(os.path.join(IMAGES_DIR,'H2_10_L1D_112_MACHI.npy'))\n",
    "print(H2_10_L1D_112_MACHI.shape)\n",
    "H2_10_L1D_112_MACHI_labels    = np.load(os.path.join(LABELS_DIR,'H2_10_L1D_112_MACHI_16end_labels.npy'), allow_pickle=True).item()\n",
    "print(H2_10_L1D_112_MACHI_labels.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data_raw          =H2_10_L1A_120\n",
    "save_name_raw           ='H2_10_L1A_120'\n",
    "image_data_corrected    =H2_10_L1D_112_MACHI\n",
    "save_name_corrected     ='H2_10_L1D_112_MACHI'\n",
    "image_gt                =H2_10_L1D_112_MACHI_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H2_L1A_files = [\n",
    "    'yucatan1_2025-04-01_flat_L1A_120_H2.npy',\n",
    "    'kemigawa_2025-01-22_flat_L1A_120_H2.npy',\n",
    "    'chapala_2025-03-25_flat_L1A_120_H2.npy',\n",
    "    'grizzlybay_2025-01-22_flat_L1A_120_H2.npy',\n",
    "    'victoriaLand_2025-03-16_flat_L1A_120_H2.npy',\n",
    "    'mjosa_2025-05-12_flat_L1A_120_H2.npy',\n",
    "    'gobabeb_2025-04-25_flat_L1A_120_H2.npy',\n",
    "    'menindee_2025-05-09_flat_L1A_120_H2.npy',\n",
    "    'erie_2025-05-10_flat_L1A_120_H2.npy',\n",
    "    'falklandsatlantic_2025-03-03_flat_L1A_120_H2.npy'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H2_L1D_files = [\n",
    "    'yucatan1_2025-04-01_flat_L1D_112_H2_MACHI.npy',\n",
    "    'kemigawa_2025-01-22_flat_L1D_112_H2_MACHI.npy',\n",
    "    'chapala_2025-03-25_flat_L1D_112_H2_MACHI.npy',\n",
    "    'grizzlybay_2025-01-22_flat_L1D_112_H2_MACHI.npy',\n",
    "    'victoriaLand_2025-03-16_flat_L1D_112_H2_MACHI.npy',\n",
    "    'mjosa_2025-05-12_flat_L1D_112_H2_MACHI.npy',\n",
    "    'gobabeb_2025-04-25_flat_L1D_112_H2_MACHI.npy',\n",
    "    'menindee_2025-05-09_flat_L1D_112_H2_MACHI.npy',\n",
    "    'erie_2025-05-10_flat_L1D_112_H2_MACHI.npy',\n",
    "    'falklandsatlantic_2025-03-03_flat_L1D_112_H2_MACHI.npy'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING AND EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_raw=BinaryDecisionTree(verbose=True)\n",
    "tree_raw.initialize_tree_structure(image_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_corrected=BinaryDecisionTree(verbose=True)\n",
    "tree_corrected.initialize_tree_structure(image_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_raw.train(image_data_raw,image_gt)\n",
    "evaluation_raw = tree_raw.evaluate(image_data_raw,image_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_corrected.train(image_data_corrected,image_gt)\n",
    "evaluation_corrected = tree_corrected.evaluate(image_data_corrected,image_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_raw.save_model_parameters(H2_tree_raw_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_corrected.save_model_parameters(H2_tree_corrected_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVE SVM TO BINARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models with remapped classes\n",
    "save_folder = SVM_DIR\n",
    "mapping = save_tree_svms_to_binary_remapped(tree_raw, save_folder)\n",
    "\n",
    "# Print the mapping for reference\n",
    "print(\"\\nClass mapping:\")\n",
    "for orig, remapped in mapping[\"original_to_remapped\"].items():\n",
    "    print(f\"Original class {orig} â†’ Remapped class {remapped}\")\n",
    "\n",
    "# Save the mapping for later use\n",
    "with open(f\"{save_folder}/H2_16_class_mapping.json\", \"w\") as f:\n",
    "    json.dump(mapping, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_raw=BinaryDecisionTree(verbose=True)\n",
    "tree_raw.load_model_parameters(tree_raw_path)\n",
    "H2_tree_raw=BinaryDecisionTree(verbose=True)\n",
    "H2_tree_raw.load_model_parameters(H2_tree_raw_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_corrected=BinaryDecisionTree(verbose=True)\n",
    "tree_corrected.load_model_parameters(tree_corrected_path)\n",
    "H2_tree_corrected=BinaryDecisionTree(verbose=True)\n",
    "H2_tree_corrected.load_model_parameters(H2_tree_corrected_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDICT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HYPSO 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_raw = tree_raw.predict(image_data_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_corrected = tree_corrected.predict(image_data_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_tampa_2024_11_12=tree_raw.predict(tampa_2024_11_12_L1A_120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_caspiansea1_2025_04_08=tree_raw.predict(caspiansea1_2025_04_08_l1A_120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_vancouver_2025_05_04_L1A_120=tree_raw.predict(vancouver_2025_05_04_L1A_120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, image in L1A_dict.items():\n",
    "    prediction = tree_raw.predict(image)\n",
    "    save_prediction(prediction, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HYPSO 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in H2_L1A_files:\n",
    "    image_path=os.path.join(IMAGES_DIR, file)\n",
    "    image=np.load(image_path)\n",
    "    prediction=H2_tree_raw.predict(image)\n",
    "    save_name = file.replace('.npy', '_16end_pred.npy')\n",
    "    save_path=os.path.join(PRED_DIR,save_name)\n",
    "    np.save(save_path, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVE PRED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_prediction(prediction_raw, save_name_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_prediction(prediction_corrected, save_name_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_prediction(predictions_tampa_2024_11_12, 'tampa_2024_11_12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_prediction(predictions_caspiansea1_2025_04_08, 'caspiansea1_2025_04_08')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_prediction(prediction_vancouver_2025_05_04_L1A_120, 'vancouver_2025_05_04_L1A_120')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_raw.plot_input_image(vancouver_2025_05_04_L1A_120.reshape(-1,HYPSO_HEIGHT,120))\n",
    "tree_raw.plot_ground_truth(vancouver_2025_05_04_L1A_120_labels, cmap='viridis')\n",
    "tree_raw.plot_prediction(prediction_vancouver_2025_05_04_L1A_120, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_raw.plot_input_image(caspiansea1_2025_04_08_l1A_120.reshape(-1,HYPSO_HEIGHT,120))\n",
    "tree_raw.plot_ground_truth(caspiansea1_2025_04_08_l1A_120_labels, cmap='viridis')\n",
    "tree_raw.plot_prediction(predictions_caspiansea1_2025_04_08, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_raw.plot_input_image(tampa_2024_11_12_L1A_120.reshape(-1,HYPSO_HEIGHT,120))\n",
    "tree_raw.plot_ground_truth(tampa_2024_11_12_L1A_120_labels, cmap='viridis')\n",
    "tree_raw.plot_prediction(predictions_tampa_2024_11_12, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_raw.plot_input_image(image_data_raw.reshape(-1,HYPSO_HEIGHT,120))\n",
    "tree_raw.plot_ground_truth(image_gt, cmap='viridis')\n",
    "tree_raw.plot_prediction(prediction_raw, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_corrected.plot_input_image(image_data_corrected.reshape(-1,HYPSO_HEIGHT,112))\n",
    "tree_corrected.plot_ground_truth(image_gt, cmap='viridis')\n",
    "tree_corrected.plot_prediction(prediction_corrected, cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEBUG"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hypso",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
