{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib as il\n",
    "from hypso import Hypso1, Hypso2\n",
    "from sklearn.svm import LinearSVC\n",
    "import src.deh as deh\n",
    "import copy\n",
    "il.reload(deh)\n",
    "sys.path.append(os.path.abspath(\"D:/Hierarchical Unmixing Label\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPSO_HEIGHT=1092\n",
    "HYPSO_WIDTH=598\n",
    "HYPSO_BANDS=120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASS AND FUCNTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryDecisionTree:\n",
    "    def __init__(self, all_labels, verbose=False):\n",
    "        self.verbose = verbose\n",
    "        self.all_labels = all_labels\n",
    "        self.endmembers = []\n",
    "        self.splitting_nodes = []\n",
    "        self.models = {}  # Dictionary to store SVM models for each splitting node\n",
    "        self.initialize_tree_structure(all_labels.keys())\n",
    "        \n",
    "    def initialize_tree_structure(self, all_keys):\n",
    "        \"\"\"Initialize the tree structure by identifying endmembers and splitting nodes\"\"\"\n",
    "        print(\"Initializing Binary Decision Tree structure...\")\n",
    "        self.identify_set_endmembers(all_keys)\n",
    "        self.identify_set_splitting_nodes(all_keys)\n",
    "        \n",
    "    def identify_set_endmembers(self, all_keys):\n",
    "        max_length = max(len(key) for key in all_keys)\n",
    "        self.endmembers = []\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"IDENTIFYING ENDMEMBERS:\")\n",
    "            print(\"=\"*50)\n",
    "            print(f\"Maximum key length found: {max_length}\")\n",
    "            print(\"-\"*50)\n",
    "        \n",
    "        for key in all_keys:\n",
    "            if len(key) == max_length:\n",
    "                self.endmembers.append(key)\n",
    "                if self.verbose:\n",
    "                    print(f\"Found endmember: '{key}'\")\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Total endmembers identified: {len(self.endmembers)}\")\n",
    "        \n",
    "    def identify_set_splitting_nodes(self, all_keys):\n",
    "        self.splitting_nodes = []\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"IDENTIFYING SPLITTING NODES:\")\n",
    "            print(\"=\"*50)\n",
    "        \n",
    "        for key in all_keys:\n",
    "            has_zero = key + '0' in all_keys\n",
    "            has_one = key + '1' in all_keys\n",
    "            if has_zero and has_one:\n",
    "                self.splitting_nodes.append(key)\n",
    "                if self.verbose:\n",
    "                    print(f\"Found splitting node: '{key}' â†’ branches to '{key}0' and '{key}1'\")\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"-\"*50)\n",
    "            print(f\"Total splitting nodes identified: {len(self.splitting_nodes)}\")\n",
    "\n",
    "    def _preprocess_input(self, X):\n",
    "        \"\"\"\n",
    "        Preprocess input data to ensure it's in the right format (n_samples, n_bands)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy.ndarray\n",
    "            Hyperspectral image data. Can be:\n",
    "            - Flattened array of shape (n_samples, n_bands)\n",
    "            - Cube of shape (height, width, n_bands) or (n_bands, height, width)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        X_processed : numpy.ndarray\n",
    "            Processed data of shape (n_samples, n_bands)\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print(f\"\\nPreprocessing input data with shape: {X.shape}\")\n",
    "            \n",
    "        # If already 2D with samples as first dimension, return as is\n",
    "        if len(X.shape) == 2:\n",
    "            if self.verbose:\n",
    "                # print(f\"Input is already in correct format: {X.shape}\")\n",
    "                print(\"-\"*50)\n",
    "            return X\n",
    "        \n",
    "        # Handle 3D data (cube)\n",
    "        elif len(X.shape) == 3:\n",
    "            # Determine which dimension is the spectral dimension\n",
    "            # Typically, spectral dimension is the smallest\n",
    "            dims = np.array(X.shape)\n",
    "            spectral_dim = np.argmin(dims)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"Detected spectral dimension: {spectral_dim}\")\n",
    "            \n",
    "            if spectral_dim == 0:  # (n_bands, height, width)\n",
    "                n_bands, height, width = X.shape\n",
    "                if self.verbose:\n",
    "                    print(f\"Reshaping from (n_bands={n_bands}, height={height}, width={width}) to ({height*width}, {n_bands})\")\n",
    "                    print(\"-\"*50)\n",
    "                return X.reshape(n_bands, -1).T  # Reshape to (height*width, n_bands)\n",
    "            \n",
    "            elif spectral_dim == 2:  # (height, width, n_bands)\n",
    "                height, width, n_bands = X.shape\n",
    "                if self.verbose:\n",
    "                    print(f\"Reshaping from (height={height}, width={width}, n_bands={n_bands}) to ({height*width}, {n_bands})\")\n",
    "                    print(\"-\"*50)\n",
    "                return X.reshape(-1, n_bands)  # Reshape to (height*width, n_bands)\n",
    "            \n",
    "            else:  # (height, n_bands, width) - unusual but handle it\n",
    "                height, n_bands, width = X.shape\n",
    "                if self.verbose:\n",
    "                    print(f\"Unusual format detected: (height={height}, n_bands={n_bands}, width={width})\")\n",
    "                    print(f\"Reshaping to ({height*width}, {n_bands})\")\n",
    "                    print(\"-\"*50)\n",
    "                return X.transpose(0, 2, 1).reshape(-1, n_bands)  # Reshape to (height*width, n_bands)\n",
    "        \n",
    "        \n",
    "        else:\n",
    "            if self.verbose:\n",
    "                print(f\"Error: Unsupported input shape: {X.shape}\")\n",
    "            raise ValueError(f\"Unsupported input shape: {X.shape}. Expected 2D or 3D array.\")\n",
    "    \n",
    "    def train(self, X, labels=None):\n",
    "        \"\"\"\n",
    "        Train SVMs for each splitting node\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy.ndarray\n",
    "            Hyperspectral image data. Can be:\n",
    "            - Flattened array of shape (n_samples, n_bands)\n",
    "            - Cube of shape (height, width, n_bands) or (n_bands, height, width)\n",
    "        labels : dict, optional\n",
    "            Labels to use for training. If None, uses the labels provided during initialization.\n",
    "        \"\"\"\n",
    "        from sklearn.svm import LinearSVC\n",
    "        \n",
    "        # Process input data to ensure it's in the right format (n_samples, n_bands)\n",
    "        X = self._preprocess_input(X)\n",
    "        \n",
    "        # Use provided labels or fall back to the ones from initialization\n",
    "        training_labels = labels if labels is not None else self.all_labels\n",
    "        \n",
    "        # if self.verbose:\n",
    "        if True:\n",
    "            print(\"Training SVMs for each splitting node...\")\n",
    "        \n",
    "        for node in self.splitting_nodes:\n",
    "            print(\"Splitting node:\",node)\n",
    "            # Get labels for this node\n",
    "            y_parent = training_labels[node]\n",
    "            \n",
    "            # Only consider pixels that belong to this node\n",
    "            mask = y_parent == 1\n",
    "            X_node = X[mask]\n",
    "            \n",
    "            # Get labels for children nodes\n",
    "            left_child = node + '0'\n",
    "            right_child = node + '1'\n",
    "            \n",
    "            # Create binary labels for SVM (1 for right child, 0 for left child)\n",
    "            y_train = np.zeros(X_node.shape[0], dtype=int)\n",
    "            \n",
    "            # Find indices where right child is 1\n",
    "            if right_child in training_labels:\n",
    "                right_mask = training_labels[right_child][mask] == 1\n",
    "                y_train[right_mask] = 1\n",
    "            \n",
    "            # Train LinearSVC\n",
    "            model = LinearSVC(dual='auto', random_state=42)\n",
    "            model.fit(X_node, y_train)\n",
    "            \n",
    "            # Store the model\n",
    "            self.models[node] = model\n",
    "            \n",
    "            # if self.verbose:\n",
    "            if True:\n",
    "                print(f\"Trained model for node '{node}'\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict endmember classes for input data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy.ndarray\n",
    "            Hyperspectral image data. Can be:\n",
    "            - Flattened array of shape (n_samples, n_bands)\n",
    "            - Cube of shape (height, width, n_bands) or (n_bands, height, width)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        predictions : dict\n",
    "            Dictionary with keys for each endmember and values as binary masks\n",
    "        \"\"\"\n",
    "        # Process input data to ensure it's in the right format (n_samples, n_bands)\n",
    "        X = self._preprocess_input(X)\n",
    "        \n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # Initialize predictions with all True for root node\n",
    "        current_predictions = {\n",
    "            '': np.ones(n_samples, dtype=bool)\n",
    "        }\n",
    "        \n",
    "        # Process each level of the tree\n",
    "        for level in range(max(len(node) for node in self.splitting_nodes) + 1):\n",
    "            # Get nodes at this level\n",
    "            level_nodes = [node for node in self.splitting_nodes if len(node) == level]\n",
    "            \n",
    "            for node in level_nodes:\n",
    "                # Skip if node doesn't exist in current_predictions\n",
    "                if node not in current_predictions:\n",
    "                    continue\n",
    "                # Skip if no samples belong to this node\n",
    "                if not np.any(current_predictions[node]):\n",
    "                    continue\n",
    "                \n",
    "                # Get samples that belong to this node\n",
    "                node_mask = current_predictions[node]\n",
    "                X_node = X[node_mask]\n",
    "                \n",
    "                # Predict using the SVM model\n",
    "                if len(X_node) > 0:\n",
    "                    y_pred = self.models[node].predict(X_node)\n",
    "                    \n",
    "                    # Create masks for children\n",
    "                    left_child = node + '0'\n",
    "                    right_child = node + '1'\n",
    "                    \n",
    "                    # Initialize child predictions\n",
    "                    if left_child not in current_predictions:\n",
    "                        current_predictions[left_child] = np.zeros(n_samples, dtype=bool)\n",
    "                    if right_child not in current_predictions:\n",
    "                        current_predictions[right_child] = np.zeros(n_samples, dtype=bool)\n",
    "                    \n",
    "                    # Update predictions for children\n",
    "                    left_indices = np.where(node_mask)[0][y_pred == 0]\n",
    "                    right_indices = np.where(node_mask)[0][y_pred == 1]\n",
    "                    \n",
    "                    current_predictions[left_child][left_indices] = True\n",
    "                    current_predictions[right_child][right_indices] = True\n",
    "        \n",
    "        # For non-splitting nodes that just have a single child with '0' appended\n",
    "        for key in self.all_labels.keys():\n",
    "            if key not in self.splitting_nodes and key != '':\n",
    "                # Find the parent node\n",
    "                parent = key[:-1]\n",
    "                if parent in current_predictions and key not in current_predictions:\n",
    "                    # If this is a non-splitting child, it inherits parent's prediction\n",
    "                    current_predictions[key] = current_predictions[parent].copy()\n",
    "        \n",
    "        # Extract predictions for endmembers\n",
    "        endmember_predictions = {}\n",
    "        for endmember in self.endmembers:\n",
    "            if endmember in current_predictions:\n",
    "                endmember_predictions[endmember] = current_predictions[endmember]\n",
    "            else:\n",
    "                # If endmember not in predictions, try to find its parent\n",
    "                parent = endmember[:-1]\n",
    "                while parent and parent not in current_predictions:\n",
    "                    parent = parent[:-1]\n",
    "                if parent:\n",
    "                    endmember_predictions[endmember] = current_predictions[parent].copy()\n",
    "                else:\n",
    "                    endmember_predictions[endmember] = np.zeros(n_samples, dtype=bool)\n",
    "        \n",
    "        return endmember_predictions\n",
    "    \n",
    "    def evaluate(self, X, gt_labels=None):\n",
    "        \"\"\"\n",
    "        Evaluate the model on test data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy.ndarray\n",
    "            Hyperspectral image data. Can be:\n",
    "            - Flattened array of shape (n_samples, n_bands)\n",
    "            - Cube of shape (height, width, n_bands) or (n_bands, height, width)\n",
    "        gt_labels : dict, optional\n",
    "            Ground truth labels for evaluation. If None, uses self.all_labels\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        accuracy : float\n",
    "            Overall accuracy of endmember predictions\n",
    "        \"\"\"\n",
    "        if gt_labels is None:\n",
    "            gt_labels = self.all_labels\n",
    "            \n",
    "        # if self.verbose:\n",
    "        if True:\n",
    "            print(\"Starting evaluation...\")\n",
    "            \n",
    "        predictions = self.predict(X)\n",
    "        \n",
    "        # Calculate accuracy for endmembers\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Evaluating accuracy for {len(self.endmembers)} endmembers...\")\n",
    "            \n",
    "        endmember_accuracies = {}\n",
    "        for endmember in self.endmembers:\n",
    "            if endmember in predictions and endmember in gt_labels:\n",
    "                pred = predictions[endmember]\n",
    "                true = gt_labels[endmember]\n",
    "                \n",
    "                endmember_correct = np.sum(pred == true)\n",
    "                endmember_total = len(true)\n",
    "                \n",
    "                correct += endmember_correct\n",
    "                total += endmember_total\n",
    "                \n",
    "                if self.verbose:\n",
    "                    endmember_acc = endmember_correct / endmember_total if endmember_total > 0 else 0\n",
    "                    endmember_accuracies[endmember] = endmember_acc\n",
    "                    print(f\"  Endmember '{endmember}': {endmember_acc:.4f} ({endmember_correct}/{endmember_total})\")\n",
    "        \n",
    "        accuracy = correct / total if total > 0 else 0\n",
    "        \n",
    "        # if self.verbose:\n",
    "        if True:\n",
    "            print(f\"Overall accuracy: {accuracy:.4f} ({correct}/{total} pixels)\")\n",
    "            \n",
    "        return accuracy\n",
    "\n",
    "    def plot_input_image(self, image_data, slice_idx=None, figsize=(15, 5), cmap='viridis', rgb_bands=(69, 46, 26)):\n",
    "        \"\"\"Plot input image data as single band, RGB composite, or flat image.\"\"\"\n",
    "        plt.figure(figsize=figsize)\n",
    "        \n",
    "        # Preprocess input data for more efficient handling\n",
    "        image_data = np.asarray(image_data)  # Ensure numpy array\n",
    "        \n",
    "        # Handle flat images (h*w, n_bands)\n",
    "        if len(image_data.shape) == 2 and image_data.shape[1] > 3:\n",
    "            slice_idx = slice_idx if slice_idx is not None else image_data.shape[1] // 2\n",
    "            band_data = image_data[:, slice_idx]\n",
    "            \n",
    "            # Try to reshape to a square-ish image if possible\n",
    "            side = int(np.sqrt(image_data.shape[0]))\n",
    "            reshaped_data = band_data.reshape(side, side) if side * side == image_data.shape[0] else band_data.reshape(-1, 1)\n",
    "            plt.imshow(np.rot90(reshaped_data), cmap=cmap, aspect=0.1)\n",
    "            plt.colorbar(label='Intensity')\n",
    "            plt.title(f'Input Image - Band {slice_idx} (Flattened)')\n",
    "        \n",
    "        # Handle 3D data (h, w, n_bands)\n",
    "        elif len(image_data.shape) == 3:\n",
    "            h, w, n_bands = image_data.shape\n",
    "            slice_idx = slice_idx if slice_idx is not None else n_bands // 2\n",
    "            \n",
    "            # Create RGB composite if bands are specified\n",
    "            if rgb_bands is not None and len(rgb_bands) == 3:\n",
    "                r_idx, g_idx, b_idx = rgb_bands\n",
    "                if max(rgb_bands) > n_bands - 1:\n",
    "                    raise ValueError(f\"RGB band indices {rgb_bands} exceed available bands (0-{n_bands-1})\")\n",
    "                \n",
    "                # More efficient RGB creation - preallocate and process in one go\n",
    "                rgb_img = np.zeros((h, w, 3), dtype=np.float32)\n",
    "                for i, band_idx in enumerate([r_idx, g_idx, b_idx]):\n",
    "                    band = image_data[:, :, band_idx].astype(np.float32)\n",
    "                    # Normalize only if needed\n",
    "                    band_min, band_max = band.min(), band.max()\n",
    "                    if band_min != band_max:\n",
    "                        band = (band - band_min) / (band_max - band_min)\n",
    "                    rgb_img[:, :, i] = band\n",
    "                \n",
    "                plt.imshow(np.rot90(rgb_img), aspect=0.1)\n",
    "                plt.title(f'RGB Composite (R:{r_idx}, G:{g_idx}, B:{b_idx})')\n",
    "            else:\n",
    "                plt.imshow(np.rot90(image_data[:, :, slice_idx]), cmap=cmap, aspect=0.1)\n",
    "                plt.colorbar(label='Intensity')\n",
    "                plt.title(f'Input Image - Band {slice_idx}')\n",
    "        \n",
    "        # Handle other cases\n",
    "        else:\n",
    "            plt.imshow(np.rot90(image_data), cmap=cmap, aspect=0.1)\n",
    "            plt.colorbar(label='Intensity')\n",
    "            plt.title('Input Image')\n",
    "        \n",
    "        plt.axis('on')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_ground_truth(self, labels=None, key='', figsize=(15, 5), cmap='tab10'):\n",
    "        \"\"\"Plot ground truth labels.\"\"\"\n",
    "        # Preprocess input\n",
    "        labels = self.all_labels if labels is None else labels\n",
    "        shape = labels[''].shape\n",
    "        reshape = labels[''].reshape(-1, HYPSO_HEIGHT)\n",
    "        height, width = reshape.shape[0], reshape.shape[1]\n",
    "        \n",
    "        # Handle dictionary-type labels\n",
    "        if isinstance(labels, dict):\n",
    "            if key == '':\n",
    "                # Create a combined view of all endmembers\n",
    "                plt.figure(figsize=figsize)\n",
    "                first_val = list(labels.values())[0]\n",
    "                \n",
    "                if len(first_val.shape) == 1:\n",
    "                    # Use the dimensions we already determined\n",
    "                    combined_labels = np.zeros((height, width), dtype=int)\n",
    "                    endmember_keys = [k for k in labels.keys() if k in self.endmembers]\n",
    "                    \n",
    "                    # Process data before visualization\n",
    "                    for i, k in enumerate(endmember_keys):\n",
    "                        if k == '':\n",
    "                            continue\n",
    "                        try:\n",
    "                            label_reshaped = labels[k].reshape(height, width)\n",
    "                            combined_labels[label_reshaped == 1] = i + 1\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error reshaping key '{k}': {e}\")\n",
    "                            continue\n",
    "                    \n",
    "                    # Rotate the image 90 degrees and set aspect ratio\n",
    "                    plt.imshow(np.rot90(combined_labels), cmap=cmap, aspect=0.1)\n",
    "                    plt.title('Ground Truth Labels - Combined View')\n",
    "                else:\n",
    "                    print(\"Cannot display labels: unexpected format\")\n",
    "            else:\n",
    "                # Plot just the specified key\n",
    "                if key not in labels:\n",
    "                    print(f\"Key '{key}' not found in labels\")\n",
    "                    return\n",
    "                \n",
    "                label_data = labels[key]\n",
    "                if len(label_data.shape) == 1:\n",
    "                    try:\n",
    "                        # Use the dimensions we already determined\n",
    "                        label_data = label_data.reshape(height, width)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reshaping label data: {e}\")\n",
    "                        return\n",
    "                \n",
    "                plt.figure(figsize=figsize)\n",
    "                # Use viridis colormap for the binary data and rotate the image 90 degrees\n",
    "                plt.imshow(np.rot90(label_data), cmap='viridis', vmin=0, vmax=1, aspect=0.1)\n",
    "                plt.title(f'Ground Truth Label for \"{key}\"')\n",
    "        \n",
    "        # Handle array-type labels\n",
    "        elif hasattr(labels, 'shape'):\n",
    "            # Preprocess array data\n",
    "            labels = np.asarray(labels)  # Ensure numpy array\n",
    "            if len(labels.shape) == 1:\n",
    "                try:\n",
    "                    # Use the dimensions we already determined\n",
    "                    labels = labels.reshape(height, width)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reshaping label data: {e}\")\n",
    "                    return\n",
    "            \n",
    "            plt.figure(figsize=figsize)\n",
    "            # Rotate the image 90 degrees and set aspect ratio\n",
    "            plt.imshow(np.rot90(labels), cmap=cmap, aspect=0.1)\n",
    "            plt.title('Ground Truth Labels')\n",
    "        else:\n",
    "            print(\"Cannot plot labels: unsupported format\")\n",
    "            return\n",
    "        \n",
    "        plt.axis('on')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_prediction(self, prediction=None, key='', figsize=(15, 5), cmap='tab10'):\n",
    "        \"\"\"Plot prediction results.\"\"\"\n",
    "        # Preprocess input\n",
    "        if prediction is None:\n",
    "            if not hasattr(self, 'last_prediction'):\n",
    "                print(\"No prediction available. Run predict() first.\")\n",
    "                return\n",
    "            prediction = self.last_prediction\n",
    "        \n",
    "        # Determine dimensions from the data\n",
    "        first_key = next(iter(prediction))\n",
    "        shape = prediction[first_key].shape\n",
    "        reshape = prediction[first_key].reshape(-1, HYPSO_HEIGHT)\n",
    "        height, width = reshape.shape[0], reshape.shape[1]\n",
    "        \n",
    "        # Handle dictionary-type predictions\n",
    "        if isinstance(prediction, dict):\n",
    "            if key == '':\n",
    "                plt.figure(figsize=figsize)\n",
    "                first_val = list(prediction.values())[0]\n",
    "                \n",
    "                if len(first_val.shape) == 1:\n",
    "                    combined_pred = np.zeros((height, width), dtype=int)\n",
    "                    endmember_keys = [k for k in prediction.keys() if k in self.endmembers]\n",
    "                    \n",
    "                    # Process data before visualization\n",
    "                    for i, k in enumerate(endmember_keys):\n",
    "                        if k == '':\n",
    "                            continue\n",
    "                        try:\n",
    "                            pred_reshaped = prediction[k].reshape(height, width)\n",
    "                            combined_pred[pred_reshaped == True] = i + 1\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error reshaping key '{k}': {e}\")\n",
    "                            continue\n",
    "                    \n",
    "                    # Rotate the image 90 degrees and set aspect ratio\n",
    "                    plt.imshow(np.rot90(combined_pred), cmap=cmap, aspect=0.1)\n",
    "                    plt.title('Predictions - Combined View')\n",
    "                else:\n",
    "                    print(\"Cannot display predictions: unexpected format\")\n",
    "            else:\n",
    "                if key not in prediction:\n",
    "                    print(f\"Key '{key}' not found in predictions\")\n",
    "                    return\n",
    "                \n",
    "                pred_data = prediction[key]\n",
    "                if len(pred_data.shape) == 1:\n",
    "                    try:\n",
    "                        # Use the dimensions we already determined\n",
    "                        pred_data = pred_data.reshape(height, width)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reshaping prediction data: {e}\")\n",
    "                        return\n",
    "                \n",
    "                plt.figure(figsize=figsize)\n",
    "                # Use viridis colormap for the binary data, rotate 90 degrees and set aspect ratio\n",
    "                plt.imshow(np.rot90(pred_data), cmap='viridis', vmin=0, vmax=1, aspect=0.1)\n",
    "                plt.title(f'Prediction for key \"{key}\"')\n",
    "        \n",
    "        # Handle array-type predictions\n",
    "        elif hasattr(prediction, 'shape'):\n",
    "            # Preprocess array data\n",
    "            prediction = np.asarray(prediction)  # Ensure numpy array\n",
    "            if len(prediction.shape) == 1:\n",
    "                try:\n",
    "                    # Use the dimensions we already determined\n",
    "                    prediction = prediction.reshape(height, width)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reshaping prediction data: {e}\")\n",
    "                    return\n",
    "            \n",
    "            plt.figure(figsize=figsize)\n",
    "            # Rotate the image 90 degrees and set aspect ratio\n",
    "            plt.imshow(np.rot90(prediction), cmap=cmap, aspect=0.1)\n",
    "            plt.title('Prediction')\n",
    "        else:\n",
    "            print(\"Cannot plot prediction: unsupported format\")\n",
    "            return\n",
    "        \n",
    "        plt.axis('on')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def save_model_parameters(self, filename='decision_tree_model.pkl'):\n",
    "        \"\"\"\n",
    "        Save the trained SVM parameters of the decision tree to a file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        filename : str\n",
    "            The name of the file to save the model parameters to.\n",
    "        \"\"\"\n",
    "        import pickle\n",
    "        import os\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Preparing to save model parameters...\")\n",
    "            # for node, model in self.models.items():\n",
    "            #     if hasattr(model, 'coef_'):\n",
    "            #         print(f\"Node {node} SVM weights shape: {model.coef_.shape}\")\n",
    "            #         print(f\"Node {node} SVM weights: {model.coef_}\")\n",
    "            #     if hasattr(model, 'intercept_'):\n",
    "            #         print(f\"Node {node} SVM intercept: {model.intercept_}\")\n",
    "        \n",
    "        # Create a dictionary to store all model data\n",
    "        model_data = {\n",
    "            'models': self.models,  # This is your dictionary of SVM models\n",
    "            'endmembers': self.endmembers,\n",
    "            'splitting_nodes': self.splitting_nodes,\n",
    "            'all_labels': self.all_labels  # Save the labels too\n",
    "        }\n",
    "        \n",
    "        # Ensure weights folder exists\n",
    "        weights_folder = 'weights'\n",
    "        os.makedirs(weights_folder, exist_ok=True)\n",
    "        \n",
    "        # Create full path to save file in weights folder\n",
    "        filepath = os.path.join(weights_folder, filename)\n",
    "        \n",
    "        try:\n",
    "            if self.verbose:\n",
    "                print(f\"Saving model to {filepath}...\")\n",
    "            with open(filepath, 'wb') as f:\n",
    "                pickle.dump(model_data, f)\n",
    "            print(f\"Model parameters successfully saved to {filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving model parameters: {e}\")\n",
    "\n",
    "    def load_model_parameters(self, filename='decision_tree_model.pkl'):\n",
    "        \"\"\"\n",
    "        Load the trained SVM parameters for the decision tree from a file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        filename : str\n",
    "            The name of the file to load the model parameters from.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        bool\n",
    "            True if loading was successful, False otherwise.\n",
    "        \"\"\"\n",
    "        import pickle\n",
    "        import os\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Attempting to load model parameters from {filename}...\")\n",
    "        \n",
    "        # Create full path to load file from weights folder\n",
    "        filepath = os.path.join('weights', filename)\n",
    "        \n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"Error: Model file {filepath} not found\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            if self.verbose:\n",
    "                print(\"Reading model file...\")\n",
    "            with open(filepath, 'rb') as f:\n",
    "                model_data = pickle.load(f)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(\"Restoring model components...\")\n",
    "            \n",
    "            # Restore the model components\n",
    "            self.models = model_data['models']\n",
    "            self.endmembers = model_data['endmembers']\n",
    "            self.splitting_nodes = model_data['splitting_nodes']\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(\"Model components restored. SVM model details:\")\n",
    "                for node, model in self.models.items():\n",
    "                    if hasattr(model, 'coef_'):\n",
    "                        print(f\"Node {node} SVM weights shape: {model.coef_.shape}\")\n",
    "                        print(f\"Node {node} SVM weights: {model.coef_}\")\n",
    "                    if hasattr(model, 'intercept_'):\n",
    "                        print(f\"Node {node} SVM intercept: {model.intercept_}\")\n",
    "            \n",
    "            # Optionally restore labels if they were saved\n",
    "            if 'all_labels' in model_data:\n",
    "                self.all_labels = model_data['all_labels']\n",
    "                if self.verbose:\n",
    "                    print(\"Labels restored from saved model\")\n",
    "            \n",
    "            print(f\"Model parameters successfully loaded from {filepath}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model parameters: {e}\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pipeline(image_path,tree_path, deh_path, hypso=1, verbose=False):\n",
    "    # Check if the image path is a .npy or .nc file\n",
    "    if image_path.endswith('.npy'):\n",
    "        # Load numpy array directly\n",
    "        \n",
    "        image_data = np.load(image_path)\n",
    "        if image_data.ndim == 3:\n",
    "            image_data_flat = image_data.reshape(-1,112)\n",
    "    elif image_path.endswith('.nc'):\n",
    "        # Use hypso to load .nc file\n",
    "        try:\n",
    "            image_data= nc_to_image(image_path, hypso, verbose, machi=True)\n",
    "        except ImportError:\n",
    "            print(\"Error: hypso package is required to process .nc files\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing .nc file: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Unsupported file format: {image_path}\")\n",
    "        print(\"Supported formats: .npy, .nc\")\n",
    "        return None\n",
    "    \n",
    "    # Print information about the image data values\n",
    "    print(f\"Image data shape: {image_data.shape}\")\n",
    "    print(f\"Image data type: {image_data.dtype}\")\n",
    "    print(f\"Min value: {np.min(image_data)}\")\n",
    "    print(f\"Max value: {np.max(image_data)}\")\n",
    "    print(f\"Mean value: {np.mean(image_data)}\")\n",
    "    \n",
    "    #create label form deh\n",
    "    image_gt = generate_huH_labels(image_data, deh_path, verbose=verbose)\n",
    "\n",
    "    # Load the decision tree model\n",
    "    tree = BinaryDecisionTree(image_gt,verbose=verbose)\n",
    "    if not tree.load_model_parameters(tree_path):\n",
    "        print(\"Failed to load tree model\")\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Make predictions using the tree\n",
    "    predictions = tree.predict(image_data)\n",
    "    evaluation = tree.evaluate(image_data,image_gt)\n",
    "    print(evaluation)\n",
    "    \n",
    "    tree.plot_input_image(image_data.reshape(-1,1092,112))\n",
    "    tree.plot_ground_truth(image_gt)\n",
    "    tree.plot_prediction(predictions)\n",
    "    # Return the predictions\n",
    "    return predictions\n",
    "\n",
    "def nc_to_image(image_path, hypso=1, verbose=False, machi=False):\n",
    "    if hypso==2:\n",
    "        satobj = Hypso2(path=image_path, verbose=verbose)\n",
    "    else:\n",
    "        satobj = Hypso1(path=image_path, verbose=verbose)\n",
    "    # Reshape if needed (depends on hypso output format)\n",
    "    satobj.generate_l1b_cube()\n",
    "    data = satobj.l1b_cube\n",
    "    #satobj.generate_l1c_cube()\n",
    "    #satobj.generate_l1d_cube()\n",
    "    image_data = np.array(data)[:,:,6:118]\n",
    "    image_data_flat=image_data.reshape(-1,112)\n",
    "\n",
    "    return image_data_flat\n",
    "\n",
    "def generate_huH_labels(image_file, deh_path, verbose=False):\n",
    "    DEH_model=deh.DEH(no_negative_residuals=True)\n",
    "    DEH_model.load(deh_path)\n",
    "    image_file_cube=image_file.reshape(-1,1092,112)\n",
    "    DEH_model.plot_size=(image_file_cube[0], image_file_cube[1])\n",
    "    DEH_model.simple_predict(image_file)\n",
    "    DEH_model.binarize_lmdas()\n",
    "    DEH_model.lmda_2_map()\n",
    "    labels_dict={}\n",
    "    for node in DEH_model.nodes:\n",
    "        labels = DEH_model.nodes[node].map.flatten()\n",
    "        labels_dict[node] = labels\n",
    "    return labels_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD L1D MACHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_10_L1A_120 = np.load('D:\\Hierarchical Unmixing Label\\hUH\\images\\combined_10_L1A_120.npy')\n",
    "combined_10_L1D_112_MACHI = np.load('D:\\Hierarchical Unmixing Label\\hUH\\images\\combined_10_L1D_112_MACHI.npy')\n",
    "combined_10_L1D_112_MACHI_labels= np.load('save\\L1D_112_MACHI_10img_8end_stabelized_aa_labels.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree=BinaryDecisionTree(combined_10_L1D_112_MACHI_labels,verbose=False)\n",
    "tree.load_model_parameters(r\"combined_10_L1A_120_8end_TREE.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_corrected=BinaryDecisionTree(combined_10_L1D_112_MACHI_labels,verbose=False)\n",
    "tree_corrected.load_model_parameters(r\"combined_10_L1D_112_MACHI_8end_TREE.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tampa_2024_11_12_L1A_120=np.load(r'images\\tampa_2024-11-12T15-31-55Z-l1a_cm_L1A_120.npy')\n",
    "tampa_2024_11_12_L1A_120_labels=np.load(r'save\\tampa_2024_11_12_L1D_112_labels.npy',allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD L1B MACHI DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_10_v2_MACHI = np.load('D:\\Hierarchical Unmixing Label\\hUH\\images\\combined_10_v2_MACHI.npy')\n",
    "combined_MACHI_labels = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\save\\combined_MACHI_binary_labels.npy', allow_pickle=True).item()\n",
    "\n",
    "machi_tampa_2024_11_12          = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\tampa_2024-11-12T15-31-55Z-l1a_cm_machi.npy')\n",
    "machi_tampa_2024_11_12_labels   = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\save\\machi_tampa_2024_11_12_binary_labels.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree=BinaryDecisionTree(combined_MACHI_labels,verbose=False)\n",
    "tree.load_model_parameters(r\"D:\\Hierarchical Unmixing Label\\hUH\\weights\\tree_MACHI.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD L1B DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1A_data = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\aregantsea2_2025-03-11T08-12-43Z-l1a_cm_L1A.npy')\n",
    "L1A_labels = np.load(r'save\\L1A_1img_8end_stabelized_ppa_FINAL_labels.npy', allow_pickle=True).item()\n",
    "L1B_data = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\aregantsea2_2025-03-11T08-12-43Z-l1a_cm_L1B.npy')\n",
    "L1B_labels = np.load(r'save\\L1B_1img_8end_stabelized_ppa_labels.npy', allow_pickle=True).item()\n",
    "L1A_data_120=np.load(r\"D:\\Hierarchical Unmixing Label\\hUH\\images\\aregantsea2_2025-03-11T08-12-43Z-l1a_cm_L1A_120.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING AND EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data=combined_10_L1A_120\n",
    "image_data_corrected=combined_10_L1D_112_MACHI\n",
    "image_gt=combined_10_L1D_112_MACHI_labels\n",
    "save_path=r\"D:\\Hierarchical Unmixing Label\\hUH\\weights\\combined_10_L1A_120_8end_TREE.pkl\"\n",
    "save_path_corrected=r\"D:\\Hierarchical Unmixing Label\\hUH\\weights\\combined_10_L1D_112_MACHI_8end_TREE.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree=BinaryDecisionTree(image_gt,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_corrected=BinaryDecisionTree(image_gt,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.train(image_data,image_gt)\n",
    "evaluation = tree.evaluate(image_data,image_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_corrected.train(image_data_corrected,image_gt)\n",
    "evaluation_corrected = tree_corrected.evaluate(image_data_corrected,image_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.save_model_parameters(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_corrected.save_model_parameters(save_path_corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path=r\"D:\\Hierarchical Unmixing Label\\hUH\\weights\\combined_10_L1A_120_8end_TREE.pkl\"\n",
    "save_path_corrected=r\"D:\\Hierarchical Unmixing Label\\hUH\\weights\\combined_10_L1D_112_MACHI_8end_TREE.pkl\"\n",
    "tree=BinaryDecisionTree(image_gt,verbose=True)\n",
    "tree.load_model_parameters(save_path)\n",
    "tree_corrected=BinaryDecisionTree(image_gt,verbose=True)\n",
    "tree_corrected.load_model_parameters(save_path_corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tree.predict(image_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_corrected = tree_corrected.predict(image_data_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_tampa_2024_11_12=tree.predict(tampa_2024_11_12_L1A_120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.plot_input_image(tampa_2024_11_12_L1A_120.reshape(-1,HYPSO_HEIGHT,120))\n",
    "tree.plot_ground_truth(tampa_2024_11_12_L1A_120_labels, cmap='viridis')\n",
    "tree.plot_prediction(predictions_tampa_2024_11_12, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.plot_input_image(image_data.reshape(-1,HYPSO_HEIGHT,120))\n",
    "tree.plot_ground_truth(image_gt, cmap='viridis')\n",
    "tree.plot_prediction(prediction, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_corrected.plot_input_image(image_data_corrected.reshape(-1,HYPSO_HEIGHT,112))\n",
    "tree_corrected.plot_ground_truth(image_gt, cmap='viridis')\n",
    "tree_corrected.plot_prediction(prediction_corrected, cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST BINARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tree_svms_to_binary_correct(tree, output_dir):\n",
    "    \"\"\"\n",
    "    Save all SVM models in the decision tree as binary files.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    tree : BinaryDecisionTree\n",
    "        The decision tree containing trained SVM models\n",
    "    output_dir : str\n",
    "        Directory where the binary files will be saved\n",
    "    \n",
    "    Output format for each file:\n",
    "    ---------------------------\n",
    "    - class1 (uint8)\n",
    "    - class2 (uint8)\n",
    "    - intercept (float32)\n",
    "    - weights (array of float32)\n",
    "    \n",
    "    Filename format: lsm{left_min_class}{right_min_class}\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import struct\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get all splitting nodes with trained models\n",
    "    splitting_nodes = tree.splitting_nodes\n",
    "    \n",
    "    print(f\"Found {len(splitting_nodes)} splitting nodes in the tree\")\n",
    "    print(f\"Found {len(tree.endmembers)} endmembers in the tree\")\n",
    "    print(f\"Saving SVM models to {output_dir}\")\n",
    "    \n",
    "    # Map each binary path to the corresponding endmember decimal value\n",
    "    path_to_value = {}\n",
    "    for i, endmember_label in enumerate(tree.endmembers):\n",
    "        if endmember_label:  # Skip empty endmembers if any\n",
    "            # Convert binary string to decimal\n",
    "            em_value = int(endmember_label, 2)\n",
    "            path_to_value[endmember_label] = em_value\n",
    "    \n",
    "    # Count of successfully saved models\n",
    "    saved_count = 0\n",
    "    \n",
    "    for node in splitting_nodes:\n",
    "        # Check if this node has an SVM model\n",
    "        if node not in tree.models:\n",
    "            print(f\"Warning: Node '{node}' is marked as a splitting node but has no SVM model\")\n",
    "            continue\n",
    "        \n",
    "        # Get the SVM model for this node\n",
    "        svm_model = tree.models[node]\n",
    "        \n",
    "        # Find all endmembers that would go to the left branch (node+'0')\n",
    "        left_values = []\n",
    "        # Find all endmembers that would go to the right branch (node+'1')\n",
    "        right_values = []\n",
    "        \n",
    "        for path, value in path_to_value.items():\n",
    "            # Only process endmembers whose paths are long enough to be affected by this node\n",
    "            if len(path) > len(node):\n",
    "                # Check if this path goes through the current node\n",
    "                if path.startswith(node):\n",
    "                    # This is a descendent of the current node, check which branch\n",
    "                    next_bit = path[len(node)]\n",
    "                    if next_bit == '0':\n",
    "                        left_values.append(value)\n",
    "                    else:\n",
    "                        right_values.append(value)\n",
    "        \n",
    "        # Find lowest class in each branch (by decimal value)\n",
    "        left_min = min(left_values) if left_values else 0\n",
    "        right_min = min(right_values) if right_values else 0\n",
    "        \n",
    "        # Format the filename\n",
    "        svm_model_name = f\"lsm{left_min:02d}{right_min:02d}\"\n",
    "        filepath = os.path.join(output_dir, svm_model_name)\n",
    "        \n",
    "        print(f\"Node {node}: Splitting between left branch (min={left_min}) and right branch (min={right_min})\")\n",
    "        print(f\"  Left contains values: {sorted(left_values)}\")\n",
    "        print(f\"  Right contains values: {sorted(right_values)}\")\n",
    "        \n",
    "        # Open the file for binary writing\n",
    "        with open(filepath, \"wb\") as file:\n",
    "            # Write class labels as uint8\n",
    "            file.write(struct.pack(\"BB\", left_min, right_min))\n",
    "            \n",
    "            # Write intercept as float32\n",
    "            intercept = svm_model.intercept_[0]\n",
    "            file.write(struct.pack('f', intercept))\n",
    "            \n",
    "            # Write weights as float32\n",
    "            weights = svm_model.coef_[0]\n",
    "            file.write(struct.pack(f'{len(weights)}f', *weights))\n",
    "        \n",
    "        print(f\"Saved model for node '{node}' to {svm_model_name}\")\n",
    "        saved_count += 1\n",
    "    \n",
    "    print(f\"Successfully saved {saved_count} SVM models to {output_dir}\")\n",
    "    return True\n",
    "\n",
    "def print_tree_structure(tree):\n",
    "    \"\"\"\n",
    "    Print the structure of the decision tree to help understand the node hierarchy.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    tree : BinaryDecisionTree\n",
    "        The decision tree to analyze\n",
    "    \"\"\"\n",
    "    # Get splitting nodes\n",
    "    splitting_nodes = sorted(list(tree.splitting_nodes))\n",
    "    \n",
    "    # Infer all nodes by adding children of splitting nodes\n",
    "    all_nodes = set(splitting_nodes)\n",
    "    for node in splitting_nodes:\n",
    "        all_nodes.add(node + '0')  # Left child\n",
    "        all_nodes.add(node + '1')  # Right child\n",
    "    all_nodes = sorted(list(all_nodes))\n",
    "    \n",
    "    # Identify leaf nodes (nodes that are not splitting nodes)\n",
    "    leaf_nodes = [node for node in all_nodes if node not in splitting_nodes]\n",
    "    \n",
    "    print(\"Tree Structure:\")\n",
    "    print(f\"Total nodes: {len(all_nodes)}\")\n",
    "    print(f\"Splitting nodes: {len(splitting_nodes)}\")\n",
    "    print(f\"Leaf nodes: {len(leaf_nodes)}\")\n",
    "    \n",
    "    # Determine tree depth\n",
    "    max_depth = max([len(node) for node in all_nodes]) if all_nodes else 0\n",
    "    print(f\"Maximum tree depth: {max_depth}\")\n",
    "    \n",
    "    # Print node hierarchy\n",
    "    print(\"\\nNode Hierarchy:\")\n",
    "    for depth in range(max_depth + 1):\n",
    "        nodes_at_depth = [node for node in all_nodes if len(node) == depth]\n",
    "        split_status = ['(split)' if node in splitting_nodes else '(leaf)' for node in nodes_at_depth]\n",
    "        print(f\"Depth {depth}: {list(zip(nodes_at_depth, split_status))}\")\n",
    "    \n",
    "    # Print SVM models\n",
    "    print(\"\\nSVM Models:\")\n",
    "    for node in splitting_nodes:\n",
    "        if node in tree.models:\n",
    "            model = tree.models[node]\n",
    "            left_child = node + '0'\n",
    "            right_child = node + '1'\n",
    "            print(f\"Node '{node}' splits between '{left_child}' and '{right_child}'\")\n",
    "            print(f\"  - Weights shape: {model.coef_.shape}\")\n",
    "            print(f\"  - Intercept: {model.intercept_}\")\n",
    "        else:\n",
    "            print(f\"Node '{node}' is marked as splitting but has no model\")\n",
    "    \n",
    "    # Print endmembers if available\n",
    "    if hasattr(tree, 'endmembers') and tree.endmembers:\n",
    "        print(\"\\nEndmembers:\")\n",
    "        for i, endmember in enumerate(tree.endmembers):\n",
    "            print(f\"  {i}: {endmember}\")\n",
    "    \n",
    "    return max_depth\n",
    "\n",
    "def debug_svm_models(tree):\n",
    "    \"\"\"\n",
    "    Debug the SVM models in the tree to identify issues with weights.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    tree : BinaryDecisionTree\n",
    "        The decision tree containing trained SVM models\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with model analysis results\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    results = {}\n",
    "    splitting_nodes = sorted(list(tree.splitting_nodes))\n",
    "    \n",
    "    print(f\"Analyzing {len(splitting_nodes)} SVM models...\")\n",
    "    \n",
    "    for node in splitting_nodes:\n",
    "        if node not in tree.models:\n",
    "            print(f\"Warning: Node '{node}' is marked as a splitting node but has no SVM model\")\n",
    "            continue\n",
    "        \n",
    "        # Get the SVM model for this node\n",
    "        svm_model = tree.models[node]\n",
    "        \n",
    "        model_info = {\n",
    "            'model_type': str(type(svm_model)),\n",
    "            'has_coef': hasattr(svm_model, 'coef_'),\n",
    "            'has_intercept': hasattr(svm_model, 'intercept_'),\n",
    "            'attrs': dir(svm_model)\n",
    "        }\n",
    "        \n",
    "        if hasattr(svm_model, 'coef_'):\n",
    "            weights = svm_model.coef_[0]\n",
    "            model_info['weights_shape'] = weights.shape\n",
    "            model_info['weights_min'] = float(np.min(weights))\n",
    "            model_info['weights_max'] = float(np.max(weights))\n",
    "            model_info['weights_mean'] = float(np.mean(weights))\n",
    "            model_info['weights_nonzero'] = int(np.sum(np.abs(weights) > 1e-10))\n",
    "            model_info['weights_sample'] = [float(w) for w in weights[:5]]  # First 5 weights\n",
    "        \n",
    "        if hasattr(svm_model, 'intercept_'):\n",
    "            model_info['intercept'] = float(svm_model.intercept_[0])\n",
    "        \n",
    "        results[node] = model_info\n",
    "        \n",
    "        # Print key info about the model\n",
    "        print(f\"\\nNode '{node}':\")\n",
    "        if hasattr(svm_model, 'coef_'):\n",
    "            print(f\"  Weights: shape={weights.shape}, non-zero={model_info['weights_nonzero']}/{len(weights)}\")\n",
    "            print(f\"  Weight stats: min={model_info['weights_min']:.6f}, max={model_info['weights_max']:.6f}\")\n",
    "            if model_info['weights_nonzero'] == 0:\n",
    "                print(\"  WARNING: All weights are zero!\")\n",
    "        else:\n",
    "            print(\"  WARNING: Model does not have coef_ attribute!\")\n",
    "            \n",
    "        if hasattr(svm_model, 'intercept_'):\n",
    "            print(f\"  Intercept: {model_info['intercept']:.6f}\")\n",
    "        else:\n",
    "            print(\"  WARNING: Model does not have intercept_ attribute!\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def save_tree_svms_to_binary_fixed(tree, output_dir):\n",
    "    \"\"\"\n",
    "    Save all SVM models in the decision tree as binary files, fixing the weight issue.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    tree : BinaryDecisionTree\n",
    "        The decision tree containing trained SVM models\n",
    "    output_dir : str\n",
    "        Directory where the binary files will be saved\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import struct\n",
    "    import numpy as np\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get all splitting nodes with trained models\n",
    "    splitting_nodes = tree.splitting_nodes\n",
    "    print(f\"Found {len(splitting_nodes)} splitting nodes in the tree\")\n",
    "    \n",
    "    # Map each binary path to the corresponding endmember decimal value\n",
    "    path_to_value = {}\n",
    "    for i, endmember_label in enumerate(tree.endmembers):\n",
    "        if endmember_label:  # Skip empty endmembers if any\n",
    "            # Convert binary string to decimal\n",
    "            em_value = int(endmember_label, 2)\n",
    "            path_to_value[endmember_label] = em_value\n",
    "    \n",
    "    # Count of successfully saved models\n",
    "    saved_count = 0\n",
    "    \n",
    "    for node in splitting_nodes:\n",
    "        # Check if this node has an SVM model\n",
    "        if node not in tree.models:\n",
    "            print(f\"Warning: Node '{node}' is marked as a splitting node but has no SVM model\")\n",
    "            continue\n",
    "        \n",
    "        # Get the SVM model for this node\n",
    "        svm_model = tree.models[node]\n",
    "        \n",
    "        # Find all endmembers that would go to the left branch (node+'0')\n",
    "        left_values = []\n",
    "        # Find all endmembers that would go to the right branch (node+'1')\n",
    "        right_values = []\n",
    "        \n",
    "        for path, value in path_to_value.items():\n",
    "            # Only process endmembers whose paths are long enough to be affected by this node\n",
    "            if len(path) > len(node):\n",
    "                # Check if this path goes through the current node\n",
    "                if path.startswith(node):\n",
    "                    # This is a descendent of the current node, check which branch\n",
    "                    next_bit = path[len(node)]\n",
    "                    if next_bit == '0':\n",
    "                        left_values.append(value)\n",
    "                    else:\n",
    "                        right_values.append(value)\n",
    "        \n",
    "        # Find lowest class in each branch (by decimal value)\n",
    "        left_min = min(left_values) if left_values else 0\n",
    "        right_min = min(right_values) if right_values else 0\n",
    "        \n",
    "        # Format the filename\n",
    "        svm_model_name = f\"lsm{left_min:02d}{right_min:02d}\"\n",
    "        filepath = os.path.join(output_dir, svm_model_name)\n",
    "        \n",
    "        print(f\"Node {node}: Splitting between left branch (min={left_min}) and right branch (min={right_min})\")\n",
    "        \n",
    "        # Debug: Check if weights are non-zero before saving\n",
    "        if hasattr(svm_model, 'coef_'):\n",
    "            weights = svm_model.coef_[0]\n",
    "            non_zero_count = np.sum(np.abs(weights) > 1e-10)\n",
    "            if non_zero_count == 0:\n",
    "                print(f\"  WARNING: All weights are zero for node {node}!\")\n",
    "                \n",
    "                # Try to access the SVM model's raw data directly\n",
    "                if hasattr(svm_model, '_impl') and hasattr(svm_model._impl, 'raw_coef_'):\n",
    "                    print(\"  Attempting to use _impl.raw_coef_ instead...\")\n",
    "                    weights = svm_model._impl.raw_coef_\n",
    "                else:\n",
    "                    print(\"  Could not find alternative weight source\")\n",
    "        \n",
    "        # Open the file for binary writing\n",
    "        with open(filepath, \"wb\") as file:\n",
    "            # Write class labels as uint8\n",
    "            file.write(struct.pack(\"BB\", left_min, right_min))\n",
    "            \n",
    "            # Write intercept as float32\n",
    "            intercept = svm_model.intercept_[0]\n",
    "            file.write(struct.pack('f', intercept))\n",
    "            \n",
    "            # Write weights as float32\n",
    "            if hasattr(svm_model, 'coef_'):\n",
    "                weights = svm_model.coef_[0]\n",
    "                file.write(struct.pack(f'{len(weights)}f', *weights))\n",
    "                print(f\"  Saved {len(weights)} weights (non-zero: {non_zero_count})\")\n",
    "            else:\n",
    "                print(\"  WARNING: Model does not have coef_ attribute!\")\n",
    "        \n",
    "        print(f\"  Saved model to {svm_model_name}\")\n",
    "        saved_count += 1\n",
    "    \n",
    "    print(f\"Successfully saved {saved_count} SVM models to {output_dir}\")\n",
    "    return True\n",
    "\n",
    "def analyze_svm_file(file_path):\n",
    "    \"\"\"\n",
    "    Analyze the structure of an SVM binary file without assuming its format.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to the binary file\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with file analysis results\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import struct\n",
    "    \n",
    "    result = {\n",
    "        'file_name': os.path.basename(file_path),\n",
    "        'file_size': 0,\n",
    "        'header': None,\n",
    "        'structure': None,\n",
    "        'error': None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Get file size\n",
    "        result['file_size'] = os.path.getsize(file_path)\n",
    "        \n",
    "        # Read the file\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = f.read()\n",
    "        \n",
    "        # Analyze the first few bytes\n",
    "        if len(data) >= 2:\n",
    "            result['header'] = struct.unpack('BB', data[:2])\n",
    "        \n",
    "        # Calculate how many floats could fit after the header\n",
    "        if len(data) > 2:\n",
    "            remaining_bytes = len(data) - 2\n",
    "            possible_float_count = remaining_bytes // 4\n",
    "            result['structure'] = {\n",
    "                'header_size': 2,\n",
    "                'remaining_bytes': remaining_bytes,\n",
    "                'possible_float_count': possible_float_count\n",
    "            }\n",
    "            \n",
    "            # Try to read the intercept if there's at least one float\n",
    "            if possible_float_count >= 1:\n",
    "                result['intercept'] = struct.unpack('f', data[2:6])[0]\n",
    "                \n",
    "            # Attempt to read all the weights\n",
    "            if possible_float_count > 1:\n",
    "                weights_fmt = f'{possible_float_count-1}f'\n",
    "                result['weights'] = struct.unpack(weights_fmt, data[6:])\n",
    "                result['weight_count'] = possible_float_count - 1\n",
    "    \n",
    "    except Exception as e:\n",
    "        result['error'] = str(e)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def compare_svm_files_flexible(our_dir, original_dir):\n",
    "    \"\"\"\n",
    "    Compare SVM binary files between two directories without assuming their format.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    our_dir : str\n",
    "        Directory containing our generated SVM files\n",
    "    original_dir : str\n",
    "        Directory containing the original SVM files\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with comparison results\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import numpy as np\n",
    "    \n",
    "    # Get list of files in both directories\n",
    "    our_files = set(os.listdir(our_dir))\n",
    "    original_files = set(os.listdir(original_dir))\n",
    "    \n",
    "    # Find common files\n",
    "    common_files = our_files.intersection(original_files)\n",
    "    our_unique = our_files - original_files\n",
    "    original_unique = original_files - original_files\n",
    "    \n",
    "    print(f\"Found {len(common_files)} common files\")\n",
    "    print(f\"{len(our_unique)} files only in our directory\")\n",
    "    print(f\"{len(original_unique)} files only in original directory\")\n",
    "    \n",
    "    results = {\n",
    "        'file_analysis': {},\n",
    "        'size_comparison': {},\n",
    "        'structure_comparison': {}\n",
    "    }\n",
    "    \n",
    "    # Compare each common file\n",
    "    for filename in sorted(common_files):\n",
    "        our_path = os.path.join(our_dir, filename)\n",
    "        original_path = os.path.join(original_dir, filename)\n",
    "        \n",
    "        # Analyze both files\n",
    "        our_analysis = analyze_svm_file(our_path)\n",
    "        original_analysis = analyze_svm_file(original_path)\n",
    "        \n",
    "        # Store analysis results\n",
    "        results['file_analysis'][filename] = {\n",
    "            'our': our_analysis,\n",
    "            'original': original_analysis\n",
    "        }\n",
    "        \n",
    "        # Compare file sizes\n",
    "        results['size_comparison'][filename] = {\n",
    "            'our_size': our_analysis['file_size'],\n",
    "            'original_size': original_analysis['file_size'],\n",
    "            'size_match': our_analysis['file_size'] == original_analysis['file_size']\n",
    "        }\n",
    "        \n",
    "        # Compare structure if both files were successfully analyzed\n",
    "        if not our_analysis.get('error') and not original_analysis.get('error'):\n",
    "            results['structure_comparison'][filename] = {\n",
    "                'header_match': our_analysis['header'] == original_analysis['header'],\n",
    "                'our_weight_count': our_analysis.get('weight_count'),\n",
    "                'original_weight_count': original_analysis.get('weight_count')\n",
    "            }\n",
    "            \n",
    "            # If intercepts exist, compare them\n",
    "            if 'intercept' in our_analysis and 'intercept' in original_analysis:\n",
    "                intercept_diff = abs(our_analysis['intercept'] - original_analysis['intercept'])\n",
    "                results['structure_comparison'][filename]['intercept_diff'] = intercept_diff\n",
    "                results['structure_comparison'][filename]['intercept_match'] = intercept_diff < 1e-5\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nFile Size Comparison:\")\n",
    "    for filename, comparison in results['size_comparison'].items():\n",
    "        print(f\"{filename}: {'âœ“' if comparison['size_match'] else 'âœ—'} \" +\n",
    "              f\"(Our: {comparison['our_size']} bytes, Original: {comparison['original_size']} bytes)\")\n",
    "    \n",
    "    # Print structure details\n",
    "    print(\"\\nFile Structure Details:\")\n",
    "    for filename, analysis in results['file_analysis'].items():\n",
    "        our = analysis['our']\n",
    "        orig = analysis['original']\n",
    "        \n",
    "        print(f\"\\n{filename}:\")\n",
    "        print(f\"  Our file: {our['file_size']} bytes\")\n",
    "        if our['header']:\n",
    "            print(f\"    Header: {our['header']}\")\n",
    "        if 'weight_count' in our:\n",
    "            print(f\"    Weight count: {our['weight_count']}\")\n",
    "            \n",
    "        print(f\"  Original file: {orig['file_size']} bytes\")\n",
    "        if orig['header']:\n",
    "            print(f\"    Header: {orig['header']}\")\n",
    "        if 'weight_count' in orig:\n",
    "            print(f\"    Weight count: {orig['weight_count']}\")\n",
    "        \n",
    "        if our.get('error') or orig.get('error'):\n",
    "            print(f\"  Errors: Our: {our.get('error')}, Original: {orig.get('error')}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def compare_lsm_file_values(our_file, original_file):\n",
    "    \"\"\"\n",
    "    Compare the actual values inside two LSM binary files.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    our_file : str\n",
    "        Path to our generated LSM file\n",
    "    original_file : str\n",
    "        Path to the original LSM file\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with detailed comparison results\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import struct\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from collections import OrderedDict\n",
    "    \n",
    "    result = OrderedDict()\n",
    "    \n",
    "    # Basic file info\n",
    "    result['our_file'] = os.path.basename(our_file)\n",
    "    result['original_file'] = os.path.basename(original_file)\n",
    "    result['our_size'] = os.path.getsize(our_file)\n",
    "    result['original_size'] = os.path.getsize(original_file)\n",
    "    \n",
    "    # Read both files\n",
    "    with open(our_file, 'rb') as f:\n",
    "        our_data = f.read()\n",
    "    \n",
    "    with open(original_file, 'rb') as f:\n",
    "        orig_data = f.read()\n",
    "    \n",
    "    print(f\"Our file size: {len(our_data)} bytes\")\n",
    "    print(f\"Original file size: {len(orig_data)} bytes\")\n",
    "    \n",
    "    # Extract and compare header values (class labels)\n",
    "    try:\n",
    "        our_class1, our_class2 = struct.unpack('BB', our_data[:2])\n",
    "        result['our_classes'] = (our_class1, our_class2)\n",
    "    except Exception as e:\n",
    "        result['our_classes_error'] = str(e)\n",
    "    \n",
    "    try:\n",
    "        orig_class1, orig_class2 = struct.unpack('BB', orig_data[:2])\n",
    "        result['orig_classes'] = (orig_class1, orig_class2)\n",
    "    except Exception as e:\n",
    "        result['orig_classes_error'] = str(e)\n",
    "    \n",
    "    # Check if classes match\n",
    "    if 'our_classes' in result and 'orig_classes' in result:\n",
    "        result['classes_match'] = result['our_classes'] == result['orig_classes']\n",
    "    \n",
    "    # Extract and compare intercept\n",
    "    try:\n",
    "        our_intercept = struct.unpack('f', our_data[2:6])[0]\n",
    "        result['our_intercept'] = our_intercept\n",
    "    except Exception as e:\n",
    "        result['our_intercept_error'] = str(e)\n",
    "    \n",
    "    try:\n",
    "        orig_intercept = struct.unpack('f', orig_data[2:6])[0]\n",
    "        result['orig_intercept'] = orig_intercept\n",
    "    except Exception as e:\n",
    "        result['orig_intercept_error'] = str(e)\n",
    "    \n",
    "    # Calculate intercept difference if both exist\n",
    "    if 'our_intercept' in result and 'orig_intercept' in result:\n",
    "        result['intercept_diff'] = result['our_intercept'] - result['orig_intercept']\n",
    "        result['intercept_match'] = abs(result['intercept_diff']) < 1e-5\n",
    "    \n",
    "    # Extract and compare weights - with careful error handling\n",
    "    our_weights = []\n",
    "    orig_weights = []\n",
    "    \n",
    "    try:\n",
    "        # Determine how many weights we can read from our file\n",
    "        our_weight_count = (len(our_data) - 6) // 4\n",
    "        print(f\"Our file should have {our_weight_count} weights\")\n",
    "        \n",
    "        if our_weight_count > 0:\n",
    "            # Try to unpack the weights\n",
    "            weights_fmt = f'{our_weight_count}f'\n",
    "            try:\n",
    "                our_weights = np.array(struct.unpack(weights_fmt, our_data[6:]))\n",
    "                print(f\"Successfully unpacked {len(our_weights)} weights from our file\")\n",
    "                result['our_weight_count'] = our_weight_count\n",
    "                result['our_weights_min'] = float(np.min(our_weights))\n",
    "                result['our_weights_max'] = float(np.max(our_weights))\n",
    "                result['our_weights_mean'] = float(np.mean(our_weights))\n",
    "                result['our_weights_nonzero'] = int(np.sum(np.abs(our_weights) > 1e-10))\n",
    "                print(f\"Our weights: min={result['our_weights_min']}, max={result['our_weights_max']}\")\n",
    "                print(f\"Non-zero weights: {result['our_weights_nonzero']}/{len(our_weights)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error unpacking our weights: {str(e)}\")\n",
    "                # Try a smaller number if full unpack fails\n",
    "                for i in range(our_weight_count, 0, -10):\n",
    "                    try:\n",
    "                        smaller_fmt = f'{i}f'\n",
    "                        our_weights = np.array(struct.unpack(smaller_fmt, our_data[6:6+i*4]))\n",
    "                        print(f\"Successfully unpacked {i} weights\")\n",
    "                        break\n",
    "                    except Exception:\n",
    "                        continue\n",
    "    except Exception as e:\n",
    "        result['our_weights_error'] = str(e)\n",
    "        print(f\"Error analyzing our weights: {str(e)}\")\n",
    "    \n",
    "    try:\n",
    "        # Determine how many weights we can read from original file\n",
    "        orig_weight_count = (len(orig_data) - 6) // 4\n",
    "        print(f\"Original file should have {orig_weight_count} weights\")\n",
    "        \n",
    "        if orig_weight_count > 0:\n",
    "            # Try to unpack the weights\n",
    "            weights_fmt = f'{orig_weight_count}f'\n",
    "            try:\n",
    "                orig_weights = np.array(struct.unpack(weights_fmt, orig_data[6:]))\n",
    "                print(f\"Successfully unpacked {len(orig_weights)} weights from original file\")\n",
    "                result['orig_weight_count'] = orig_weight_count\n",
    "                result['orig_weights_min'] = float(np.min(orig_weights))\n",
    "                result['orig_weights_max'] = float(np.max(orig_weights))\n",
    "                result['orig_weights_mean'] = float(np.mean(orig_weights))\n",
    "                result['orig_weights_nonzero'] = int(np.sum(np.abs(orig_weights) > 1e-10))\n",
    "                print(f\"Original weights: min={result['orig_weights_min']}, max={result['orig_weights_max']}\")\n",
    "                print(f\"Non-zero weights: {result['orig_weights_nonzero']}/{len(orig_weights)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error unpacking original weights: {str(e)}\")\n",
    "                # Try a smaller number if full unpack fails\n",
    "                for i in range(orig_weight_count, 0, -10):\n",
    "                    try:\n",
    "                        smaller_fmt = f'{i}f'\n",
    "                        orig_weights = np.array(struct.unpack(smaller_fmt, orig_data[6:6+i*4]))\n",
    "                        print(f\"Successfully unpacked {i} weights\")\n",
    "                        break\n",
    "                    except Exception:\n",
    "                        continue\n",
    "    except Exception as e:\n",
    "        result['orig_weights_error'] = str(e)\n",
    "        print(f\"Error analyzing original weights: {str(e)}\")\n",
    "    \n",
    "    # Compare weights if both have weights\n",
    "    if len(our_weights) > 0 and len(orig_weights) > 0:\n",
    "        # If weight counts are different, we'll truncate to the shorter one\n",
    "        min_count = min(len(our_weights), len(orig_weights))\n",
    "        if min_count > 0:\n",
    "            our_subset = our_weights[:min_count]\n",
    "            orig_subset = orig_weights[:min_count]\n",
    "            \n",
    "            weight_diff = our_subset - orig_subset\n",
    "            result['weight_comparison'] = {\n",
    "                'common_count': min_count,\n",
    "                'max_diff': float(np.max(np.abs(weight_diff))),\n",
    "                'mean_diff': float(np.mean(np.abs(weight_diff))),\n",
    "                'diff_greater_than_1e-5': int(np.sum(np.abs(weight_diff) > 1e-5)),\n",
    "                'percent_similar': float(100 * (1 - np.sum(np.abs(weight_diff) > 1e-5) / min_count))\n",
    "            }\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"\\nComparison of {result['our_file']} vs {result['original_file']}\")\n",
    "    print(f\"File sizes: Our {result['our_size']} bytes, Original {result['original_size']} bytes\")\n",
    "    \n",
    "    if 'our_classes' in result and 'orig_classes' in result:\n",
    "        print(f\"Class labels: Our {result['our_classes']}, Original {result['orig_classes']}\")\n",
    "        print(f\"  Match: {'Yes' if result['classes_match'] else 'No'}\")\n",
    "    \n",
    "    if 'our_intercept' in result and 'orig_intercept' in result:\n",
    "        print(f\"Intercept: Our {result['our_intercept']:.6f}, Original {result['orig_intercept']:.6f}\")\n",
    "        print(f\"  Difference: {result['intercept_diff']:.6f}\")\n",
    "        print(f\"  Match within tolerance: {'Yes' if result['intercept_match'] else 'No'}\")\n",
    "    \n",
    "    print(f\"Weight counts: Our {result.get('our_weight_count', 'N/A')}, \"\n",
    "          f\"Original {result.get('orig_weight_count', 'N/A')}\")\n",
    "    \n",
    "    if 'weight_comparison' in result:\n",
    "        comp = result['weight_comparison']\n",
    "        print(f\"Weight comparison (first {comp['common_count']} weights):\")\n",
    "        print(f\"  Maximum difference: {comp['max_diff']:.6f}\")\n",
    "        print(f\"  Mean absolute difference: {comp['mean_diff']:.6f}\")\n",
    "        print(f\"  Number of weights differing by >1e-5: {comp['diff_greater_than_1e-5']} \"\n",
    "              f\"({100-comp['percent_similar']:.2f}%)\")\n",
    "    \n",
    "    # Visualize the weight differences\n",
    "    if len(our_weights) > 0 and len(orig_weights) > 0:\n",
    "        plt.figure(figsize=(15, 12))\n",
    "        \n",
    "        # Plot the weights\n",
    "        plt.subplot(4, 1, 1)\n",
    "        plt.plot(our_weights, 'b-', alpha=0.7, label='Our weights')\n",
    "        plt.plot(orig_weights, 'r-', alpha=0.7, label='Original weights')\n",
    "        plt.title('SVM Weights Comparison')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Plot our weights separately to see their pattern\n",
    "        plt.subplot(4, 1, 2)\n",
    "        plt.plot(our_weights, 'b-')\n",
    "        plt.title('Our Weights Only')\n",
    "        plt.grid(True)\n",
    "        plt.ylabel('Weight Value')\n",
    "        \n",
    "        # Plot the differences\n",
    "        plt.subplot(4, 1, 3)\n",
    "        min_length = min(len(our_weights), len(orig_weights))\n",
    "        if min_length > 0:\n",
    "            diff = our_weights[:min_length] - orig_weights[:min_length]\n",
    "            plt.plot(diff, 'g-')\n",
    "            plt.title('Weight Differences (Our - Original)')\n",
    "            plt.grid(True)\n",
    "            plt.ylabel('Difference')\n",
    "        \n",
    "        # Plot a histogram of differences\n",
    "        plt.subplot(4, 1, 4)\n",
    "        if min_length > 0:\n",
    "            plt.hist(diff, bins=50)\n",
    "            plt.title('Histogram of Weight Differences')\n",
    "            plt.xlabel('Difference Value')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Also plot with normalized weights to compare patterns\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        if len(our_weights) > 0 and np.max(np.abs(our_weights)) > 0:\n",
    "            our_norm = our_weights / np.max(np.abs(our_weights))\n",
    "        else:\n",
    "            our_norm = our_weights\n",
    "            \n",
    "        if len(orig_weights) > 0 and np.max(np.abs(orig_weights)) > 0:\n",
    "            orig_norm = orig_weights / np.max(np.abs(orig_weights))\n",
    "        else:\n",
    "            orig_norm = orig_weights\n",
    "        \n",
    "        plt.plot(our_norm, 'b-', alpha=0.7, label='Our weights (normalized)')\n",
    "        plt.plot(orig_norm, 'r-', alpha=0.7, label='Original weights (normalized)')\n",
    "        plt.title('Normalized SVM Weights Comparison')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tree_svms_to_binary_remapped_fixed(tree, output_dir):\n",
    "    \"\"\"\n",
    "    Save all SVM models in the decision tree as binary files, with class labels remapped\n",
    "    to sequential integers starting from 0.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    tree : BinaryDecisionTree\n",
    "        The decision tree containing trained SVM models\n",
    "    output_dir : str\n",
    "        Directory where the binary files will be saved\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        A dictionary containing the mapping information\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import struct\n",
    "    import numpy as np\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get all splitting nodes with trained models\n",
    "    splitting_nodes = tree.splitting_nodes\n",
    "    print(f\"Found {len(splitting_nodes)} splitting nodes in the tree\")\n",
    "    \n",
    "    # Map each binary path to the corresponding endmember decimal value\n",
    "    path_to_value = {}\n",
    "    for i, endmember_label in enumerate(tree.endmembers):\n",
    "        if endmember_label:  # Skip empty endmembers if any\n",
    "            # Convert binary string to decimal\n",
    "            em_value = int(endmember_label, 2)\n",
    "            path_to_value[endmember_label] = em_value\n",
    "    \n",
    "    # Collect all unique class values\n",
    "    all_classes = set()\n",
    "    \n",
    "    # Identify all class values\n",
    "    for node in splitting_nodes:\n",
    "        # Skip nodes without models\n",
    "        if node not in tree.models:\n",
    "            continue\n",
    "        \n",
    "        # Find all endmembers that would go to the left branch (node+'0')\n",
    "        left_values = []\n",
    "        # Find all endmembers that would go to the right branch (node+'1')\n",
    "        right_values = []\n",
    "        \n",
    "        for path, value in path_to_value.items():\n",
    "            # Only process endmembers whose paths are long enough to be affected by this node\n",
    "            if len(path) > len(node):\n",
    "                # Check if this path goes through the current node\n",
    "                if path.startswith(node):\n",
    "                    # This is a descendent of the current node, check which branch\n",
    "                    next_bit = path[len(node)]\n",
    "                    if next_bit == '0':\n",
    "                        left_values.append(value)\n",
    "                    else:\n",
    "                        right_values.append(value)\n",
    "        \n",
    "        # Find lowest class in each branch (by decimal value)\n",
    "        if left_values:\n",
    "            all_classes.add(min(left_values))\n",
    "        if right_values:\n",
    "            all_classes.add(min(right_values))\n",
    "    \n",
    "    # Sort classes for consistent mapping\n",
    "    original_classes = sorted(list(all_classes))\n",
    "    \n",
    "    # Create mapping from original classes to sequential integers\n",
    "    class_mapping = {orig: i for i, orig in enumerate(original_classes)}\n",
    "    reverse_mapping = {i: orig for i, orig in enumerate(original_classes)}\n",
    "    \n",
    "    print(f\"Found {len(original_classes)} unique class values in the tree\")\n",
    "    print(f\"Original classes: {original_classes}\")\n",
    "    print(f\"Remapped to: {list(range(len(original_classes)))}\")\n",
    "    print(f\"Mapping: {class_mapping}\")\n",
    "    \n",
    "    # Count of successfully saved models\n",
    "    saved_count = 0\n",
    "    \n",
    "    for node in splitting_nodes:\n",
    "        # Check if this node has an SVM model\n",
    "        if node not in tree.models:\n",
    "            print(f\"Warning: Node '{node}' is marked as a splitting node but has no SVM model\")\n",
    "            continue\n",
    "        \n",
    "        # Get the SVM model for this node\n",
    "        svm_model = tree.models[node]\n",
    "        \n",
    "        # Find all endmembers that would go to the left branch (node+'0')\n",
    "        left_values = []\n",
    "        # Find all endmembers that would go to the right branch (node+'1')\n",
    "        right_values = []\n",
    "        \n",
    "        for path, value in path_to_value.items():\n",
    "            # Only process endmembers whose paths are long enough to be affected by this node\n",
    "            if len(path) > len(node):\n",
    "                # Check if this path goes through the current node\n",
    "                if path.startswith(node):\n",
    "                    # This is a descendent of the current node, check which branch\n",
    "                    next_bit = path[len(node)]\n",
    "                    if next_bit == '0':\n",
    "                        left_values.append(value)\n",
    "                    else:\n",
    "                        right_values.append(value)\n",
    "        \n",
    "        # Find lowest class in each branch (by decimal value)\n",
    "        left_min = min(left_values) if left_values else 0\n",
    "        right_min = min(right_values) if right_values else 0\n",
    "        \n",
    "        # Remap class values to sequential indices\n",
    "        left_mapped = class_mapping[left_min] if left_min in class_mapping else 0 \n",
    "        right_mapped = class_mapping[right_min] if right_min in class_mapping else 0\n",
    "        \n",
    "        # Format the filename using remapped values\n",
    "        svm_model_name = f\"lsm{left_mapped:02d}{right_mapped:02d}\"\n",
    "        filepath = os.path.join(output_dir, svm_model_name)\n",
    "        \n",
    "        print(f\"Node {node}: Splitting between left branch (orig={left_min}, remapped={left_mapped}) and right branch (orig={right_min}, remapped={right_mapped})\")\n",
    "        \n",
    "        # Debug: Check if weights are non-zero before saving\n",
    "        if hasattr(svm_model, 'coef_'):\n",
    "            weights = svm_model.coef_[0]\n",
    "            non_zero_count = np.sum(np.abs(weights) > 1e-10)\n",
    "            if non_zero_count == 0:\n",
    "                print(f\"  WARNING: All weights are zero for node {node}!\")\n",
    "                \n",
    "                # Try to access the SVM model's raw data directly\n",
    "                if hasattr(svm_model, '_impl') and hasattr(svm_model._impl, 'raw_coef_'):\n",
    "                    print(\"  Attempting to use _impl.raw_coef_ instead...\")\n",
    "                    weights = svm_model._impl.raw_coef_\n",
    "                else:\n",
    "                    print(\"  Could not find alternative weight source\")\n",
    "        \n",
    "        # Open the file for binary writing\n",
    "        with open(filepath, \"wb\") as file:\n",
    "            # Write REMAPPED class labels as uint8\n",
    "            file.write(struct.pack(\"BB\", left_mapped, right_mapped))\n",
    "            \n",
    "            # Write intercept as float32\n",
    "            intercept = svm_model.intercept_[0]\n",
    "            file.write(struct.pack('f', intercept))\n",
    "            \n",
    "            # Write weights as float32\n",
    "            if hasattr(svm_model, 'coef_'):\n",
    "                weights = svm_model.coef_[0]\n",
    "                file.write(struct.pack(f'{len(weights)}f', *weights))\n",
    "                print(f\"  Saved {len(weights)} weights (non-zero: {non_zero_count})\")\n",
    "            else:\n",
    "                print(\"  WARNING: Model does not have coef_ attribute!\")\n",
    "        \n",
    "        print(f\"  Saved model to {svm_model_name}\")\n",
    "        saved_count += 1\n",
    "    \n",
    "    print(f\"Successfully saved {saved_count} SVM models to {output_dir}\")\n",
    "    \n",
    "    # Return the mapping for reference\n",
    "    return {\n",
    "        \"original_to_remapped\": class_mapping,\n",
    "        \"remapped_to_original\": reverse_mapping,\n",
    "        \"original_classes\": original_classes,\n",
    "        \"remapped_classes\": list(range(len(original_classes)))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models with remapped classes\n",
    "mapping = save_tree_svms_to_binary_remapped_fixed(tree, \"weights/svm_models_remapped\")\n",
    "\n",
    "# Print the mapping for reference\n",
    "print(\"\\nClass mapping:\")\n",
    "for orig, remapped in mapping[\"original_to_remapped\"].items():\n",
    "    print(f\"Original class {orig} â†’ Remapped class {remapped}\")\n",
    "\n",
    "# Save the mapping for later use\n",
    "import json\n",
    "with open(\"weights/class_mapping.json\", \"w\") as f:\n",
    "    json.dump(mapping, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, debug the SVM models to see if they have proper weights\n",
    "model_analysis = debug_svm_models(tree)\n",
    "\n",
    "# Then, use the fixed function to save the files\n",
    "save_tree_svms_to_binary_fixed(tree, \"weights/svm_models_combined_10_L1D_112_MACHI_8end_TREE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, examine your tree structure\n",
    "print_tree_structure(tree)\n",
    "{np.int32(0): 0, np.int32(1): 1, np.int32(2): 2, np.int32(4): 3, np.int32(6): 4, np.int32(7): 5, np.int32(8): 6, np.int32(16): 7}\n",
    "# Then save the SVM models with the correct max_depth\n",
    "save_tree_svms_to_binary_correct(tree, \"weights/svm_models\")  # Adjust depth as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "our_directory = \"weights/svm_models\"\n",
    "original_directory = \"weights/svm_models_old\"  # Adjust this path to your original files\n",
    "\n",
    "# Compare files with a more flexible approach\n",
    "analysis = compare_svm_files_flexible(our_directory, original_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "our_file = \"weights/svm_models_fixed/lsm0008\"\n",
    "original_file = \"weights/svm_models_old/lsm0008\"\n",
    "\n",
    "# Detailed comparison of a single file\n",
    "comparison = compare_lsm_file_values(our_file, original_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST HYPSO 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMBINED MACHI\n",
    "image_path = r'D:\\Hierarchical Unmixing Label\\hUH\\images\\combined_10_v2_MACHI.npy'\n",
    "tree_path = r\"D:\\Hierarchical Unmixing Label\\hUH\\weights\\tree_MACHI.pkl\"\n",
    "deh_path = r'D:\\Hierarchical Unmixing Label\\hUH\\save\\MACHI_10img_256to8_stab.h5_aa_FINAL.h5'\n",
    "prediction = predict_pipeline(image_path,tree_path, deh_path, hypso=1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USING NC FILE\n",
    "image_path = r'D:\\Downloads\\aregantsea2_2025-03-11T08-12-43Z-l1a.nc'\n",
    "tree_path = r\"D:\\Hierarchical Unmixing Label\\hUH\\weights\\tree_MACHI.pkl\"\n",
    "deh_path = r'D:\\Hierarchical Unmixing Label\\hUH\\save\\MACHI_10img_256to8_stab.h5_aa_FINAL.h5'\n",
    "prediction = predict_pipeline(image_path,tree_path, deh_path, hypso=1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USING NPY FILE\n",
    "image_path = r'D:\\Hierarchical Unmixing Label\\hUH\\images\\aregantsea2_2025-03-11T08-12-43Z-l1a_cm_machi.npy'\n",
    "tree_path = r\"D:\\Hierarchical Unmixing Label\\hUH\\weights\\tree_MACHI.pkl\"\n",
    "deh_path = r'D:\\Hierarchical Unmixing Label\\hUH\\save\\MACHI_10img_256to8_stab.h5_aa_FINAL.h5'\n",
    "prediction = predict_pipeline(image_path,tree_path, deh_path, hypso=1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aregantsea2_2025_03_11_MACHI          = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\aregantsea2_2025-03-11T08-12-43Z-l1a_cm_machi.npy')\n",
    "aregantsea2_2025_03_11_MACHI_labels   = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\save\\aregantsea2_2025_03_11_MACHI_binary_labels.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tree.predict(aregantsea2_2025_03_11_MACHI)\n",
    "evaluation = tree.evaluate(aregantsea2_2025_03_11_MACHI,aregantsea2_2025_03_11_MACHI_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.plot_input_image(aregantsea2_2025_03_11_MACHI.reshape(-1,1092,112))\n",
    "tree.plot_ground_truth(aregantsea2_2025_03_11_MACHI_labels, key='0000')\n",
    "tree.plot_prediction(prediction, key='0000')  \n",
    "tree.plot_ground_truth(aregantsea2_2025_03_11_MACHI_labels)\n",
    "tree.plot_prediction(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST HYPSO 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losmanzanosfire_2025_03_11_MACHI      = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\images\\losmanzanosfire_2025-03-11T14-56-42Z-l1a_cm_MACHI.npy')\n",
    "losmanzanosfire_2025_03_11_MACHI_labels   = np.load(r'D:\\Hierarchical Unmixing Label\\hUH\\save\\losmanzanosfire_2025_03_11_MACHI_binary_labels.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tree.predict(losmanzanosfire_2025_03_11_MACHI)\n",
    "evaluation = tree.evaluate(losmanzanosfire_2025_03_11_MACHI,losmanzanosfire_2025_03_11_MACHI_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.plot_input_image(losmanzanosfire_2025_03_11_MACHI.reshape(-1,1092,112))\n",
    "tree.plot_ground_truth(losmanzanosfire_2025_03_11_MACHI_labels, key='0000')\n",
    "tree.plot_prediction(prediction, key='0000')  \n",
    "tree.plot_ground_truth(losmanzanosfire_2025_03_11_MACHI_labels)\n",
    "tree.plot_prediction(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hypso",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
